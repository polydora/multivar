---
title: "Анализ главных компонент"
subtitle: "Анализ и визуализация многомерных данных с использованием R"
author: Марина Варфоломеева, Вадим Хайтов
presenters: [{
  name: 'Марина Варфоломеева',
  company: 'Каф. Зоологии беспозвоночных, СПбГУ',
  }]
output:
 ioslides_presentation:
  widescreen: true
  css: assets/my_styles.css
  logo: assets/Linmod_logo.png
---

```{r setup, include = FALSE, cache = FALSE, purl = FALSE}
# output options
options(width = 70, scipen = 6, digits = 3)
library(knitr)
# chunk default options
opts_chunk$set(fig.align='center', tidy = FALSE, fig.width = 7, fig.height = 3, warning = FALSE)
```

## Вы сможете

- Найти собственые значения и собственные векторы матрицы и объяснить их смысл.
- Проводить анализ главных компонент при помощи базовых функций R и функций из пакета `vegan`
- Оценивать долю дисперсии, объясненной компонентами
- Снизить размерность данных, оставив небольшое число компонент
- Интерпретировать смысл компонент по их факторным нагрузкам
- Строить ординацию объектов в пространстве главных компонент
- Создавать комплексные переменные и использовать их в других видах анализов

# Постановка задачи для анализа главных компонент

## Зачем нужен анализ главных компонент?

Когда признаков много, можно представить все объекты как облако точек в многомерном пространстве. Обычно в биологических исследованиях признаки объектов взаимозависимы (между ними есть ненулевая ковариация или корреляция).

![](./images/Migration-DonMcCullough-Flickr.jpg)

Migration by Don McCullough
on [Flickr](https://flic.kr/p/fEFhCj)

## Не все проекции несут важную информацию

![](./images/BlackShadows-FerranJorda-Flickr.jpg)

black shadows for a white horses / les negres ombres dels cavalls blancs by  Ferran Jordà
on [Flickr](https://flic.kr/p/9XJxiL)

## Можно найти оптимальную проекцию, чтобы сохранить максимум информации в минимуме измерений

![](./images/CatsShadow-MarinaDelCastell-Flickr.jpg)

Cat's shadow by Marina del Castell on [Flickr](https://flic.kr/p/ebe5UF)

## Посмотрим, как это выглядит на реальных данных

Данные о размерах медуз _Catostylus mosaicus_ (Lunn & McNeil 1991). Медузы собраны в реке Хоксбери (Новый Южный Уэльс, Австралия): часть --- на острове Дангар, другая --- в заливе Саламандер.

<div class="columns-2">

<img src="images/Blubberjellies-KirstiScott-Flickr.jpg" height=300>

Blubber jellies! by  Kirsti Scott
on [Flickr](https://flic.kr/p/nWikVp)

```{r, echo = FALSE, purl = FALSE}
jelly <- read.delim("data/jellyfish.csv")
head(jelly, 10)
```

</div>

## Двумерные исходные данные {.columns-2}

```{r echo = FALSE, fig.width=5, purl = FALSE}
library(ggplot2)
theme_set(theme_bw() + theme(legend.key = element_blank()))
update_geom_defaults("point", list(shape = 19))

jelly <- read.delim("data/jellyfish.csv")
X_raw <- jelly[, 2:3]
# График исходных данных
gg <- ggplot(as.data.frame(X_raw), aes(x = width, y = length)) + 
  geom_point(size = 2) + 
  coord_equal(expand = c(0, 0))
gg
```

```{r, echo = FALSE, purl = FALSE}
head(jelly, 10)
```

## Задача анализа главных компонент

Исходные переменные скоррелированы.

Хочется найти такую трансформацию исходных данных, чтобы "новые" переменные:

- содержали всю исходную информацию
- были независимы друг от друга
- были ранжированы в порядке убывания важности (например, в порядке убывания их дисперсии)

Интуитивно, мы можем добиться этого, если проведем одну ось вдоль направления, в котором максимально вытянуто облако исходных данных. Вторую ось проведем перпендикулярно первой (и они будут независимы).

```{r echo = FALSE, purl = FALSE}
# Центрируем
X <- scale(X_raw, center = TRUE, scale = FALSE)
A <- cov(X)    # Матрица ковариаций
E <- eigen(A)  # Спектральное разложение
# Координаты точек в новом пространстве
Y <- X %*% E$vectors

# График точек в новом пространстве
gg_rotated <- gg %+% as.data.frame(Y) + 
  aes(x = V1, y = V2) + 
  geom_hline(yintercept = 0) + 
  geom_vline(xintercept = 0) + 
  coord_equal() + 
  labs(x = "PC1", y = "PC2")

library(cowplot)
plot_grid(gg + ggtitle("До"),
          gg_rotated + ggtitle("После"),
          align = "h")
# 
```


## Анализ главных компонент (Principal Component Analysis, PCA)

PCA позволяет:

- Заменить исходное множество скоррелированных признаков набором синтетических взаимонезависимых переменных (редукция данных, dimension reduction)
- Описать систему взаимосвязей между множеством исходных признаков, ранжировать признаки по важности
- Описать взаимное сходство объектов по многим признакам (ординация объектов)

### Практическое использование PCA

- снижение размерности многомерных данных
- визуализация системы связей между признаками
- сжатие изображений
- распознавание лиц
- анализ формы (геометрическая морфометрия)
- создание синтетических взаимонезависимых признаков для других анализов (например, для регрессии, дискриминантного анализа)

# PCA в матричном виде

## Задача PCA на языке матриц

Нам необходимо найти такую матрицу $P$, которая позволит вместо $\mathbf{X}$ взаимозависимых признаков получить $\mathbf{Y}$ взаимонезависимых признаков.

$$\mathbf{Y} = \mathbf{X}\mathbf{P}$$

Как математически выражается взаимонезависимость?

>- Для независимых признаков ковариационная матрица $cov(\mathbf{Y}) = cov(\mathbf{X}\mathbf{P})$ --- диагональная матрица, выше и ниже диагонали нули. Числа на диагонали --- дисперсии --- упорядочены в порядке убывания.

Как нам выбрать $\mathbf{P}$?

>- Если исходные данные $\mathbf{X}$ центрированы, то  $\mathbf{A} = cov(X) = \frac{1}{n - 1} \mathbf{X}\mathbf{X'}$ --- это симметричная квадратная матрица.
>- Для симметричных квадратных матриц должно существовать спектральное разложение $\mathbf{A} = \mathbf{E \Lambda E'}$.
>- __Если использовать $\mathbf{P} = \mathbf{E}$ --- матрицу собственных векторов для $\mathbf{A}$, то ковариационная матрица $cov(\mathbf{X}\mathbf{P})$ будет обладать нужными нам свойствами__.

## Как работает PCA?

1. Исходные данные предварительно центрируют $\mathbf{X}$
2. Из $\mathbf{X}$ получают матрицу $\mathbf{A} = \frac{1}{n - 1}\mathbf{X}\mathbf{X'}$
3. После спектрального разложения $\mathbf{A} = \mathbf{E \Lambda E'}$ получают матрицы собственных векторов $\mathbf{E}$ и собственных чисел $\mathbf{\Lambda}$

### Результаты работы PCA

- __Факторные нагрузки__ $\mathbf{E}$ --- собственные векторы, задают направление новых осей
- __Факторные координаты__ --- новые координаты в пространстве главных компонент $\mathbf{Y} = \mathbf{X}\mathbf{E}$
- __Собственные числа__ $\mathbf{\Lambda}$ --- оценивают дисперсию вдоль главных компонент

## Еще раз повторим свойства собственных векторов:

- Собственные векторы и собственные числа есть только у некоторых квадратных матриц
- У матрицы $n \times n$ будет _n_ собственных векторов
- Собственные векторы перпендикулярны друг другу (ортогональны)
- __Собственные векторы задают направления в пространстве, вдоль которых дисперсия максимальна__
- Первый собственный вектор, связан с первым собственным числом (максимальное значение $\lambda_i$), соответствует максимальной дисперсии, второй - чуть меньшей и т.д.

# PCA вручную

## Сделаем PCA вручную

```{r}
# Исходные данные
jelly <- read.delim("data/jellyfish.csv")

X_raw <- jelly[, 2:3]

gg <- ggplot(as.data.frame(X_raw), aes(x = width, y = length)) + 
  geom_point(size = 2) +
  coord_equal()
gg
```

## Центрируем исходные данные

```{r}
# Центрируем
X <- scale(X_raw, center = TRUE, scale = FALSE)

# График центрированных данных
gg_centered <- gg %+% as.data.frame(X) + 
  geom_hline(yintercept = 0) + 
  geom_vline(xintercept = 0) + 
  coord_equal() + 
  ggtitle("Центрированные данные")
gg_centered
```

## PCA

```{r}
A <- cov(X)    # Матрица ковариаций
E <- eigen(A)  # Спектральное разложение
E$vectors      # Собственные векторы
E$values       # Собственные числа

# Доля объясненной изменчивости
E$values/sum(E$values)
# Координаты точек в новом пространстве
Y <- X %*% E$vectors
```

## Главные компоненты в исходном пространстве

```{r}
# Коэф. угла наклона новых осей в старом пространстве
slope_1 <- E$vectors[1, 1] / E$vectors[2, 1]
slope_2 <- E$vectors[1, 2] / E$vectors[2, 2]

gg_centered +
  geom_abline(slope = slope_1, intercept = 0, 
              linetype = "dashed", 
              colour = "orangered", size = 1) +
  geom_abline(slope = slope_2, intercept = 0, 
              linetype = "dashed", 
              colour = "violet", size = 1)
```

## Ординация точек в пространстве главных компонент

```{r}
gg_rotated <- gg %+% as.data.frame(Y) + 
  aes(x = V1, y = V2) + 
  geom_hline(yintercept = 0) + 
  geom_vline(xintercept = 0) + 
  coord_equal(ylim = c(-10, 10)) + 
  labs(title = "После вращения", x = "PC1", y = "PC2")
gg_rotated
```

## Медузы упорядочились по размеру

```{r echo=FALSE, fig.width=9, message=FALSE}
gg_rotated + 
  aes(colour = jelly$location) + 
  scale_color_brewer("Location", palette = "Set1")
```

- PC1 --- больше всего изменчивости
- PC2 --- то, что осталось

# Восстановление исходных данных и их редукция

## Можно восстановить исходные данные

Мы видели, что матрица факторных координат $\mathbf{Y} = \mathbf{XE}$

$\mathbf{\hat{X}} = \mathbf{YE'} = \mathbf{XEE'}$

Матрица $\mathbf{EE'}$ называется __проекционной матрицей__

>- Если мы используем все собственные векторы, то  
$\mathbf{EE'} = \mathbf{I}$,  
$\mathbf{\hat{X}} = \mathbf{X}$

>- Если используем только часть, то  
$\mathbf{EE'} \ne \mathbf{I}$,  
$\mathbf{\hat{X}} \ne \mathbf{X}$
 
## Восстанавливаем __полные__ исходные данные

```{r}
X_back_full <- X %*% E$vectors %*% t(E$vectors)

gg_back_full <- gg_centered %+%
  as.data.frame(X_back_full) +
  aes(x = V1, y = V2) +
  labs(x = "width", y = "length")

plot_grid(gg_centered,
          gg_back_full + ggtitle("Восстановленные"),
          align = "h")
#
```

## Восстанавливаем __неполные__ исходные данные

Попробуем восстановить исходные данные, используя только первую главную компоненту

```{r}
X_back_pc1 <- X %*% E$vectors[, 1] %*% t(E$vectors[, 1])

gg_back_pc1 <- gg_centered %+% as.data.frame(X_back_pc1) +
  aes(x = V1, y = V2) +
  labs(x = "width", y = "length")

plot_grid(gg_centered,
          gg_back_full + ggtitle("Восстановленные"),
          gg_back_pc1 + ggtitle("Редуцированные"),
          nrow = 1, align = "h")
#
```

# PCA и SVD

## Сейчас PCA обычно делают при помощи SVD

Пусть 
$\mathbf{A} = \mathbf{E \Lambda E'}$ (спектральное разложение), и 
$\mathbf{X} = \mathbf{UDV'}$ (сингулярное разложение)

Если $\mathbf{Х}$ --- матрица центрированных данных, то тогда  
$$\begin{align}\mathbf{A} & = \frac{1}{n - 1}\mathbf{XX'} = \\
& = \frac{1}{n - 1}\mathbf{X'X} = \\
& = \frac{1}{n - 1}\mathbf{(UDV')'UDV'} = \\
& = \frac{1}{n - 1}\mathbf{VDU'UDV'} = \\
& = \frac{1}{n - 1}\mathbf{V D^2 V'}\end{align}$$

>- Т.е.  
$\mathbf{V} = \mathbf{E}$ --- собственные векторы  
$\mathbf{D}^2/{(n - 1)} = \mathbf{\Lambda}$ --- собственные числа.

## Можно найти главные компоненты при помощи SVD:

$$\begin{align}\mathbf{Y} & = \mathbf{XE} = \\
& = \mathbf{UDV'V} = \\
& = \mathbf{UD}\end{align}$$

Более того, можно сказать, что PCA это частный случай SVD для центрированной матрицы исходных данных.

# Действительно многомерные данные

## Пример: Потребление белков в странах Европы с разными видами продуктов питания

![](./images/PaleoDiet-zsoolt-Flickr.jpg)

Paleo Diet by zsoolt on [Flickr](https://flic.kr/p/pPK1nz)

<div class = "footnote">Данные из Weber, 1973</div>


## Открываем данные {.smaller}

```{r}
protein <- read.table(file="data/protein.csv", sep="\t", dec=".", header=TRUE)
protein$region <- factor(protein$region)
rownames(protein) <- protein$country
head(protein)
```

<div class = "footnote">Данные из Weber, 1973</div>


## Задание

- Постройте ординацию стран при помощи nMDS с использованием евклидова расстояния
- Постройте график ординации
- Нанесите при помощи envfit векторы изменения исходных переменных

## Решение

```{r, message=FALSE, fig.width = 7, fig.height=4, purl = FALSE}
library(vegan)
ord_nmds <- metaMDS(protein[, 3:ncol(protein)],
                    distance = "euclidean",
                    trace = 0)
ef <- envfit(ord_nmds, protein[, 3:ncol(protein)])
plot(ord_nmds, display = "site", type = "t", cex = 0.8)
plot(ef)
```

# PCA: сколько компонент нужно оставить?

## Проведем исследование тех же данных с помощью PCA

```{r, message=FALSE, echo=FALSE}
library(vegan)
prot_pca <- rda(protein[, -c(1, 2)], scale = TRUE)
summary(prot_pca)
```

### Собственные векторы и собственные числа

```
Eigenvalues, and their contribution to the correlations 

Importance of components:
                         PC1    PC2    PC3    PC4     PC5     PC6  ...  
Eigenvalue            4.0064 1.6350 1.1279 0.9547 0.46384 0.32513 ...
Proportion Explained  0.4452 0.1817 0.1253 0.1061 0.05154 0.03613 ...
Cumulative Proportion 0.4452 0.6268 0.7521 0.8582 0.90976 0.94589 ...
```

## Сколько компонент нужно оставить, если мы хотим редуцировать данные?

- Эмпирические правила
    - Объясняют больше чем по Brocken Stick Model.
    - Компоненты у которых соб. число > 1 (правило Кайзера-Гатмана)
    - В сумме объясняют заданный % от общей изменчивости (60-80%) - слишком субъективно

> - Это условности!

## График собственных чисел

```{r}
eigenvals(prot_pca) # собственные числа
bstick(prot_pca) # ожидаемое по Brocken Stick Model

screeplot(prot_pca, type = "lines", bstick = TRUE) # график собственных чисел
```

# Интерпретация компонент

## Интерпретация компонент {.smaller .columns-2}

Факторные нагрузки оценивают вклады переменных в изменчивость по главной компоненте

- Модуль значения нагрузки - величина вклада 
- Знак значения нагрузки - направление вклада

```{r}
scores(prot_pca, display = "species", 
       choices = c(1, 2, 3), scaling = 0)
```

### Первая главная компонента:

Высокие __положительные нагрузки по первой главной компоненте__ у переменных `cereals` и `nuts`. Значит, чем больше значение первой компоненты, тем больше потребление злаков и орехов.

Высокие __отрицательные нагрузки__ у переменных `eggs`, `milk`, `whitemeat`, `redmeat`. Значит, чем меньше значение первой компоненты, тем больше потребление яиц, молока, белого и красного мяса. 

> - Т.е. первую компоненту можно назвать "Мясо - злаки и орехи"

## Интерпретация компонент {.smaller .columns-2}

Факторные нагрузки оценивают вклады переменных в изменчивость по главной компоненте

- Модуль значения нагрузки - величина вклада 
- Знак значения нагрузки - направление вклада

```{r}
scores(prot_pca, display = "species", 
       choices = c(1, 2, 3), scaling = 0)
```


### Вторая главная компонента:

Высокие __положительные нагрузки по второй главной компоненте__ у переменных `fish`, `frveg`. Значит, чем больше значение второй компоненты, тем больше потребление рыбы, овощей.

Высоких __отрицательных нагрузкок по второй главной компоненте__ нет ни у одной из переменных. 

> - Т.е. вторую компоненту можно назвать "Потребление рыбы и овощей"

# Графики факторных нагрузок и ординации

## Параметр `scaling`

Внимание! Координаты объектов или переменных можно получить в нескольких вариантах, отличающихся масштабом. От этого масштаба будет зависеть интерпретация.

```{r, echo=FALSE, results='asis', purl = FALSE}
df <- data.frame(
  scaling = c("1, sites", "2, species", "3, symmetric", "0, none"),
  bip = c("биплот расстояний", "биплот корреляций", "", ""),
  scaled = c("координаты объектов масштабированы (х корень из соб. чисел)", "координаты признаков масштабированы (х корень из соб. чисел)", "масштабированы координаты объектов и признаков (х корень 4-й степени из соб. чисел)", "нет масштабирования"),
  dist = c("апроксимируют евклидовы", "НЕ апроксимируют евклидовы", "", ""),
  ang = c("нет смысла", "отражают корреляции", "", "")
  )

colnames(df) <- c("scaling", "Название графика", "Масштаб", "Расстояния между объектами", "Углы между векторами")

kable(df)
```


## Графики

```{r, fig.width=7, fig.height=5}
op <- par(mfrow = c(1, 2))
# График факторных нагрузок
biplot(prot_pca, display = "species", scaling = 2)
# График факторных координат
biplot(prot_pca, display = "sites")
par(op)
```

## Те же самые графики можно построить в ggplot

```{r load-p, echo=FALSE}
df_load <- as.data.frame(scores(prot_pca, display = "species", 
                                choices = c(1, 2, 3), scaling = 2))
# поправки для размещения подписей
df_load$hjust[df_load$PC1 >= 0] <- -0.1
df_load$hjust[df_load$PC1 < 0] <- 1
df_load$vjust[df_load$PC2 >= 0] <- -0.1
df_load$vjust[df_load$PC2 < 0] <- 1
library(grid) # для стрелочек
ar <- arrow(length = unit(0.25, "cm"))

p_load <- ggplot(df_load) + 
  geom_text(aes(x = PC1, y = PC2, label = rownames(df_load)), 
            size = 3, vjust = df_load$vjust, hjust = df_load$hjust) + 
  geom_segment(aes(x = 0, y = 0, xend = PC1, yend = PC2), 
               colour = "grey40", arrow = ar) + 
  coord_equal(xlim = c(-1.9, 1.9), ylim = c(-1.9, 1.9))
```

```{r, ord-p, echo=FALSE}
## График ординации в ggplot
df_scores <- data.frame(protein[, 1:2],
  scores(prot_pca, display = "sites", choices = c(1, 2, 3), scaling = 1))

p_scores <- ggplot(df_scores, aes(x = PC1, y = PC2, colour = region)) + 
  geom_text(aes(label = country)) + 
  coord_equal(xlim = c(-1.2, 1.2), ylim = c(-1.2, 1.2))
```

```{r, warning=FALSE, fig.width=10, echo=FALSE}
plot_grid(p_load, p_scores, align = "h",
          rel_widths = c(0.36, 0.64))
# 
```

## Исходный код графика нагрузок

```{r load-p, eval=FALSE, purl = FALSE}
```

## Исходный код графика ординации

```{r ord-p, eval=FALSE, purl = FALSE}
```


# PCA и nMDS

## В данном случае, результаты PCA похожи на nMDS

```{r, compare-nmds, include=FALSE, results='hide', fig.height=6, fig.width=8, purl = FALSE}
nMDS <- ggplot(as.data.frame(metaMDS(protein[, -c(1:2)])$points), aes(x=MDS1, y=MDS2)) + geom_point(aes(color = protein$region)) + coord_equal()

plot_grid(p_scores + guides(color=FALSE),
  nMDS + guides(color=FALSE),
  align = "h")
# 
```


## В этом примере результаты PCA похожи на nMDS потому что:

- исходные признаки - количественные, нормально распределенные переменные, связанные друг с другом линейно. Для ординации объектов с такими признаками подходит PCA. Для описания различий между объектами подходит евклидово расстояние
- для данной nMDS-ординации использовано евклидово расстояние
- расстояния между объектами на любой ординации PCA соответствуют их евклидовым расстояниям в пространстве главных компонент

# Создание комплексных переменных при помощи PCA

## Создание комплексных переменных

Факторные координаты - это новые составные признаки, которых можно использовать вместо исходных переменных

Свойства факторных координат:

- Среднее = 0, Дисперсия = 1
- Не коррелируют друг с другом

Применение:

  - Уменьшение числа зависимых переменных - для дисперсионного анализа
  - Уменьшение числа предикторов - во множественной регрессии

```{r, echo=FALSE, purl = FALSE}
# Значения факторов (= факторные координаты)
head(scores(prot_pca, display = "sites", 
       choices = c(1, 2, 3), scaling = 1))
```

## При помощи дисперсионного анализа можно проверить, различается ли значение первой главной компоненты ("Мясо - злаки и орехи") между разными регионами Европы


```{r}
# Значения факторов (= факторные координаты)
df <- data.frame(region = protein$region,
  scores(prot_pca, display = "sites", choices = c(1, 2, 3), scaling = 1))
mod <- lm(PC1 ~ region, data = df)
anova(mod)
```

> - Регионы Европы различаются по потреблению мяса, злаков и орехов

## Проверка условий применимости дисперсионного анализа

```{r, fig.width=10, fig.height=4, echo = FALSE}
mod_diag <- fortify(mod)
res_p <- ggplot(data = mod_diag, aes(x = .fitted, y = .stdresid)) + geom_point(aes(size = .cooksd)) + geom_hline(yintercept = 0) + geom_smooth(method="loess", se=FALSE) 
mean_val <- mean(mod_diag$.stdresid)
sd_val <- sd(mod_diag$.stdresid)
norm_p <- ggplot(mod_diag, aes(sample = .stdresid)) + geom_point(stat = "qq") + geom_abline(intercept = mean_val, slope = sd_val)
plot_grid(res_p, norm_p, ncol = 2, rel_widths = c(0.55, 0.45))
# 
```

> - Условия применимости дисперсионного анализа выполняются


## График значений первой компоненты по регионам

```{r, pc1_p, fig.width = 10, fig.height=6, echo=FALSE}
df$region <- reorder(df$region, df$PC1, FUN=mean)
ggplot(df, aes(x = region, y = PC1, colour = region)) + 
  stat_summary(geom = "pointrange", fun.data = "mean_cl_boot", size = 1) + 
  theme(axis.text.x = element_text(angle = 30, vjust = 1, hjust = 1)) 
```


## Пост-хок тест

```{r}
TukeyHSD(aov(mod))
```


## Summary

- Применение метода главных компонент (PCA):
    - снижение размерности данных
    - исследование связей между переменными
    - построение ординации объектов
    - создание комплексных переменных
- Терминология:
    - Собственные числа - вклад компонент в общую изменчивость
    - Факторные нагрузки - корреляции исходных переменных с компонентами - используются для интерпретации
    - Значения факторов - новые координаты объектов в пространстве уменьшенной размерности



## Что почитать

- Borcard, D., Gillet, F., Legendre, P., 2011. Numerical ecology with R. Springer.
- Legendre, P., Legendre, L., 2012. Numerical ecology. Elsevier.
- Oksanen, J., 2011. Multivariate analysis of ecological communities in R: vegan tutorial. R package version 2–0.
- The Ordination Web Page http://ordination.okstate.edu/ (accessed 05.04.17).
- Quinn, G.G.P., Keough, M.J., 2002. Experimental design and data analysis for biologists. Cambridge University Press.
- Zuur, A.F., Ieno, E.N., Smith, G.M., 2007. Analysing ecological data. Springer.

