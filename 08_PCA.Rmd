---
title: "Анализ главных компонент"
subtitle: "Анализ и визуализация многомерных данных с использованием R"
author: Марина Варфоломеева, Вадим Хайтов
presenters: [{
  name: 'Марина Варфоломеева',
  company: 'Каф. Зоологии беспозвоночных, СПбГУ',
  }]
output:
 ioslides_presentation:
  widescreen: true
  css: assets/my_styles.css
  logo: assets/Linmod_logo.png
---

```{r setup, include = FALSE, cache = FALSE, purl = FALSE}
# output options
options(width = 70, scipen = 6, digits = 3)
library(knitr)
# chunk default options
opts_chunk$set(fig.align='center', tidy = FALSE, fig.width = 7, fig.height = 3, warning = FALSE)
```

## Вы сможете

- Найти собственые значения и собственные векторы матрицы и объяснить их смысл.
- Проводить анализ главных компонент при помощи базовых функций R и функций из пакета `vegan`
- Оценивать долю дисперсии, объясненной компонентами
- Снизить размерность данных, оставив небольшое число компонент
- Интерпретировать смысл компонент по их факторным нагрузкам
- Строить ординацию объектов в пространстве главных компонент
- Создавать комплексные переменные и использовать их в других видах анализов

# Постановка задачи для анализа главных компонент

## Зачем нужен анализ главных компонент?

Когда признаков много, можно представить все объекты как облако точек в многомерном пространстве. Обычно в биологических исследованиях признаки объектов взаимозависимы (между ними есть ненулевая ковариация или корреляция).

![](./images/Migration-DonMcCullough-Flickr.jpg)

Migration by Don McCullough
on [Flickr](https://flic.kr/p/fEFhCj)

## Не все проекции несут важную информацию

![](./images/BlackShadows-FerranJorda-Flickr.jpg)

black shadows for a white horses / les negres ombres dels cavalls blancs by  Ferran Jordà
on [Flickr](https://flic.kr/p/9XJxiL)

## Можно найти оптимальную проекцию, чтобы сохранить максимум информации в минимуме измерений

![](./images/CatsShadow-MarinaDelCastell-Flickr.jpg)

Cat's shadow by Marina del Castell on [Flickr](https://flic.kr/p/ebe5UF)

## Анализ главных компонент (Principal Component Analysis, PCA)

- Ординация объектов по многим признакам.

- Описание системы взаимосвязей между множеством исходных признаков и ранжирование признаков по важности.

- Снижение размерности многомерных данных (dimension reduction) и создание синтетических взаимонезависимых признаков для других анализов (например, для регрессии, дискриминантного анализа)

# Вспомним математику PCA на игрушечном примере

## Пример: Размеры медуз

Данные о размерах медуз _Catostylus mosaicus_ (Lunn & McNeil 1991). Медузы собраны в реке Хоксбери (Новый Южный Уэльс, Австралия): часть --- на острове Дангар, другая --- в заливе Саламандер.

<div class="columns-2">

<img src="images/Blubberjellies-KirstiScott-Flickr.jpg" height=300>

Blubber jellies! by  Kirsti Scott
on [Flickr](https://flic.kr/p/nWikVp)

```{r, echo = FALSE, purl = FALSE}
jelly <- read.delim("data/jellyfish.csv")
head(jelly, 10)
```

</div>

## Двумерные исходные данные {.columns-2}

```{r echo = FALSE, fig.width=5, purl = FALSE}
library(ggplot2)
theme_set(theme_bw() + theme(legend.key = element_blank()))
update_geom_defaults("point", list(shape = 19))

jelly <- read.delim("data/jellyfish.csv")
X_raw <- jelly[, 2:3]

# График исходных данных
gg <- ggplot(as.data.frame(X_raw), aes(x = width, y = length)) + 
  geom_point(size = 2) 
gg + coord_equal(expand = c(0, 0))
```

```{r, echo = FALSE, purl = FALSE}
head(jelly, 10)
```

Исходные переменные скоррелированы. 


## Задача анализа главных компонент

Нужно найти такую трансформацию исходных данных, чтобы "новые" переменные:

- содержали всю исходную информацию
- были независимы друг от друга
- были ранжированы в порядке убывания важности (например, в порядке убывания их дисперсии)

Интуитивно, мы можем добиться этого, если проведем одну ось вдоль направления, в котором максимально вытянуто облако исходных данных. Вторую ось проведем перпендикулярно первой (и они будут независимы).

```{r echo = FALSE, purl = FALSE, message=FALSE}
# Центрируем
X <- scale(X_raw, center = TRUE, scale = FALSE)
A <- cov(X)    # Матрица ковариаций
E <- eigen(A)  # Спектральное разложение
# Координаты точек в новом пространстве
Y <- X %*% E$vectors

# График точек в новом пространстве
gg_rotated <- gg %+% as.data.frame(Y) + 
  aes(x = V1, y = V2) + 
  geom_hline(yintercept = 0) + 
  geom_vline(xintercept = 0) + 
  labs(x = "PC1", y = "PC2") %+% 
  coord_equal() 

library(cowplot)
plot_grid(gg + ggtitle("До"),
          gg_rotated + ggtitle("После"),
          align = "h")
```

## Задача PCA на языке матриц

Нужно найти такую матрицу $P$, которая позволит вместо $\mathbf{X}$ взаимозависимых признаков получить $\mathbf{Y}$ взаимонезависимых признаков.

$$\mathbf{Y} = \mathbf{X}\mathbf{P}$$

Как математически выражается взаимонезависимость?

>- Для независимых признаков ковариационная матрица $cov(\mathbf{Y}) = cov(\mathbf{X}\mathbf{P})$ --- диагональная матрица, выше и ниже диагонали нули. Числа на диагонали --- дисперсии --- упорядочены в порядке убывания.

Как нам выбрать $\mathbf{P}$?

>- Если исходные данные $\mathbf{X}$ центрированы, то  $\mathbf{A} = cov(X) = \frac{1}{n - 1} \mathbf{X}\mathbf{X'}$ --- это симметричная квадратная матрица.
>- Для симметричных квадратных матриц должно существовать спектральное разложение $\mathbf{A} = \mathbf{U \Lambda U'}$.
>- __Если использовать $\mathbf{P} = \mathbf{U}$ --- матрицу собственных векторов для $\mathbf{A}$, то ковариационная матрица $cov(\mathbf{X}\mathbf{P})$ будет обладать нужными нам свойствами__.


## Исходные данные

```{r}
# Исходные данные
jelly <- read.delim("data/jellyfish.csv")

X_raw <- jelly[, 2:3]

gg <- ggplot(as.data.frame(X_raw), aes(x = width, y = length)) + 
  geom_point(size = 2) +
  geom_hline(yintercept = 0) + 
  geom_vline(xintercept = 0)
gg + coord_equal(xlim = c(0, 25), ylim = c(0, 25))
```

## Как работает PCA?

1.Исходные данные предварительно центрируют $\mathbf{X}$

```{r}
X <- scale(X_raw, center = TRUE, scale = FALSE) # Центрируем
```

2.Из $\mathbf{X}$ получают ковариационную матрицу $\mathbf{A} = \frac{1}{n - 1}\mathbf{X}\mathbf{X'}$

```{r}
A <- cov(X)    # Матрица ковариаций
```

3.После спектрального разложения $\mathbf{A} = \mathbf{U \Lambda U'}$ получают матрицы собственных векторов $\mathbf{U}$ и собственных чисел $\mathbf{\Lambda}$

```{r}
E <- eigen(A)            # Спектральное разложение
U <- E$vectors           # Собственные векторы
Lambda <- E$values       # Собственные числа
```

# Результаты работы PCA


## Собственные векторы == Факторные нагрузки

__Собственные векторы__ $\mathbf{U}$ перпендикулярны друг другу (ортогональны, независимы). Они задают __главные компоненты__ --- направления новых осей. 

Собственные векторы также называются __факторные нагрузки__, т.к. это линейные комбинации исходных признаков.

```{r}
dimnames(U) <- list(colnames(X),paste0("PC", 1:ncol(X)))
U
```

Обычно собственные векторы масштабируют (стандартизуют)

## Собственные числа

__Собственные числа__ $\mathbf{\Lambda}$ показывают дисперсию вдоль осей, заданных собственными векторами.

```{r}
Lambda
```

Первый собственный вектор соответствует максимальной дисперсии, второй --- чуть меньшей и т.д.

По значениям собственных чисел можно вычислить долю общей изменчивости, связанной с каждой из главных компонент.

```{r}
Explained <- Lambda/sum(Lambda)       # Доля объясненной изменчивости
Explained
```


## Факторные координаты 

Координаты точек в пространстве главных компонент --- __факторные координаты__ --- можно получить так: $\mathbf{Y} = \mathbf{X}\mathbf{U}$.

```{r}
Y <- X %*% U # Координаты точек в новом пространстве

gg_rotated <- gg %+% as.data.frame(Y) + 
  aes(x = PC1, y = PC2) + 
  labs(title = "После вращения") +
  coord_equal(ylim = c(-10, 10))
gg_rotated
```

## Медузы упорядочились по размеру

```{r echo=FALSE, fig.width=9, message=FALSE}
gg_rotated + 
  aes(colour = jelly$location) + 
  scale_color_brewer("Location", palette = "Set1") +
  labs(title = "Результаты PCA",
       x = paste0("PC1, ", round(Explained[1] * 100, 1), "%"),
       y = paste0("PC2, ", round(Explained[2] * 100, 1), "%"))
```

- PC1 --- больше всего изменчивости
- PC2 --- то, что осталось

# Новые наблюдения на ординаци

## Новые наблюдения на ординаци

```{r}
new_raw <- as.matrix(data.frame(width = 6, length = 11))
new_cent <- (new_raw - colMeans(X_raw)) # Центрируем
# Факторные координаты (координаты точек в новом пространстве)
new_y <- new_cent %*% E$vectors

gg_rotated + geom_point(data = data.frame(new_y), 
                        aes(x = X1, y = X2), colour = "red") +
  labs(title = "Предсказание для нового наблюдения")

```

# Восстановление исходных данных и их редукция

## Можно восстановить исходные данные

Мы видели, что матрица факторных координат $\mathbf{Y} = \mathbf{XU}$

$\mathbf{\hat{X}} = \mathbf{YU'} = \mathbf{XUU}$

Матрица $\mathbf{UU'}$ называется __проекционной матрицей__ (или ротационной матрицей)

>- Если мы используем все собственные векторы, то  
$\mathbf{UU'} = \mathbf{I}$,  
$\mathbf{\hat{X}} = \mathbf{X}$

>- Если используем только часть, то  
$\mathbf{UU'} \ne \mathbf{I}$,  
$\mathbf{\hat{X}} \ne \mathbf{X}$

>- Чем больше собственных векторов мы используем, тем точнее будут восстановленные данные (см. предыдущую лекцию).
 
## Восстанавливаем исходные данные с использованием разного числа главных компонент

```{r}
X_back_full <- X %*% U %*% t(U)
gg_back_full <- gg %+% as.data.frame(X_back_full) +
  labs(x = "width", y = "length", title = "Восстановленные")

X_back_pc1 <- X %*% U[, 1] %*% t(U[, 1])
gg_back_pc1 <- gg_back_full %+% as.data.frame(X_back_pc1) +
  labs(title = "Редуцированные")
  
plot_grid(gg %+% as.data.frame(X) + ggtitle("Исходные"),
          gg_back_full, gg_back_pc1, align = "h", nrow = 1)

```

# PCA и SVD

## Сейчас PCA обычно делают при помощи SVD

Легко показать, что результаты будут эквивалентны.

Пусть 
$\mathbf{A} = \mathbf{E \Lambda E'}$ (спектральное разложение), и 
$\mathbf{X} = \mathbf{UDV'}$ (сингулярное разложение)

Если $\mathbf{Х}$ --- матрица центрированных данных, то тогда  
$$\begin{align}\mathbf{A} & = \frac{1}{n - 1}\mathbf{XX'} = \\
& = \frac{1}{n - 1}\mathbf{X'X} = \\
& = \frac{1}{n - 1}\mathbf{(UDV')'UDV'} = \\
& = \frac{1}{n - 1}\mathbf{VDU'UDV'} = \\
& = \frac{1}{n - 1}\mathbf{V D^2 V'}\end{align}$$

>- Т.е.  
$\mathbf{V} = \mathbf{E}$ --- собственные векторы  
$\mathbf{D}^2/{(n - 1)} = \mathbf{\Lambda}$ --- собственные числа.

## Можно найти главные компоненты при помощи SVD:

$$\begin{align}\mathbf{Y} & = \mathbf{XE} = \\
& = \mathbf{UDV'V} = \\
& = \mathbf{UD}\end{align}$$

Более того, можно сказать, что PCA это частный случай SVD для центрированной матрицы исходных данных.

# Действительно многомерные данные

## Пример: Потребление белков в странах Европы с разными видами продуктов питания

![](./images/PaleoDiet-zsoolt-Flickr.jpg)

Paleo Diet by zsoolt on [Flickr](https://flic.kr/p/pPK1nz)

<div class = "footnote">Данные из Weber, 1973</div>


## Открываем данные {.smaller}

```{r}
protein <- read.table(file="data/protein.csv", sep="\t", dec=".", header=TRUE)
protein$region <- factor(protein$region)
rownames(protein) <- protein$country
head(protein)
```

<div class = "footnote">Данные из Weber, 1973</div>


## Делаем PCA {.columns-2}

```{r message=FALSE, fig.width=5, fig.height=5}
library(vegan)
prot_pca <- rda(protein[, -c(1, 2)], 
                scale = TRUE)
biplot(prot_pca)
```

![](images/how-to-owl.jpg)

## Разбираемся с результатами PCA

```{r message=FALSE}
summary(prot_pca)
```

# 1. Сколько компонент нужно оставить?

## Собственные векторы и собственные числа

```
Eigenvalues, and their contribution to the correlations 

Importance of components:
                         PC1    PC2    PC3    PC4     PC5     PC6  ...  
Eigenvalue            4.0064 1.6350 1.1279 0.9547 0.46384 0.32513 ...
Proportion Explained  0.4452 0.1817 0.1253 0.1061 0.05154 0.03613 ...
Cumulative Proportion 0.4452 0.6268 0.7521 0.8582 0.90976 0.94589 ...
```

```{r purl=FALSE}
eigenvals(prot_pca) # собственные числа
```

## Сколько компонент нужно оставить, если мы хотим редуцировать данные?

- Эмпирические правила
    - Компоненты у которых соб. число > 1 (правило Кайзера-Гатмана)
    - В сумме объясняют заданный % от общей изменчивости (60-80%) - слишком субъективно
    - Объясняют больше чем по Broken Stick Model.
    

```{r}
eigenvals(prot_pca) # собственные числа

bstick(prot_pca) # ожидаемое по Broken Stick Model
```

> - Это условности!

## График собственных чисел

```{r}
screeplot(prot_pca, type = "lines", bstick = TRUE) # график собственных чисел
```

# 2. Графики факторных нагрузок и ординации

## Параметр `scaling`

Внимание! Координаты объектов или переменных можно получить в нескольких вариантах, отличающихся масштабом. От этого масштаба будет зависеть интерпретация.

```{r, echo=FALSE, results='asis', purl = FALSE}
df <- data.frame(
  scaling = c("1, sites", "2, species", "3, symmetric", "0, none"),
  bip = c("биплот расстояний", "биплот корреляций", "", ""),
  scaled = c("координаты объектов масштабированы (х корень из соб. чисел)", "координаты признаков масштабированы (х корень из соб. чисел)", "масштабированы координаты объектов и признаков (х корень 4-й степени из соб. чисел)", "нет масштабирования"),
  dist = c("апроксимируют евклидовы", "НЕ апроксимируют евклидовы", "", ""),
  ang = c("нет смысла", "отражают корреляции", "", "")
  )

colnames(df) <- c("scaling", "Название графика", "Масштаб", "Расстояния между объектами", "Углы между векторами")

kable(df)
```


## Графики

```{r, fig.width=7, fig.height=5}
op <- par(mfrow = c(1, 2))
# График факторных нагрузок
biplot(prot_pca, display = "species", scaling = "species")
# График факторных координат
biplot(prot_pca, display = "sites")
par(op)
```

## Те же самые графики можно построить в ggplot2

```{r load-p, echo=FALSE}
df_load <- as.data.frame(scores(prot_pca, display = "species", 
                                choices = c(1, 2, 3), scaling = "species"))
# поправки для размещения подписей
df_load$hjust[df_load$PC1 >= 0] <- -0.1
df_load$hjust[df_load$PC1 < 0] <- 1
df_load$vjust[df_load$PC2 >= 0] <- -0.1
df_load$vjust[df_load$PC2 < 0] <- 1
library(grid) # для стрелочек
ar <- arrow(length = unit(0.25, "cm"))

p_load <- ggplot(df_load) + 
  geom_text(aes(x = PC1, y = PC2, label = rownames(df_load)), 
            size = 3, vjust = df_load$vjust, hjust = df_load$hjust) + 
  geom_segment(aes(x = 0, y = 0, xend = PC1, yend = PC2), 
               colour = "grey40", arrow = ar) + 
  coord_equal(xlim = c(-1.9, 1.9), ylim = c(-1.9, 1.9))
```

```{r, ord-p, echo=FALSE}
## График ординации в ggplot
df_scores <- data.frame(protein[, 1:2],
  scores(prot_pca, display = "sites", choices = c(1, 2, 3), scaling = "sites"))

p_scores <- ggplot(df_scores, aes(x = PC1, y = PC2, colour = region)) + 
  geom_text(aes(label = country)) +
  coord_equal(xlim = c(-1.2, 1.2), ylim = c(-1.2, 1.2))
```

```{r, warning=FALSE, fig.width=10, echo=FALSE}
plot_grid(p_load, p_scores, align = "h",
          rel_widths = c(0.36, 0.64))
# 
```

## Исходный код графика нагрузок

```{r load-p, eval=FALSE, purl = FALSE}
```

## Исходный код графика ординации

```{r ord-p, eval=FALSE, purl = FALSE}
```



# 3. Интерпретация компонент

## Интерпретация компонент {.smaller .columns-2}

Факторные нагрузки оценивают вклады переменных в изменчивость по главной компоненте

- Модуль значения нагрузки --- величина вклада 
- Знак значения нагрузки --- направление вклада

```{r}
scores(prot_pca, display = "species", 
       choices = c(1, 2, 3), scaling = 0)
```

### Первая главная компонента:

Высокие __положительные нагрузки по первой главной компоненте__ у переменных `cereals` и `nuts`. Значит, чем больше значение первой компоненты, тем больше потребление злаков и орехов.

Высокие __отрицательные нагрузки__ у переменных `eggs`, `milk`, `whitemeat`, `redmeat`. Значит, чем меньше значение первой компоненты, тем больше потребление яиц, молока, белого и красного мяса. 

> - Т.е. первую компоненту можно назвать "Мясо -- злаки и орехи"

## Интерпретация компонент {.smaller .columns-2}

Факторные нагрузки оценивают вклады переменных в изменчивость по главной компоненте

- Модуль значения нагрузки --- величина вклада 
- Знак значения нагрузки --- направление вклада

```{r}
scores(prot_pca, display = "species", 
       choices = c(1, 2, 3), scaling = 0)
```


### Вторая главная компонента:

Высокие __положительные нагрузки по второй главной компоненте__ у переменных `fish`, `frveg`. Значит, чем больше значение второй компоненты, тем больше потребление рыбы, овощей.

Высоких __отрицательных нагрузкок по второй главной компоненте__ нет ни у одной из переменных. 

> - Т.е. вторую компоненту можно назвать "Рыба и овощи"

# PCA и nMDS

## Задание

- Постройте ординацию стран при помощи nMDS с использованием евклидова расстояния
- Постройте график ординации
- Нанесите при помощи envfit векторы изменения исходных переменных

## Решение

```{r, message=FALSE, fig.width = 7, fig.height=4, purl = FALSE}
library(vegan)
sprotein <- scale(protein[, 3:ncol(protein)])
ord_nmds <- metaMDS(sprotein,
                    distance = "euclidean",
                    trace = 0, autotransform = F)
ef <- envfit(ord_nmds, sprotein)
plot(ord_nmds, display = "site", type = "t", cex = 0.8)
plot(ef)
```

## В данном случае, результаты PCA похожи на nMDS

```{r, compare-nmds, echo=FALSE, fig.height=6, fig.width=8, purl = FALSE}
nMDS <- ggplot(as.data.frame(ord_nmds$points), aes(x=MDS1, y=MDS2)) + geom_text(aes(color = protein$region, label = protein$country)) + coord_equal()

plot_grid(p_scores + guides(color=FALSE) + ggtitle("PCA"),
  nMDS + guides(color=FALSE) + ggtitle("nMDS"),
  align = "h")
```


## В этом примере результаты PCA похожи на nMDS потому что:

- Исходные признаки --- количественные переменные, связанные друг с другом линейно. Для ординации объектов с такими признаками подходит PCA. Для описания различий между объектами подходит евклидово расстояние.
- Расстояния между объектами на любой ординации PCA соответствуют их евклидовым расстояниям в пространстве главных компонент.
- Для данной nMDS-ординации использовано евклидово расстояние.

```{r, compare-nmds, echo=FALSE, fig.height=4, fig.width=7, purl = FALSE}
```

## Между PCA и nMDS есть принципиальное различие:

- PCA представляет многомерные данные в пространстве независимых осей, ранжированных по важности, поэтому __есть возможность оставить только самые важные оси изменчивости__.
- nMDS пытается найти отображение многомерного пространства в заданном числе измерений (например, на плоскости) __с максимальным сохранением информации из всех измерений__.

```{r, compare-nmds, echo=FALSE, fig.height=4, fig.width=7, purl = FALSE}
```

# Создание составных переменных при помощи PCA

## Создание составных переменных

Факторные координаты --- это новые составные признаки, которых можно использовать вместо исходных переменных

Свойства факторных координат:

- Среднее = 0, Дисперсия = 1
- Не коррелируют друг с другом

Применение:

  - Уменьшение числа зависимых переменных --- для дисперсионного анализа
  - Уменьшение числа предикторов --- во множественной регрессии

```{r, echo=FALSE, purl = FALSE}
# Значения факторов (= факторные координаты)
head(scores(prot_pca, display = "sites", 
       choices = c(1, 2, 3), scaling = "sites"))
```

## Задание

При помощи дисперсионного анализа проверьте, различается ли значение первой главной компоненты ("Мясо -- злаки и орехи") между разными регионами Европы

## При помощи дисперсионного анализа можно проверить, различается ли значение первой главной компоненты ("Мясо -- злаки и орехи") между разными регионами Европы


```{r}
# Значения факторов (= факторные координаты)
df <- data.frame(region = protein$region,
  scores(prot_pca, display = "sites", choices = c(1, 2, 3), scaling = "sites"))
mod <- lm(PC1 ~ region, data = df)
anova(mod)
```

> - Регионы Европы различаются по потреблению мяса, злаков и орехов

## Проверка условий применимости дисперсионного анализа

```{r, fig.width=10, fig.height=4, echo = FALSE}
mod_diag <- fortify(mod)
res_p <- ggplot(data = mod_diag, aes(x = .fitted, y = .stdresid)) + geom_point(aes(size = .cooksd)) + geom_hline(yintercept = 0) + geom_smooth(method="loess", se=FALSE) 
mean_val <- mean(mod_diag$.stdresid)
sd_val <- sd(mod_diag$.stdresid)
norm_p <- ggplot(mod_diag, aes(sample = .stdresid)) + geom_point(stat = "qq") + geom_abline(intercept = mean_val, slope = sd_val)
plot_grid(res_p, norm_p, ncol = 2, rel_widths = c(0.55, 0.45))
# 
```

> - Условия применимости дисперсионного анализа выполняются


## График значений первой компоненты по регионам

```{r, pc1_p, fig.width = 10, fig.height=6, echo=FALSE}
df$region <- reorder(df$region, df$PC1, FUN=mean)
ggplot(df, aes(x = region, y = PC1, colour = region)) + 
  stat_summary(geom = "pointrange", fun.data = "mean_cl_boot", size = 1) + 
  theme(axis.text.x = element_text(angle = 30, vjust = 1, hjust = 1)) 
```


## Пост-хок тест

```{r}
TukeyHSD(aov(mod))
```


## Summary

- Применение метода главных компонент (PCA):
    - снижение размерности данных
    - исследование связей между переменными
    - построение ординации объектов
    - создание комплексных переменных
- Терминология:
    - Собственные числа --- вклад компонент в общую изменчивость
    - Факторные нагрузки --- корреляции исходных переменных с компонентами --- используются для интерпретации
    - Значения факторов --- новые координаты объектов в пространстве уменьшенной размерности



## Что почитать

- Borcard, D., Gillet, F., Legendre, P., 2011. Numerical ecology with R. Springer.
- Legendre, P., Legendre, L., 2012. Numerical ecology. Elsevier.
- Oksanen, J., 2011. Multivariate analysis of ecological communities in R: vegan tutorial. R package version 2–0.
- The Ordination Web Page http://ordination.okstate.edu/ (accessed 05.04.17).
- Quinn, G.G.P., Keough, M.J., 2002. Experimental design and data analysis for biologists. Cambridge University Press.
- Zuur, A.F., Ieno, E.N., Smith, G.M., 2007. Analysing ecological data. Springer.

