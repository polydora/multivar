---
title: "Краткое введение в мир линейной алгебры. Часть 2"
subtitle: "Анализ и визуализация многомерных данных с использованием R"
author: Вадим Хайтов, Марина Варфоломеева
presenters: [{
    name: 'Вадим Хайтов',
    company: 'Каф. Зоологии беспозвоночных, СПбГУ',
    }]
output:
 ioslides_presentation:
  widescreen: true
  css: assets/my_styles.css
  logo: assets/Linmod_logo.png
---

## Вы сможете
- Объяснить что такое матрицы и какие бывают их основные разновидности
- Выполнить базовые операции с матрицами с использованием функций R
- Применить в среде R методы линейной алгебры для решения простейших задач

```{r setup, include = FALSE, cache = FALSE, purl = FALSE}
# output options
options(width = 70, scipen = 6, digits = 3)
library(knitr)
library(ggplot2)
library(grid)
library(gridExtra)


# chunk default options
opts_chunk$set(fig.align='center', tidy = FALSE, fig.width = 7, fig.height = 3, warning = FALSE)

ar <- arrow(type = "closed", length = unit(0.15,"cm"))
```

# Level 1: Зоопарк матричных объектов

## Матричные объекты
- Есть много типов объектов, для которых такое выражение оказывается наиболее естественным (изображения, описания многомерных объектов и т.д.)
- В матрицах, как и в обычных числах, скрыта информация, которую можно извлекать и преобразовывать по определенным правилам


## Структура матриц

$$\begin{pmatrix}
a_{11} & a_{12} & \cdots & a_{1c} \\
a_{21} & a_{22} & \cdots & a_{2c} \\    
\vdots & \vdots & \ddots & \vdots \\
a_{r1} & a_{r2} & \cdots & a_{rc}
\end{pmatrix}
    $$

Размер (порядок) матрицы $r \times c$ 

## Разновидности матриц 


$$ 
\textbf {a} =
\begin{pmatrix}
1 & 2 & 3 
\end{pmatrix}
$$
Вектор-строка (Row matrix)

$$ 
\textbf {b} =
\begin{pmatrix}
1 \\
4 \\    
7 \\
10 
\end{pmatrix}
$$
Вектор-столбец (column matrix)


## Разновидности матриц 



$$ 
\textbf {C} =
\begin{pmatrix}
1 & 2 & 3 \\
4 & 5 & 6 \\    
7 & 8 & 9 \\
10 & 11 & 12 
\end{pmatrix}
$$




$$ 
\textbf {D} =
\begin{pmatrix}
1 & 2 & 3 \\
4 & 5 & 6     
\end{pmatrix}
$$

> Прямоугольные матрицы (rectangular matrices)


> В таком виде обычно представляются исходные данные при многомерном анализе. 

В такой матрице столбцы - признаки (p), а строки - объекты (n).

Лучше, когда n > p, то есть когда объектов больше, чем признаков.


## Квадратные матрицы (square matrices) {.smaller}

Это наиболее "операбельные" матрицы

$$ \textbf {E} =
\begin{pmatrix}
1 & 2 & 3 \\
4 & 5 & 6 \\    
7 & 8 & 9 
\end{pmatrix}
$$

Диагональные матрицы (diagonal matrix)

$$ \textbf {F} =
\begin{pmatrix}
1 & 0 & 0 & 0 \\
0 & 5 & 0 & 0 \\    
0 & 0 & 9 & 0 \\
0 & 0 & 0 & 1
\end{pmatrix}
$$

## Квадратные матрицы (square matrices) {.smaller}

Треугольные матрицы (triangular matrices)
$$ \textbf {H} =
\begin{pmatrix}
1 & 2 & 3 & 4 \\
0 & 5 & 6 & 7 \\    
0 & 0 & 9 & 10 \\
0 & 0 & 0 & 1
\end{pmatrix}
$$

или

$$ \textbf {H} =
\begin{pmatrix}
1 & 0 & 0 & 0 \\
3 & 5 & 0 & 0 \\    
4 & 7 & 9 & 0 \\
5 & 8 & 10 & 11
\end{pmatrix}
$$

## Квадратные матрицы (square matrices) {.smaller}

Единичная матрица (identity matrix)

$$ 
\textbf {I} =
\begin{pmatrix}
1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 \\    
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1
\end{pmatrix}
$$


Единичная матрица (обозначение $\textbf{I}$) занимают особое место в матричной алгебре.   
Она выполняет ту же роль, которую выполняет единица в обычной алгебре. 


## Матрицы ассоциации

Изначально результаты исследования имеют вид исходной матрицы (обычно прямоугольной)

$$ 
\textbf{Y} = [n_{objects} \times p_{descriptors}]
$$

Информация из этой матрицы конденсируется в двух других матрицах 


Q анализ

$$ 
\textbf{A}_{nn} = [n_{objects} \times n_{objects}]
$$

R анализ

$$ 
\textbf{A}_{pp} = [p_{descriptors} \times p_{descriptors}]
$$


## Матрицы ассоциации 

Это симметричные квадратные матрицы  

$$ 
\textbf{A}_{pp} =  
\begin{pmatrix}
a_{11} & a_{12} & \cdots & a_{1p} \\
a_{21} & a_{22} & \cdots & a_{2p} \\    
\vdots & \vdots & \ddots & \vdots \\
a_{p1} & a_{p2} & \cdots & a_{pp}
\end{pmatrix}
$$

В этой матрице $a_{ij} = a_{ji}$

Большинство многомерных методов имеет дело именно с такими матрицами

## Особенность квадратных матриц

Для квадратных матриц могут быть найдены (но не обязательно существуют) некоторые важные для линейной алгебры показатели: *определитель*, *инверсия*, *собственные значения* и *собственные векторы*


 
 
 

## Задание
Создайте с помощью R следующие матрицы
```{r, echo=FALSE, purl=FALSE}
matrix(1:12, ncol = 3)
```


```{r, echo=FALSE, purl=FALSE}
diag(rep(1,5))
```



# Level 2: Простейшие операции с матричными объектами



## Транспонирование матриц

```{r, purl=TRUE}
A <- matrix(1:12, ncol = 3)
A
```

Транспонированная матрица $\textbf{B} = \textbf{A}'$ синонимичная запись $\textbf{B} = \textbf{A}^{T}$

```{r, purl=TRUE}
B <- t(A)
B
```

## Сложение матриц
```{r, purl=TRUE}
A + 4
```

```{r, purl=TRUE}
A + A
```

Но! Нельзя складывать матрицы разных размеров
```{r, eval=FALSE, purl=TRUE}
A + B
```



## Простое умножение 
Умножение на число
```{r, purl=TRUE}
A * 4
```

Простое умножение матрицы на вектор возможно только если число элементов в векторе равно числу строк в матрице

```{r, purl=TRUE}
A * c(10, 11, 12, 13)

```

Все элементы первой строки матрицы умножаются на первый элемент вектора, все элементы второй строки на второй элемент вектора и т.д.  




# Level 3: Векторы

## Интерпретация вектора{.columns-2 .smaller}

Вектор -- это последовательность чисел: $(x_1, x_2, ..., x_n)$.

Геометрической интерпретацией вектора является направленный отрезок в n-мерном пространстве с началом в точке $(0, 0 .... 0)$. 

Если в векторе всего два числа, то это направленный отрезок на плоскости.

Пример: `vec = (1, 5)`

```{r fig.width=4}
vec <- data.frame(x = c(0, 1), y = c(0, 5))
ggplot(vec, aes(x, y)) + geom_point() + geom_path(arrow = arrow(), size = 2, color = "red") + labs(x="Первое число вектора", y ="второе число вектора")  
  
  
```



## Длина вектора 

Длина вектора, или норма вектора

$$
||\textbf{b}|| = \sqrt{b_1^2 + b_2^2 + \dots + b_n^2}
$$

Длина вектора 

```{r, purl=TRUE}
Vec <- 1:5

sqrt(sum(Vec^2))

norm(t(Vec), type = "F") #Аналогчное решение
```




##  Скалярное произведение векторов   
Допустимо только для векторов одинаковой размерности

$$
\textbf{a} \cdot \textbf{b} =  
\begin{pmatrix}
a_1 \\
a_3 \\    
a_4 \\
a_5 \\
a_6 \\
a_7
\end{pmatrix}
\times
\begin{pmatrix}
b_1 &
b_3 &    
b_4 &
b_5 &
b_6 &
b_7
\end{pmatrix}
= a_1b_1 + a_2b_2 + ... + a_7b_7 = x
$$

Результат этой операции - число (скаляр)


## Зачем это нужно? 
### Бытовой пример


В доме есть следующие электроприборы.

Электроприбор | Количество | Мощность (Вт) |    
--------------|------------|---------------|     
Чайник        | 2 шт       |       1200    |     
Обогреватели  | 3 шт.      |    1300       |
Осушитель     | 1 шт.      |     1100 |
Стиральная машина | 1 шт.| 1500 |
Фен | 2 шт. | 800 |

Вопрос: Какова будет суммарная мощность всех электроприборов, если их включить одновременно?

## Решение

```{r, purl=TRUE}
a <- c(2, 3, 1, 1, 2)
b <- c(1200, 1300, 1100, 1500, 800)

a %*% b
```




## Геометрическая интерпретация скалярного произведения векторов

Скалярное произведение равно произведению длин векторов на $cos$ угла между ними

$$
\textbf{a} \cdot \textbf{b} =  ||\textbf{a}|| \times ||\textbf{b}|| \times \cos(\alpha)
$$

Если угол между векторами равен 90 градусов, то такие векторы называются ортогональными.

У таких векторов скалярное произведение $\textbf{a} \cdot \textbf{b} = 0$

## Задание

Выясните, являются ли ортогональными следующие векторы?

```{r, purl=TRUE}
a <- c(0, 1)
b <- c(1, 0)
c <- c(1, 1)
d <- c(1, -1)
```

## Решение {.smaller .columns-2}
Аналитическое решение
```{r, purl=FALSE}
(a) %*% (b) #Ортогональны
(c) %*% (a) #Неортогональны
(c) %*% (b) #Неортогональны
(d) %*% (c) #Ортогональны
(c) %*% (a) #неортогональны
(b) %*% (d) #неортогональны


```

Геометрическое решение
```{r, echo=FALSE, fig.height=2.5, purl=FALSE }

ggplot() + geom_vline(xintercept = 0) + geom_hline(yintercept = 0) + geom_segment(aes(x = 0, xend = 1, y = 0, yend = 0), arrow = arrow(), size = 2, color = "red")  + geom_segment(aes(x = 0, xend = 0, y = 0, yend = 1), arrow = arrow(), size = 2, color = "blue")   + geom_segment(aes(x = 0, xend = 1, y = 0, yend = 1), arrow = arrow(), size = 2, color = "green")  + geom_segment(aes(x = 0, xend = 1, y = 0, yend = -1), arrow = arrow(), size = 2, color = "red") + xlim(-1.2, 1.2) + ylim(-1.2,1.2) + coord_equal() + geom_text(aes(x=c(1.1, 0.1, 1.1, 1.1), y = c(0.1, 1.1, 1.1, -1.1), label=c("a", "b", "c", "d")))
```


## Угол между векторами
Пусть, векторы отражают признаки.   
Что характеризует угол между векторами?  

```{r, echo=FALSE, purl=TRUE}
Dat <- matrix(c(c(0.5, 0.5), c(0, 3)), nrow = 2)

row.names(Dat) <- c("Object1", "Object2")
colnames(Dat) <- c("Tr1", "Tr2") 

Dat_df <- as.data.frame(t(Dat))
Dat_df

ggplot(Dat_df) + geom_vline(xintercept = 0) + geom_hline(yintercept = 0) + geom_segment(aes(x = 0, xend = Object1, y = 0, yend = Object2), arrow = arrow(), size = 2, color = "red") + labs(x = "Объект 1", y = "Объект 2") + xlim(0, 3) + coord_fixed()
 
```

## Угол между векторами
### Длина векторов

```{r, purl=TRUE}
norm(t(Dat[ ,1 ]), type = "F") #Длина вектора Tr1 
norm(t(Dat[ , 2]), type = "F") #Длина вектора Tr2

```

## Угол между веторами
$$
\cos(\alpha) = \frac{\textbf{a} \cdot \textbf{b}} {||\textbf{a}|| \times ||\textbf{b}||}
$$

```{r, purl=TRUE}

cos_a <- (Dat[, 1] %*% Dat[, 2])/(norm(t(Dat[, 1]), type = "F") * 
                                    norm(t(Dat[, 2]), type = "F"))

cos_a

```

Угол между векторами - мера сонаправленности векторов, то есть их взаимосвязи (корреляции).



## Нормализованные векторы

$$
\textbf{c} = \frac{\textbf{b}} {||\textbf{b}||}
$$


## Задание 

Найдите нормализованный вектор для следующего вектора и определите его длину

```{r, purl=TRUE}
Vec <- 1:5
Vec
```

## Решение
```{r, purl=FALSE}
normalized_Vec <- Vec/norm(t(Vec), type = "F")
normalized_Vec
```

Длина нормализованного вектора 

```{r, purl=FALSE}
norm(t(normalized_Vec), type = "F")

```


>- Длина нормализованных векторов равна 1. Это важное свойство для многомерных методов. 





## Нормализованные векторы {.smaller .columns-2}

Исходные данные

```{r, echo=FALSE, purl=TRUE}
Dat <- matrix(c(c(0.5, 0.5), c(0, 3)), nrow = 2)

row.names(Dat) <- c("Object1", "Object2")
colnames(Dat) <- c("Tr1", "Tr2") 

Dat_df <- as.data.frame(t(Dat))

Dat_df_norm <- Dat_df

Dat_df
```

Нормализованные данные

```{r,echo=FALSE, purl=TRUE}

Dat_df_norm[1, ] <- Dat_df[1,]/norm(t(Dat_df[1,]), type = "F")

Dat_df_norm[2, ] <- Dat_df[2,]/norm(t(Dat_df[2,]), type = "F")

Dat_df_norm

ggplot() + geom_vline(xintercept = 0) + geom_hline(yintercept = 0) + geom_segment(dat = Dat_df, aes(x = 0, xend = Object1, y = 0, yend = Object2), arrow = arrow(), size = 2, color = "red") +  geom_segment(dat = Dat_df_norm, aes(x = 0, xend = Object1, y = 0, yend = Object2), arrow = arrow(), size = 2, color = "blue") + labs(x = "Объект 1", y = "Объект 2") + xlim (0, 3) + coord_fixed()

```
Характер взаимосвязи между нормализованными векторами такой же, как и у исходных векторов.



# Level 4: Операции с матрицами 



## Матричное умножение матрицы на вектор {.smaller .columns-2}

$$
\mathbf{A} \times \mathbf{a}
$$

Умножать можно только в том случае, если число столбцов в матрице равно количеству чисел в векторе.  

Пусть, есть матрица  $\mathbf{A}$

```{r, echo=FALSE, purl=TRUE}
A
```

<br>

<br>

<br>

<br>

```{r, purl=TRUE}
A %*% c(10, 10, 10)
```

- Первое число итогового вектора - скалярное произведение первой строки $\mathbf{A}$ на вектор $\mathbf{a}$    
- Второе число - скалярное произведение второй строки $\mathbf{A}$ на вектор $\mathbf{a}$   
и т.д.


Но! если поменять местами множители, то будет ошибка
```{r, error=TRUE, purl=TRUE}
c(10, 10, 10) %*% A
```

## Матричное умножение вектора на матрицу

$$
\mathbf{a} \times  \mathbf{A}
$$

Умножать можно только в том случае, если количество чисел в векторе равно количеству строк в матрице.

```{r, echo=TRUE, purl=TRUE}
c(10, 10, 10, 10) %*% A
```

Но! если поменять местами множители, то будет ошибка
```{r, error=TRUE, purl=TRUE}
A %*% c(10, 10, 10, 10)
```








## Умножение матриц {.smaller .columns-2}


Умножать можно только в том случае, если число колонок в первой матрице равно числу строк второй матрицы: $\mathbf{A} \times \mathbf{B}$ 

Пусть, есть матрица $\mathbf{A}$

```{r, echo=FALSE, purl=TRUE}
A
```

и матрица $\mathbf{B}$


```{r, echo=FALSE, purl=TRUE}
B
```

<br>

```{r, purl=TRUE}
A %*% B
```


НО! Нельзя произвести такое умножение
```{r, error=TRUE, purl=TRUE}
A %*% A

```

## Схема умножения матриц

$$
\begin{pmatrix}
A & B  \\
C & D \\
\end{pmatrix}
\times
\begin{pmatrix}
E & F \\ G & H\\ 
\end{pmatrix}
=
\begin{pmatrix}
(A \cdot E + B \cdot G)  & (A \cdot F + B \cdot H ) \\ (C \cdot E + D \cdot G)  & (C \cdot F + D \cdot H)  \\
\end{pmatrix}
$$

## Некоторые свойства произведения матриц {.smaller .columns-2}

Если существует произведение матриц $\textbf{BC}$, то не обязательно существует $\textbf{CB}$

```{r, purl=TRUE}
B <- matrix(1:24, ncol = 4)
B
C <- matrix(1:12, ncol = 3)
C

B %*% C
```

HO! 

```{r, error=TRUE, purl=TRUE}

C %*% B
```





## Некоторые свойства произведения матриц

Всегда существует такое произведение матриц $\textbf{C}\textbf{C}'$ и $\textbf{C}'\textbf{C}$

```{r, purl=TRUE}
C %*% t(C)
```

```{r, purl=TRUE}
t(C) %*% C
```


## Некоторые свойства произведения матриц

Произведение матриц $\textbf{BC}$ как правило не равно $\textbf{CB}$
```{r, purl=TRUE}
B <- matrix(1:9, ncol = 3)
C <- matrix(11:19, ncol = 3)

B %*%  C

C %*% B
```


## Некоторые свойства произведения матриц

$[\textbf{BC}]' = \textbf{C}'\textbf{B}'$
```{r, purl=TRUE}
t(B %*% C)
```

```{r, purl=TRUE}
t(C) %*% t(B)
```

## Некоторые свойства произведения матриц

Произведение $\textbf{B}\textbf{B}'$ и $\textbf{B}'\textbf{B}$ всегда дает симметричную матрицу 

```{r, purl=TRUE}
B %*% t(B)
```

```{r, purl=TRUE}
t(B) %*% B

```

## Зачем это нужно?  {.smaller .columns-2}
### Бытовой пример
Представим себе, что вы решили купить четыре товара, по следующим ценам 

Товар | Цена    
------|-----    
Товар 1 | 10    
Товар 2 | 20    
Товар 3 | 30   
Товар 4 | 40    



Прямых выходов на продавца у вас нет, но есть три посредника, которые выставляют следующие "накрутки" цен. 

Посредники  | Товар 1 | Товар 2 | Товар 3 | Товар 4 
------------|---------|---------|---------|--------
Посредник 1 | 0.1 | 0.15 | 0.05 | 0.05 
Посредник 2 | 0.15 | 0.15 | 0.09 | 0.01 
Посредник 3 | 0.2 | 0.05 | 0.1 | 0.1  

<br>

Какой из посредников выгоднее?


## Решение

```{r, purl=TRUE}
cost <- c(10, 20, 30, 40)

retailer <- matrix(c(0.1, 0.15, 0.05, 0.05,  
                     0.15, 0.15, 0.09, 0.01, 
                     0.2, 0.05, 0.1, 0.1 ), byrow = TRUE, ncol = 4)


retailer %*% cost  

```


# Level 5.Матрицы, как инструмент преобразования 

## Матрицы позволяют преобразовывать системы векторов

Начальная система расположения точек

```{r, purl=TRUE}
y = c(2,2,3,3,2,2,3,4,5,6,6,5,4,3,2)
x = c(2,3,4,5,6,7,7,7,6,5,4,3,2,2,2)

Image <- cbind((x), (y))

qplot(Image[,1], Image[,2] ) + geom_polygon(fill = "red") + coord_fixed()


```



## Матрицы позволяют преобразовывать системы векторов {.smaller .columns-2}

Вращающая матрица
$$
\textbf{Rot} = 
\begin{pmatrix}
\cos\phi & -\sin \phi \\
\sin\phi & \cos\phi
\end{pmatrix}
$$
Поворот изображения на заданный угол

```{r, purl=TRUE}

angle <- 30*pi/180

Rot <- matrix(c(cos(angle), sin(angle), 
                -sin(angle), cos(angle)), nrow = 2)

Image_trans <-   t((Rot) %*% t(Image))     


qplot(Image_trans[,1], Image_trans[,2] ) + 
  geom_polygon(fill = "red") + coord_fixed()


```

## Матрицы позволяют преобразовывать системы векторов {.smaller .columns-2}

Масштабирующая матрица
$$
\textbf{Scale} = 
\begin{pmatrix}
a & 0 \\
0 & b
\end{pmatrix}
$$

<br>

<br>

```{r, purl=TRUE}
Scale <- matrix(c(1, 0, 0, 2), nrow = 2)

Image_trans2 <-   t((Scale) %*% t(Image_trans))     

qplot(Image_trans2[,1], Image_trans2[,2] ) + 
  geom_polygon(fill = "red") + coord_fixed()

```

  

# Level 6: Корреляционные и ковариационные матрицы

## Ковариационная матрица
Во многих методах многомерной статистики применяется матрица ковариации.

Ковариация (согласованное отклонение от среднего):

$$
cov(X, Y) = \frac{1}{n - 1}\sum{(x_i - \bar{x})(y_i - \bar{y})}
$$

```{r, echo=FALSE, purl=FALSE}

library(gridExtra)

X_back <- rnorm(20, 0, 4)
Y_back <- rnorm(20, 1, 4)

XY <- data.frame(x = X_back, y = Y_back)

meanX <- 0
meanY <- 1
Xmin <- -5
Xmax <- 5
Ymin <- -5
Ymax <- 5

X <- 2
Y <- 4
X1 <- -2
Y1 <- -4
X2 <- 2
Y2 <- -4

ar <- arrow(type = "closed", length = unit(0.15,"cm"))

Pl_positiv1 <- 
  ggplot() + 
  geom_point(data = XY, aes(x = X, y = Y), color = "lightgrey") + 
  geom_point(aes(x = c(meanX, X), y = c(meanY, Y))) +
  xlim(Xmin, Xmax) + ylim(Ymin, Ymax) + guides(size = "none") + 
  geom_segment(aes(x = meanX, y = meanY, xend = X - 0.2, yend = Y - 0.2), arrow = ar) +
  geom_segment(aes(x = meanX, y = meanY, xend = meanX, yend = Ymin), linetype = 2) +
  geom_segment(aes(x = meanX, y = meanY, xend = Xmin, yend = meanY), linetype = 2) +
  geom_segment(aes(x = X, y = Y, xend = Xmin, yend = Y), linetype = 2) +
  geom_segment(aes(x = X, y = Y, xend = X, yend = Ymin), linetype = 2) +
  geom_segment(aes(x = meanX, y = Ymin, xend = X, yend = Ymin), arrow = ar, color = ("red"), size = 1) + 
  labs(x = "x", y = "y") +
  geom_segment(aes(x = Xmin, y = meanY, xend = Xmin, yend = Y), arrow = ar, color = ("red"), size = 1) + 
  ggtitle("Положительные\nотклонения") 


Pl_positiv2 <- 
  ggplot() + 
  geom_point(data = XY, aes(x = X, y = Y), color = ("lightgrey")) + 
  geom_point(aes(x = c(meanX, X1), y = c(meanY, Y1))) +
  xlim(Xmin, Xmax) + ylim(Ymin, Ymax) + guides(size = "none") + 
  geom_segment(aes(x = meanX, y = meanY, xend = X1 + 0.2, yend = Y1 + 0.2), arrow = ar) +
  geom_segment(aes(x = meanX, y = meanY, xend = meanX, yend = Ymin), linetype = 2) +
  geom_segment(aes(x = meanX, y = meanY, xend = Xmin, yend = meanY), linetype = 2) +
  geom_segment(aes(x = X1, y = Y1, xend = Xmin, yend = Y1), linetype = 2) +
  geom_segment(aes(x = X1, y = Y1, xend = X1, yend = Ymin), linetype = 2) +
  geom_segment(aes(x = meanX, y = Ymin, xend = X1, yend = Ymin), arrow = ar, color = ("blue"), size = 1) + 
  geom_segment(aes(x = Xmin, y = meanY, xend = Xmin, yend = Y1), arrow = ar, color = ("blue"), size = 1) + 
  labs(x = "x", y = "y") +
  ggtitle("Отрицательные\nотклонения")

Pl_negative <- 
  ggplot() + 
  geom_point(data = XY, aes(x = X, y = Y), color = ("lightgrey")) + 
  geom_point(aes(x = c(meanX, X2), y = c(meanY, Y2))) +
  xlim(Xmin, Xmax) + ylim(Ymin, Ymax) + guides(size = "none") + 
  geom_segment(aes(x = meanX, y = meanY, xend = X2 - 0.2, yend = Y2 + 0.2), arrow = ar) +
  geom_segment(aes(x = meanX, y = meanY, xend = meanX, yend = Ymin), linetype = 2) +
  geom_segment(aes(x = meanX, y = meanY, xend = Xmin, yend = meanY), linetype = 2) +
  geom_segment(aes(x = X2, y = Y2, xend =Xmin, yend = Y2), linetype = 2) +
  geom_segment(aes(x = X2, y = Y2, xend = X, yend = Ymin), linetype = 2) +
  geom_segment(aes(x = meanX, y = Ymin, xend = X2, yend = Ymin), arrow = ar, color = ("red"), size = 1)+ 
  geom_segment(aes(x = Xmin, y = meanY, xend = Xmin, yend = Y2), arrow = ar, color = ("blue"), size = 1) + 
  labs(x = "x", y = "y") +
  ggtitle("Отклонения\nв разных направлениях")

grid.arrange(Pl_positiv1, Pl_positiv2, Pl_negative, nrow = 1)

```





## Ковариационная матрица

$$
\textbf{S} = \frac{1}{n - 1} \textbf{Y}_{centered}'\textbf{Y}_{centered}
$$


где $\textbf{Y}_{centered}$ - центрированная матрица исходных значений



## Корреляционая матрица

То же самое, что ковариационная матрица, но только на основе стандартизованных исходных значений

$$
\textbf{R} = \frac{1}{n - 1} \textbf{Y}_{stand}'\textbf{Y}_{stand}
$$


## Вычисление матрицы ковариации с помощью линейной алгебры {.smaller .columns-2}

Исходная матрица
```{r, purl=TRUE}
M <- matrix(c(1,2,3,4,5,5,2,1,2,5,2,1,3,5,4,6,8,4,0,2), ncol = 4)
M
```


<br>


<br>


Матрица центрированных значений
```{r, purl=TRUE}
Cent_M <- scale(M, center = TRUE, scale = FALSE) 
Cent_M
```


## Вычисление матрицы ковариации с помощью линейной алгебры
### Задание:  
Вычислите ковариационную матрицу с помощью методов линейной алгебры и сравните ее с матрицей, полученной с помощью функции `cov()`  

## Вычисление матрицы ковариации  с помощью линейной алгебры
### Решение: 

```{r, purl=FALSE}
# Вычисление вручную
Cov_M <- (t(Cent_M) %*% Cent_M)/(nrow(M) - 1)
Cov_M
```


```{r, purl=FALSE}
cov(M) # Стандартная функция R
```

## Вычисление матрицы ковариации  с помощью линейной алгебры

По главной диагонали ковариационной матрицы лежат квадраты стандартных отклонений Каждого из векторов (колонок, признаков) исходной матрицы

```{r, purl=TRUE}
diag(Cov_M)
```
Сравним
```{r, purl=TRUE}
apply(M, 2, FUN = function(x)sd(x)^2)
```


## Вычисление матрицы  корреляций с помощью линейной алгебры {.smaller .columns-2}

Для вычисления матрицы корреляций необходимо стандартизировать значения в исходной матрице

```{r, purl=TRUE}
Stand_M <- scale(M, center = TRUE, scale = TRUE)
Stand_M
```

<br>

<br>
<br>
<br>
<br>

```{r, purl=TRUE}
# Вычисление вручную
Cor_M <- (t(Stand_M) %*% Stand_M)/(nrow(M) - 1)
Cor_M
```

```{r}
cor(M) # Стандартная функция R
```


## Зачем нужна ковариационная матрица?

В ковариационной матрице содержится вся информация о варьировании признаков и о их взаимосвязи   
 
Свойства этой матрицы  позволяют раскладывать изменчивость на отдельные составляющие.  



# Level 7: Обращение (инверсия) матриц

## Обращение (инверсия) матриц {.smaller .columns-2}

В линейной алгебре нет процедуры деления. Вместо нее используют обращение матриц.

$$
\textbf{X}^{-1}\textbf{X} = \textbf{I}
$$


Обратить можно только такую матрицу, у которой определитель не равен нулю $$|\textbf{X}| \ne 0$$

Матрицы, у которых определитель $|\textbf{X}| = 0$ называются *сингулярными* матрицами они не могут быть инвертированы.

**Важное свойство**: Только квадратные матрицы имеют  обратную матрицу.

Поэтому для квадратных матриц справедливо $\textbf{X} \textbf{X}^{-1} = \textbf{X}^{-1} \textbf{X}$


**Важное свойство**: Если квадратная матрица состоит из ортогональных векторов (ортогональная матрица), то $\textbf{X}'  = \textbf{X}^{-1}$


## Решение в среде R
Создадим матрицу
```{r, echo=FALSE, purl=FALSE}
X <- matrix(c(seq(1, 8),10), nrow = 3, byrow = T)
X
```

Ее определитель
```{r, purl=FALSE}
det(X)
```

## Решение в среде R
Обратная матрица
```{r, , purl=FALSE}
solve(X)

```

По определению $\textbf{X}^{-1}\textbf{X} = \textbf{I}$
```{r, , purl=FALSE}
round(solve(X) %*% X )
```






# Level 8:  Собственные значения, собственные векторы и главные оси 

## Постановка проблемы

Одним из подходов к снижению размерности многомерного пространства признаков является представление исходной матрицы ($\textbf{Y}$) в **ортогональной форме**. 

В исходной матрице большинство векторов неортогональны, то есть взаимозависимые.  
Задача сводится к тому, чтобы заменить исходные векторы (признаки) новыми комплексными признаками, которые были бы взаимно независимыми, то есть ортогональными. 

Каждый из этих векторов берет на себя некоторую долю общей изменчивости. 




## Собственные числа и собственные векторы  {.smaller .columns-2}

Связь между векторами в исходной матрице $\textbf{Y}$ выражается через ковариационную  матрицу  $\textbf{S}$ или корреляционную матрицу ($\textbf {R}$).   

$\textbf{S}$ - это квадратная симметричная матрица.

Задача сводится к тому, чтобы из матрицы $\textbf{S}$


получить матрицу

$$ 
\bf{\Lambda} =  
\begin{pmatrix}
\lambda_{1} & 0 & \cdots & 0 \\
0 & \lambda_{2} & \cdots & 0 \\    
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & \lambda_{p}
\end{pmatrix}
$$

По главной диагонали лежат так называемые **собственные значения**, все остальные значения равны нулю  

Каждому собственному значению $\lambda_i$ставится в соответствие так называемый **собственный вектор**  $\textbf{u}_i$

Собственные векторы перпендикулярны друг другу (ортогональны)

Собственные векторы формируют матрицу собственных векторов $\textbf{U}$

## Связь между матрицами

Связь между матрицей $\bf{\Lambda}$ и $\textbf{S}$ выражается следующим уравнением

$$
\textbf{S}\textbf{U} = \textbf{U}\bf{\Lambda} 
$$


где   
$\textbf{S}$ - матрица ковариации    
$\textbf{U}$ - матрица собственных векторов     
$\bf{\Lambda}$ - Диагональная матрица собственных значений     

Тогда 

$$
\textbf{S} = \textbf{U}\bf{\Lambda}\textbf{U}^{-1} 
$$
Это позволяет раскладывать матрицу $\textbf{S}$  на составляющие 


## Главные оси {.smaller}

Во многих многомерных методах требуется найти оси максимального варьирования 

```{r, purl=TRUE}
set.seed(123456789)

x <- rnorm(1000, 50, 10)
y <- 10 * x + rnorm(1000, 0, 100)
XY <-data.frame(x = x, y = y)
qplot(XY$x, XY$y) + labs(x = "Переменная 1", y = "Переменная 2") + 
  geom_point(aes(x = mean(x), y = mean(y)), size = 4, color = "yellow")

```




## Главные оси {.smaller}

```{r, purl=TRUE}
qplot(XY$x, XY$y) + labs(x = "Переменная 1", y = "Переменная 2") + 
  geom_point(aes(x = mean(x), y = mean(y)), size = 4, color = "yellow") + 
  stat_ellipse(type = "norm", level = 0.999) + 
  geom_segment(aes(x = mean(x), y = mean(y), xend = (mean(x) + 3.5*sd(x)), yend = (mean(y) + 3.5*sd(y))),  arrow = ar, color = "blue", size = 2)

```


Главные оси проходят через центроид ($\mathbf{\mu} = [\mu_1, \mu_2 \dots \mu_n]$) и некоторую точку на поверхности эллипса ($\mathbf{y} = [y_1, y_2 \dots y_n]$).

Задача: найти такую точку.

Эта точка соответствует максимальной длине вектора, соединяющего центроид и данную точку.


## Нормализуем векторы {.smaller}

```{r, purl=TRUE}
x_norm <- XY$x/sqrt(sum(XY$x)^2)
y_norm <- XY$y/sqrt(sum(XY$y)^2)


XY_norm <- data.frame(x = x_norm, y = y_norm)


ggplot(XY_norm , aes(x = x, y = y)) + geom_point() +  
  geom_point(aes(x = mean(x), y = mean(y)), size = 4, color = "yellow")

```



## Центрируем нормализованные векторы {.smaller}


```{r, purl=TRUE}
XY_norm_cent <- as.data.frame(scale(XY_norm, center = TRUE, scale = FALSE))

ggplot(XY_norm_cent , aes(x = x, y = y)) + geom_point() +  
  geom_point(aes(x = mean(x), y = mean(y)), size = 4, color = "yellow")

```




## Находим ковариационную матрицу {.smaller}

```{r, purl=TRUE}
mXY_norm_cent <- as.matrix(XY_norm_cent)

Sxy_norm_cent <- t(mXY_norm_cent) %*% mXY_norm_cent /(nrow(mXY_norm_cent) - 1)

Sxy_norm_cent
```


## Находим собственные числа и собственные векторы {.smaller} 

```{r, purl=TRUE}
eig <- eigen(Sxy_norm_cent) # Стандартная функция R для извлечения собственных чисел и собственных векторов

Lambda <- eig$values # Собственные числа

Lambda

U <- eig$vectors # Собственные векторы

U
```

## Стандартизованные собственные векторы {.smaller}
Собственные векторы безразмерны.

Но их можно масштабировать, выразив в величинах стандартных отклонений $\textbf{U}_{scaled} = \textbf{U}\bf{\Lambda}^{1/2}$  

Почему так?

$\textbf{U}_{scaled} \textbf{U}'_{scaled} = [\textbf{U}\bf{\Lambda}^{1/2}][\textbf{U}\bf{\Lambda}^{1/2}]' = \textbf{U}\bf{\Lambda}\textbf{U}' = \textbf{U}\bf{\Lambda}\textbf{U}^{-1} = \textbf{S}$

Ковариационная матрица для стандартизованной матрицы собственных векторов совпадает с ковариационной матрицей исходной матрицы



```{r, purl=TRUE}
U_scaled <- U %*% sqrt(diag(Lambda)) #

(U %*% sqrt(diag(Lambda))) %*% t(U %*% sqrt(diag(Lambda)))

```


## Рисуем собственные векторы {.smaller}

```{r, purl=TRUE}

PC1 <- data.frame(x = c(mean(XY_norm_cent$x), U_scaled[1, 1]), 
                  y = c(mean(XY_norm_cent$y),  U_scaled[2,1]))

PC2 <- data.frame(x = c(mean(XY_norm_cent$x),  U_scaled[1, 2]), 
                  y = c(mean(XY_norm_cent$y),  U_scaled[2,2]))

ggplot(XY_norm_cent, aes(x = x, y = y)) + geom_point() +  
  geom_point(aes(x = mean(x), y = mean(y)), size = 4, color = "yellow") + 
  geom_line(data = PC1, aes(x = x, y = y), color = "yellow", size = 1)  +
  geom_line(data = PC2, aes(x = x, y = y), color = "yellow", size = 1) +
  coord_equal()


```

## Рисуем главные оси {.smaller .columns-2}

<img src="images/Principal_axes_Legendre_and_Legendre.png" style="height:350px;">

<small>Из  Legendre & Legendre, 2012</small>

<!-- ![](images/Principal_axes_Legendre_and_Legendre.png) -->

$$
\cos(\alpha_{11}) = u_{11}\\
\cos(\alpha_{21}) = u_{21}\\
\cos(\alpha_{12}) = u_{12}\\
\cos(\alpha_{22}) = u_{22}
$$


```{r}
U
```



```{r,  fig.height=3, purl=TRUE}
ggplot(XY_norm_cent, aes(x = x, y = y)) + geom_point() +  
  geom_point(aes(x = mean(x), y = mean(y)), size = 4, color = "yellow") + 
  geom_line(data = PC1, aes(x = x, y = y), color = "yellow", size = 1.5)  +
  geom_line(data = PC2, aes(x = x, y = y), color = "yellow", size = 1.5) +
  coord_equal() + geom_abline(slope = tan(acos(U[1,1])), color = "blue") +
  geom_abline(slope = tan(acos(U[1,1]) + acos(U[2,1]) + acos(U[2,2])), color = "blue") 

```

## Вращение осей {.smaller .columns-2}

<img src="images/Principal_axes_Legendre_and_Legendre.png" style="height:350px;">

<small>Из  Legendre & Legendre, 2012</small>

Вращающая матрица
```{r, purl=TRUE}
angle <- - acos(U[1,1]) #Отрицательный угол, так как поворачиваем оси по часовой стрелке

Rot <- matrix(c(cos(angle), sin(angle), 
                -sin(angle), cos(angle)), nrow = 2)
Rot
```

## Вращение осей {.smaller}


```{r, purl=TRUE}
XY_norm_cent_rot <- as.data.frame(t(Rot %*% t(mXY_norm_cent)))

ggplot(XY_norm_cent, aes(x = x, y = y)) + 
  geom_point(color = "gray") + 
  geom_point(data = XY_norm_cent_rot, aes(x = V1, y = V2)) + 
  labs(x = "Первая главная ось", y = "Вторая главная ось")


```




# Level 9: Сингулярное разложение матриц (Singular value decomposition)

## Теорема Экарта-Янга {.smaller}
Любую прямоугольную матрицу $\textbf{Y}$ можно представить в виде произведения трех матриц: 

$$
\textbf{Y}_{n \times p} = \textbf{U}_{n \times p} \textbf{D}_{p \times p} \textbf{V}'_{p \times p} 
$$

То есть можно найти три "вспомогательных" матрицы, через которые можно выразить любую другую матрицу. 

---

$$
\textbf{Y}_{n \times p} = \textbf{U}_{n \times p} \textbf{D}_{p \times p} \textbf{V}'_{p \times p} 
$$

Здесь    

$\textbf{Y}_{n \times p}$ - любая прямоугольная матрица $\begin{pmatrix}a_{11} & a_{12} & \cdots & a_{1c} \\ a_{21} & a_{22} & \cdots & a_{2p} \\  \vdots & \vdots & \ddots & \vdots \\ a_{r1} & a_{n2} & \cdots & a_{np} \end{pmatrix}$   

$\textbf{D}_{p \times p}$ - диагональная матрица $\begin{pmatrix} d_{11} & 0 & \cdots & 0 \\ 0 & d_{22} & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & d_{pp} \end{pmatrix}$

По главной диагонали располагаются "особые" числа, называющиеся **сингулярными числами**. Сингулярные числа ранжируются от большего к меньшему.

$\textbf{U}_{n \times p}$ и $\textbf{V}_{p \times p}$ - левая и правая матрицы сингулярных векторов.

## Сингулярное разложение матрицы средствами R {.smaller}
```{r, purl=TRUE}
set.seed(123456789)
B <- matrix(round(runif(50, 1, 5))  , byrow = T, ncol=5) #Некоторая матрица
SVD <- svd(B) #Сингулярное Разложение матрицы B с помощью функции svd()
V <- SVD$v #"Вспомогательная" матрица - левые сингулярные векторы
D <- SVD$d #Вектор сингулярных чисел
U <- SVD$u #"Вспомогательная" матрица - правые сингулярные векторы 


```

Вычислим $\textbf{V} \textbf{D} \textbf{U}'$

```{r, purl=TRUE}
U %*% diag(D) %*% t(V) 
```

## Задание

Вычислите матрицу, которая получится при использовании только 1 и 2 сингулярного числа для матрицы $\textbf{B}$, использованной на предыдущем слайде.


## Решение
```{r, purl=TRUE}
U[,1:2] %*% diag(D[1:2]) %*% t(V[,1:2]) 

```


## Важное свойство сингулярных чисел

Если вычислить матрицу на основе не всех, а части сингулярных чисел, то новая матрица будет подобна исходной матрице.


```{r, echo=FALSE, purl=FALSE}
library(ggplot2)
Dat <- data.frame(Init = rep(as.vector(B), 4), SingValue = rep(2:5, each = length(as.vector(B))), Calc = c(as.vector((U[,1:2] %*% diag(D[1:2]) %*% t(V[,1:2]))), as.vector((U[,1:3] %*% diag(D[1:3]) %*% t(V[,1:3]))), as.vector((U[,1:4] %*% diag(D[1:4]) %*% t(V[,1:4]))), as.vector((U[,1:5] %*% diag(D[1:5]) %*% t(V[,1:5]))))) 

ggplot(Dat, aes(x = Init, y = Calc)) + geom_point(size = 2) + labs(x = "значения в исходной матрицы", y = "значения в редуцированной матрице") + facet_wrap(~SingValue) + geom_abline(slope = 1)  

```


## Применение свойства сингулярных чисел в сжатии изображений

```{r,echo=FALSE, purl=TRUE}
load("data/face.rda")
gg_face <- function(x) {
  library(reshape)
  library(ggplot2)
    rotate <- function(x) t(apply(x, 2, rev))
  dd <- rotate(x)
  ddd <- melt(dd)
  ggplot(ddd, aes(X1, X2)) + geom_tile(aes(fill = value)) + scale_fill_gradient(low = "darkblue",   high =  "white" ) + coord_equal()
}
gg_face(faceData)

```

<small> Пример взят из курса лекций "Data Analysis" by Jeffrey Leek  

(https://github.com/jtleek/dataanalysis/tree/master/week3)</small>




## Произведем сингулярное разложение матрицы `faceData` {.smaller}

```{r, purl=TRUE}
SVD_face <- svd(faceData)

U <- SVD_face$u
D <- SVD_face$d
V <- SVD_face$v

```

## Рекоструируем изображение, используя только часть информации

```{r, purl=TRUE}
reduction <- function(x) U[,1:x] %*% diag(D[1:x]) %*% t(V[, 1:x])
gg_face(reduction(4))
```



## Применение SVD в биологических исследованиях

SVD - это метод, на котором основаны разные типы анализа, связанные со снижением размерности: PCA, CA, CCA, RDA.

О них в следующех лекциях

## Summary
- Линейная алгебра позволяет решать самые разные типы задач.
- Матричные методы лежат в основе очень многих типов анализа.
- В основе многих методов снижения размерности лежит SVD.

## Что почитать
* Legendre P., Legendre L. (2012) Numerical ecology. Second english edition. Elsevier, Amsterdam. Глава 2. Matrix algebra: a summary.

# Not The End
![](images/matrix_2.jpg)

