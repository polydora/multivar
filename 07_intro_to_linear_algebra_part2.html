<!DOCTYPE html>
<html>
<head>
  <title>Краткое введение в мир линейной алгебры</title>

  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <meta name="generator" content="pandoc" />




  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">

  <base target="_blank">

  <script type="text/javascript">
    var SLIDE_CONFIG = {
      // Slide settings
      settings: {
                title: 'Краткое введение в мир линейной алгебры',
                        useBuilds: true,
        usePrettify: true,
        enableSlideAreas: true,
        enableTouch: true,
                        favIcon: '07_intro_to_linear_algebra_part2_files/logo.png',
              },

      // Author information
      presenters: [
            {
        name:  'Вадим Хайтов, Марина Варфоломеева' ,
        company: '',
        gplus: '',
        twitter: '',
        www: '',
        github: ''
      },
            ]
    };
  </script>

  <script src="site_libs/header-attrs-2.11/header-attrs.js"></script>
  <link href="site_libs/ioslides-13.5.1/fonts/fonts.css" rel="stylesheet" />
  <link href="site_libs/ioslides-13.5.1/theme/css/default.css" rel="stylesheet" />
  <link href="site_libs/ioslides-13.5.1/theme/css/phone.css" rel="stylesheet" />
  <script src="site_libs/ioslides-13.5.1/js/modernizr.custom.45394.js"></script>
  <script src="site_libs/ioslides-13.5.1/js/prettify/prettify.js"></script>
  <script src="site_libs/ioslides-13.5.1/js/prettify/lang-r.js"></script>
  <script src="site_libs/ioslides-13.5.1/js/prettify/lang-yaml.js"></script>
  <script src="site_libs/ioslides-13.5.1/js/hammer.js"></script>
  <script src="site_libs/ioslides-13.5.1/js/slide-controller.js"></script>
  <script src="site_libs/ioslides-13.5.1/js/slide-deck.js"></script>

  <style type="text/css">

    b, strong {
      font-weight: bold;
    }

    em {
      font-style: italic;
    }

    summary {
      display: list-item;
    }

    slides > slide {
      -webkit-transition: all 0.4s ease-in-out;
      -moz-transition: all 0.4s ease-in-out;
      -o-transition: all 0.4s ease-in-out;
      transition: all 0.4s ease-in-out;
    }

    .auto-fadein {
      -webkit-transition: opacity 0.6s ease-in;
      -webkit-transition-delay: 0.4s;
      -moz-transition: opacity 0.6s ease-in 0.4s;
      -o-transition: opacity 0.6s ease-in 0.4s;
      transition: opacity 0.6s ease-in 0.4s;
      opacity: 0;
    }
/* https://github.com/ropensci/plotly/pull/524#issuecomment-468142578 */
slide:not(.current) .plotly.html-widget{
  display: block;
}

    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
            pre > code.sourceCode { white-space: pre; position: relative; }
            pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
            pre > code.sourceCode > span:empty { height: 1.2em; }
            .sourceCode { overflow: visible; }
            code.sourceCode > span { color: inherit; text-decoration: inherit; }
            div.sourceCode { margin: 1em 0; }
            pre.sourceCode { margin: 0; }
            @media screen {
            div.sourceCode { overflow: auto; }
            }
            @media print {
            pre > code.sourceCode { white-space: pre-wrap; }
            pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
            }
            pre.numberSource code
              { counter-reset: source-line 0; }
            pre.numberSource code > span
              { position: relative; left: -4em; counter-increment: source-line; }
            pre.numberSource code > span > a:first-child::before
              { content: counter(source-line);
                position: relative; left: -1em; text-align: right; vertical-align: baseline;
                border: none; display: inline-block;
                -webkit-touch-callout: none; -webkit-user-select: none;
                -khtml-user-select: none; -moz-user-select: none;
                -ms-user-select: none; user-select: none;
                padding: 0 4px; width: 4em;
                color: #aaaaaa;
              }
            pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
            div.sourceCode
              {   }
            @media screen {
            pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
            }
            code span.al { color: #ff0000; font-weight: bold; } /* Alert */
            code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
            code span.at { color: #7d9029; } /* Attribute */
            code span.bn { color: #40a070; } /* BaseN */
            code span.bu { color: #008000; } /* BuiltIn */
            code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
            code span.ch { color: #4070a0; } /* Char */
            code span.cn { color: #880000; } /* Constant */
            code span.co { color: #60a0b0; font-style: italic; } /* Comment */
            code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
            code span.do { color: #ba2121; font-style: italic; } /* Documentation */
            code span.dt { color: #902000; } /* DataType */
            code span.dv { color: #40a070; } /* DecVal */
            code span.er { color: #ff0000; font-weight: bold; } /* Error */
            code span.ex { } /* Extension */
            code span.fl { color: #40a070; } /* Float */
            code span.fu { color: #06287e; } /* Function */
            code span.im { color: #008000; font-weight: bold; } /* Import */
            code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
            code span.kw { color: #007020; font-weight: bold; } /* Keyword */
            code span.op { color: #666666; } /* Operator */
            code span.ot { color: #007020; } /* Other */
            code span.pp { color: #bc7a00; } /* Preprocessor */
            code span.sc { color: #4070a0; } /* SpecialChar */
            code span.ss { color: #bb6688; } /* SpecialString */
            code span.st { color: #4070a0; } /* String */
            code span.va { color: #19177c; } /* Variable */
            code span.vs { color: #4070a0; } /* VerbatimString */
            code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
        
    slides > slide:not(.nobackground):before {
      font-size: 12pt;
      content: "";
      position: absolute;
      bottom: 20px;
      left: 60px;
      background: url(07_intro_to_linear_algebra_part2_files/logo.png) no-repeat 0 50%;
      -webkit-background-size: 30px 30px;
      -moz-background-size: 30px 30px;
      -o-background-size: 30px 30px;
      background-size: 30px 30px;
      padding-left: 40px;
      height: 30px;
      line-height: 1.9;
    }
  </style>

  <link rel="stylesheet" href="assets/my_styles.css" type="text/css" />

</head>

<body style="opacity: 0">

<slides class="layout-widescreen">

  <slide class="title-slide segue nobackground">
        <aside class="gdbar"><img src="07_intro_to_linear_algebra_part2_files/logo.png"></aside>
        <!-- The content of this hgroup is replaced programmatically through the slide_config.json. -->
    <hgroup class="auto-fadein">
      <h1 data-config-title><!-- populated from slide_config.json --></h1>
      
      <p data-config-presenter><!-- populated from slide_config.json --></p>
          </hgroup>
  </slide>

<slide class=""><hgroup><h2>Вы сможете</h2></hgroup><article  id="вы-сможете">

<ul>
<li>Найти собственные значения и собственные векторы матрицы и объяснить их смысл.</li>
<li>Переходить из исходной системы системы координат в систему координат, связанную с главными осями.</li>
<li>Оценивать информативность главных осей.</li>
<li>Изображать исходные данные в новой системе координат.</li>
<li>Проводить сингулярное разложение любой матрицы.</li>
<li>Убирать &ldquo;лишнюю&rdquo; информацию из матрицы.</li>
</ul>

</article></slide><slide class="segue dark nobackground level1"><hgroup class = 'auto-fadein'><h2>Level 8: Собственные значения, собственные векторы и главные оси</h2></hgroup><article  id="level-8-собственные-значения-собственные-векторы-и-главные-оси">

</article></slide><slide class=""><hgroup><h2>Проблема</h2></hgroup><article  id="проблема" class="columns-2 smaller">

<p>Пусть есть некоторый набор данных</p>

<pre class = 'prettyprint lang-r'>set.seed(123456789)

x1 &lt;- rnorm(500, 30, 4)
y1 &lt;- rnorm(500, 700, 50)
x2 &lt;- rnorm(500, 40, 5)
y2 &lt;- 10 * x2 + 200 + rnorm(500, 0, 100)

XY &lt;-data.frame(x = c(x1, x2), y = c(y1, y2) )</pre>

<p>Проекция на каждую из осей может не дать информацию о структуре многомерного облака. НО! Для двумерного случая эта проблема не так важна.</p>

<p><img src="07_intro_to_linear_algebra_part2_files/figure-html/unnamed-chunk-5-1.png" width="480" style="display: block; margin: auto;" /></p>

</article></slide><slide class=""><hgroup><h2>Как решить проблему разделения облаков?</h2></hgroup><article  id="как-решить-проблему-разделения-облаков">

<p>Можно ввести новую систему координат</p>

<p><img src="07_intro_to_linear_algebra_part2_files/figure-html/unnamed-chunk-6-1.png" width="672" style="display: block; margin: auto;" /></p>

<p>Проекции точек на новую ось дадут две совокупности.</p>

</article></slide><slide class=""><hgroup><h2>Переформулировка проблемы</h2></hgroup><article  id="переформулировка-проблемы" class="columns-2 smaller">

<p>В исходной матрице векторы могут быть неортогональны</p>

<p>Между векторами существует корреляция</p>

<h3>Корреляция между переменными</h3>

<pre class = 'prettyprint lang-r'>XY_cent &lt;- as.data.frame(scale(XY, 
                               center = T, 
                               scale = F))

head(XY_cent)</pre>

<pre >##         x     y
## 1  -3.028 39.64
## 2  -3.464 46.17
## 3   0.614 91.89
## 4  -7.937 14.63
## 5  -7.521  9.19
## 6 -11.298 62.74</pre>

<pre class = 'prettyprint lang-r'>Cos_alpha &lt;- 
  with(XY_cent,
       (x %*% y) / 
         (norm(t(x), type = &quot;F&quot;) * 
            norm(t(y), type = &quot;F&quot;))  ) 

Cos_alpha</pre>

<pre >##       [,1]
## [1,] 0.212</pre>

<pre class = 'prettyprint lang-r'>cor(XY$x, XY$y)</pre>

<pre >## [1] 0.212</pre>

<p>Наличие корреляции между векторами - это проблема, так как в каждой из переменных &ldquo;спрятана&rdquo; информация и о другой переменной.</p>

<p>Задача сводится к тому, чтобы заменить исходные векторы (признаки) новыми комплексными признаками, которые были бы взаимно независимыми, то есть ортогональными.</p>

<p>Каждый из этих векторов несет определенную часть информации об исходной системе векторов (берет на себя некоторую долю общей дисперсии).</p>

</article></slide><slide class=""><hgroup><h2>Собственные числа и собственные векторы</h2></hgroup><article  id="собственные-числа-и-собственные-векторы" class="smaller columns-2">

<p>Связь между векторами в исходной матрице \(\textbf{Y}\) выражается через ковариационную матрицу \(\textbf{S}\) (или корреляционную матрицу \(\textbf {R}\)).</p>

<p>\(\textbf{S}\) - это квадратная симметричная матрица.</p>

<p>Задача сводится к тому, чтобы разложить матрицу \(\textbf{S}\) на две части</p>

<p><strong><em>Первая часть.</em></strong> Вектор <strong>собственных значений</strong> \(\bf{\lambda}\), или, что то же самое, матрица \(\bf{\Lambda}\)</p>

<p>\[ 
\bf{\Lambda} =  
\begin{pmatrix}
\lambda_{1} &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; \lambda_{2} &amp; \cdots &amp; 0 \\    
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; \lambda_{p}
\end{pmatrix}
\]</p>

<p>По главной диагонали лежат <strong>собственные значения</strong>, все остальные значения равны нулю</p>

<p><strong><em>Вторая часть.</em></strong> Каждому собственному значению \(\lambda_i\) ставится в соответствие так называемый <strong>собственный вектор</strong> \(\textbf{u}_i\). Совокупность собственных векторов формирует матрицу собственных векторов \(\textbf{U}\). Собственные векторы перпендикулярны друг другу (ортогональны).</p>

</article></slide><slide class=""><hgroup><h2>Связь между матрицами</h2></hgroup><article  id="связь-между-матрицами">

<p>Связь между матрицей \(\bf{\Lambda}\) и \(\textbf{S}\) выражается следующим уравнением</p>

<p>\[
\textbf{S}\textbf{U} = \textbf{U}\bf{\Lambda} 
\]</p>

<p>где<br/>\(\textbf{S}\) - матрица ковариации<br/>\(\textbf{U}\) - матрица собственных векторов<br/>\(\bf{\Lambda}\) - Диагональная матрица собственных значений</p>

</article></slide><slide class=""><hgroup><h2>Разложение ковариационной матрицы</h2></hgroup><article  id="разложение-ковариационной-матрицы">

<p>Если</p>

<p>\[
\textbf{S}\textbf{U} = \textbf{U}\bf{\Lambda} 
\]</p>

<p>то</p>

<p>\[
\textbf{S} = \textbf{U}\bf{\Lambda}\textbf{U}^{-1} 
\] НО!</p>

<p><strong>Важное свойство</strong>: Если квадратная матрица состоит из ортогональных векторов (ортогональная матрица), то \(\textbf{X}^\prime = \textbf{X}^{-1}\)</p>

<p>Тогда</p>

<p>\[
\textbf{S} = \textbf{U}\bf{\Lambda}\textbf{U}^\prime 
\]</p>

</article></slide><slide class=""><hgroup><h2>Разложение ковариационной матрицы</h2></hgroup><article  id="разложение-ковариационной-матрицы-1">

<p>\[
\textbf{S} = \textbf{U}\bf{\Lambda}\textbf{U}&#39; 
\]</p>

<p>Это позволяет раскладывать матрицу \(\textbf{S}\) на составляющие (<strong>eigenvalues decomposition</strong>, или спектральное разложение).</p>

<p>Каждое собственное число в матрице \(\bf{\Lambda}\) (и соответствующий ему вектор из матрицы \(\textbf{U}\)) несет часть информации об исходной матрице \(\textbf{S}\).</p>

<p>Разложение матрицы позволяет использовать не всю информацию, а только ее наиболее важную часть.</p>

</article></slide><slide class=""><hgroup><h2>Геомтерическая интерпретация: <em>Главные оси</em></h2></hgroup><article  id="геомтерическая-интерпретация-главные-оси" class="smaller">

<p>Мерой количества информации может служить изменчивость, то есть дисперсия. Поэтому очень правильно было бы перейти к системе координат (осям), которые несли бы максимум информации. Это означает, что варьирование вдоль этих осей должно быть максимальным.</p>

<p>Именно это и позволяет сделать спектральное разложение ковариационной матрицы.</p>

</article></slide><slide class=""><hgroup><h2>Геомтерическая интерпретация: <em>Главные оси</em></h2></hgroup><article  id="геомтерическая-интерпретация-главные-оси-1" class="smaller">

<p>Во многих многомерных методах требуется найти оси максимального варьирования</p>

<p><img src="07_intro_to_linear_algebra_part2_files/figure-html/unnamed-chunk-10-1.png" width="672" style="display: block; margin: auto;" /></p>

<p>Главные оси проходят через центроид (\(\mathbf{\mu} = [\mu_1, \mu_2 \dots \mu_n]\)) и некоторую точку на поверхности эллипса (\(\mathbf{var} = [var_1, var_2 \dots var_n]\)).</p>

<p>Задача: найти такую точку.</p>

<p>Эта точка соответствует максимальной длине вектора, соединяющего центроид и данную точку.</p>

</article></slide><slide class=""><hgroup><h2>Нормализуем векторы</h2></hgroup><article  id="нормализуем-векторы" class="smaller">

<pre class = 'prettyprint lang-r'>x_norm &lt;- XY$x/sqrt(sum(XY$x)^2)
y_norm &lt;- XY$y/sqrt(sum(XY$y)^2)


XY_norm &lt;- data.frame(x = x_norm, y = y_norm)


ggplot(XY_norm , aes(x = x, y = y)) + geom_point() +  
  geom_point(aes(x = mean(x), y = mean(y)), size = 4, color = &quot;yellow&quot;)  + 
  labs(x = &quot;Нормализованная Переменная 1&quot;, y = &quot;Нормализованная Переменная 2&quot;)</pre>

<p><img src="07_intro_to_linear_algebra_part2_files/figure-html/unnamed-chunk-11-1.png" width="672" style="display: block; margin: auto;" /></p>

</article></slide><slide class=""><hgroup><h2>Центрируем нормализованные векторы</h2></hgroup><article  id="центрируем-нормализованные-векторы" class="smaller">

<pre class = 'prettyprint lang-r'>XY_norm_cent &lt;- as.data.frame(scale(XY_norm, center = TRUE, scale = FALSE))

ggplot(XY_norm_cent , aes(x = x, y = y)) + geom_point() + geom_vline(xintercept = 0) + geom_hline(yintercept = 0) +   
  geom_point(aes(x = mean(x), y = mean(y)), size = 4, color = &quot;yellow&quot;) + 
  labs(x = &quot;Центрированная Нормализованная Переменная 1&quot;, y = &quot;Центрированная Нормализованная Переменная 2&quot;)</pre>

<p><img src="07_intro_to_linear_algebra_part2_files/figure-html/unnamed-chunk-12-1.png" width="672" style="display: block; margin: auto;" /></p>

</article></slide><slide class=""><hgroup><h2>Находим ковариационную матрицу</h2></hgroup><article  id="находим-ковариационную-матрицу" class="smaller">

<p>\[
\textbf{S} = \frac{1}{n - 1} \textbf{Y}_{centered}&#39;\textbf{Y}_{centered}
\]</p>

<pre class = 'prettyprint lang-r'>mXY_norm_cent &lt;- as.matrix(XY_norm_cent)

Sxy_norm_cent &lt;- t(mXY_norm_cent) %*% mXY_norm_cent /(nrow(mXY_norm_cent) - 1)

Sxy_norm_cent</pre>

<pre >##               x             y
## x 0.00000003612 0.00000000548
## y 0.00000000548 0.00000001856</pre>

</article></slide><slide class=""><hgroup><h2>Находим собственные числа и собственные векторы</h2></hgroup><article  id="находим-собственные-числа-и-собственные-векторы" class="smaller">

<pre class = 'prettyprint lang-r'>eig &lt;- eigen(Sxy_norm_cent) # Стандартная функция R для извлечения собственных чисел и собственных векторов

Lambda &lt;- eig$values # Собственные числа

Lambda</pre>

<pre >## [1] 0.0000000377 0.0000000170</pre>

<pre class = 'prettyprint lang-r'>U &lt;- eig$vectors # Собственные векторы

U</pre>

<pre >##        [,1]   [,2]
## [1,] -0.961  0.275
## [2,] -0.275 -0.961</pre>

</article></slide><slide class=""><hgroup><h2>Собственные векторы ортогональны</h2></hgroup><article  id="собственные-векторы-ортогональны">

<p>Проверим</p>

<pre class = 'prettyprint lang-r'>U[,1] %*% U[,2]</pre>

<pre >##      [,1]
## [1,]    0</pre>

</article></slide><slide class=""><hgroup><h2>Стандартизованные собственные векторы</h2></hgroup><article  id="стандартизованные-собственные-векторы" class="smaller">

<p>Собственные векторы безразмерны.</p>

<p>Но их можно масштабировать, выразив в величинах стандартных отклонений \(\textbf{U}_{scaled} = \textbf{U}\bf{\Lambda}^{1/2}\)</p>

<p>Почему так?</p>

<p>\(\textbf{U}_{scaled} \textbf{U}&#39;_{scaled} = [\textbf{U}\bf{\Lambda}^{1/2}][\textbf{U}\bf{\Lambda}^{1/2}]&#39; = \textbf{U}\bf{\Lambda}\textbf{U}&#39; = \textbf{U}\bf{\Lambda}\textbf{U}^{-1} = \textbf{S}\)</p>

<p>Ковариационная матрица для стандартизованной матрицы собственных векторов совпадает с ковариационной матрицей исходной матрицы</p>

</article></slide><slide class=""><hgroup><h2>Стандартизованные собственные векторы</h2></hgroup><article  id="стандартизованные-собственные-векторы-1" class="smaller">

<p>Проверим…</p>

<pre class = 'prettyprint lang-r'>U_scaled &lt;- U %*% sqrt(diag(Lambda)) #

(U %*% sqrt(diag(Lambda))) %*% t(U %*% sqrt(diag(Lambda)))</pre>

<pre >##               [,1]          [,2]
## [1,] 0.00000003612 0.00000000548
## [2,] 0.00000000548 0.00000001856</pre>

<p>Сравним</p>

<pre class = 'prettyprint lang-r'>Sxy_norm_cent</pre>

<pre >##               x             y
## x 0.00000003612 0.00000000548
## y 0.00000000548 0.00000001856</pre>

</article></slide><slide class=""><hgroup><h2>Рисуем собственные векторы</h2></hgroup><article  id="рисуем-собственные-векторы" class="smaller">

<pre class = 'prettyprint lang-r'>PC1 &lt;- data.frame(x = c(mean(XY_norm_cent$x), U_scaled[1, 1]), 
                  y = c(mean(XY_norm_cent$y),  U_scaled[2,1]))

PC2 &lt;- data.frame(x = c(mean(XY_norm_cent$x),  U_scaled[1, 2]), 
                  y = c(mean(XY_norm_cent$y),  U_scaled[2,2]))

ggplot(XY_norm_cent, aes(x = x, y = y)) + geom_point() +  
  geom_point(aes(x = mean(x), y = mean(y)), size = 4, color = &quot;yellow&quot;) + 
  geom_line(data = PC1, aes(x = x, y = y), color = &quot;yellow&quot;, size = 1)  +
  geom_line(data = PC2, aes(x = x, y = y), color = &quot;yellow&quot;, size = 1) +
  coord_equal() </pre>

<p><img src="07_intro_to_linear_algebra_part2_files/figure-html/unnamed-chunk-18-1.png" width="672" style="display: block; margin: auto;" /></p>

<p>Собственные векторы задают &ldquo;опору&rdquo; для новых осей координат</p>

</article></slide><slide class=""><hgroup><h2>Рисуем главные оси</h2></hgroup><article  id="рисуем-главные-оси" class="smaller columns-2">

<!-- <img src="images/Principal_axes_Legendre_and_Legendre.png" style="height:350px;"> -->

<!-- <small>Из  Legendre & Legendre, 2012</small> -->

<!-- ![](images/Principal_axes_Legendre_and_Legendre.png) -->

<!-- $$ -->

<!-- \cos(\alpha_{11}) = u_{11}\\ -->

<!-- \cos(\alpha_{21}) = u_{21}\\ -->

<!-- \cos(\alpha_{12}) = u_{12}\\ -->

<!-- \cos(\alpha_{22}) = u_{22} -->

<!-- $$ -->

<p>Матрица собственных векторов</p>

<pre class = 'prettyprint lang-r'>U</pre>

<pre >##        [,1]   [,2]
## [1,] -0.961  0.275
## [2,] -0.275 -0.961</pre>

<pre class = 'prettyprint lang-r'>  ggplot(XY_norm_cent, aes(x = x, y = y)) + 
  geom_point() +  
  geom_point(aes(x = mean(x), y = mean(y)), 
             size = 4, color = &quot;yellow&quot;) + 
  geom_line(data = PC1, aes(x = x, y = y), 
            color = &quot;yellow&quot;, size = 1.5)  +
  geom_line(data = PC2, aes(x = x, y = y), 
            color = &quot;yellow&quot;, size = 1.5) +
  coord_equal() + 
    geom_abline(slope = (U[2,1])/(U[1,1]),
              color = &quot;blue&quot;) + 
geom_abline(slope = (U[2,2])/(U[1,2]),   
              color = &quot;blue&quot;) </pre>

<p><img src="07_intro_to_linear_algebra_part2_files/figure-html/unnamed-chunk-20-1.png" width="672" style="display: block; margin: auto auto auto 0;" /></p>

<!-- <br> -->

<!-- <br> -->

<!-- <br> -->

<!-- ```{r} -->

<!-- Pl_main_axis -->

<!-- ``` -->

<p>Для удобства восприятия лучше повернуть систему координат так, чтобы ось максимального варьирования была расположена горизонтально.</p>

</article></slide><slide class=""><hgroup><h2>Вращение осей</h2></hgroup><article  id="вращение-осей" class="smaller columns-2">

<p><img src="images/Principal_axes_Legendre_and_Legendre.png" style="height:350px;"></p>

<p><small>Из Legendre &amp; Legendre, 2012</small></p>

<p>\[
\cos(\alpha_{11}) = u_{11}\\
\cos(\alpha_{21}) = u_{21}\\
\cos(\alpha_{12}) = u_{12}\\
\cos(\alpha_{22}) = u_{22}
\]</p>

<p>Вращающая матрица</p>

<pre class = 'prettyprint lang-r'>angle &lt;- -1 * acos(U[1,1]) #Отрицательный угол, так как 
# поворачиваем оси по часовой стрелке

Rot &lt;- matrix(c(cos(angle), sin(angle), 
                -sin(angle), cos(angle)), nrow = 2)
Rot</pre>

<pre >##        [,1]   [,2]
## [1,] -0.961  0.275
## [2,] -0.275 -0.961</pre>

</article></slide><slide class=""><hgroup><h2>Вращение осей</h2></hgroup><article  id="вращение-осей-1" class="smaller">

<p>Новые оси</p>

<pre class = 'prettyprint lang-r'>XY_norm_cent_rot &lt;- as.data.frame(t(Rot %*% t(mXY_norm_cent)))

Pl_main_axis_rotated &lt;- ggplot(XY_norm_cent, aes(x = x, y = y)) + 
  geom_point(color = &quot;gray&quot;) + 
  geom_point(data = XY_norm_cent_rot, aes(x = V1, y = V2)) + 
  labs(x = &quot;Первая главная ось&quot;, y = &quot;Вторая главная ось&quot;) +
  geom_hline(yintercept = 0) + 
  geom_vline(xintercept = 0)

Pl_main_axis_rotated</pre>

<p><img src="07_intro_to_linear_algebra_part2_files/figure-html/unnamed-chunk-22-1.png" width="672" style="display: block; margin: auto;" /></p>

</article></slide><slide class=""><hgroup><h2>Вращение осей</h2></hgroup><article  id="вращение-осей-2" class="smaller">

<p>Новые и старые главные оси</p>

<pre class = 'prettyprint lang-r'>Pl_main_axis_rotated +
  geom_point(aes(x = mean(x), y = mean(y)), 
             size = 4, color = &quot;yellow&quot;) +
  geom_line(data = PC1, aes(x = x, y = y), 
            color = &quot;yellow&quot;, size = 1.5)  +
  geom_line(data = PC2, aes(x = x, y = y), 
            color = &quot;yellow&quot;, size = 1.5) +
  coord_equal() + 
  geom_abline(slope = (U[2,1])/(U[1,1]),
              color = &quot;blue&quot;) + 
geom_abline(slope = (U[2,2])/(U[1,2]),   
              color = &quot;blue&quot;) </pre>

<p><img src="07_intro_to_linear_algebra_part2_files/figure-html/unnamed-chunk-23-1.png" width="672" style="display: block; margin: auto;" /></p>

</article></slide><slide class=""><hgroup><h2>Проблема решена!</h2></hgroup><article  id="проблема-решена" class="smaller">

<p><img src="07_intro_to_linear_algebra_part2_files/figure-html/unnamed-chunk-25-1.png" width="672" style="display: block; margin: auto;" /></p>

<p><img src="07_intro_to_linear_algebra_part2_files/figure-html/unnamed-chunk-26-1.png" width="672" style="display: block; margin: auto;" /></p>

<p>Кроме того! Теперь мы знаем, что первая главная ось отвечает за определенную долю дисперсии.</p>

<pre class = 'prettyprint lang-r'>Lambda</pre>

<pre >## [1] 0.0000000377 0.0000000170</pre>

<pre class = 'prettyprint lang-r'>Lambda[1]/sum(Lambda)</pre>

<pre >## [1] 0.689</pre>

</article></slide><slide class=""><hgroup><h2>Главное о снижении размерности</h2></hgroup><article  id="главное-о-снижении-размерности" class="smaller">

<p>1.Информацию об исходной матрице \(\textbf{Y}\) можно сконцентрировать в ковариационной матрице</p>

<p>\[
\textbf{S} = \frac{1}{n - 1} \textbf{Y}_{centered}&#39;\textbf{Y}_{centered}
\]</p>

<p>2.Матрицу ковариации можно разложить на собственные числа и собственные векторы</p>

<p>\[
\textbf{S} = \textbf{U}\bf{\Lambda}\textbf{U}^{-1} 
\]</p>

<p>3.Собственные векторы ортогональны и позволяют созадать новую систему координат. Новые оси (главные оси) &ldquo;опираются&rdquo; на собственные векторы.</p>

<p>4.Новые координаты точек могут быть вычислены так</p>

<p>\[
\bf {Y_{new} = Y \times U}
\]</p>

<p>5.Каждая ось в новой системе координат несет некоторую часть информации об исходной системе координат. Информативность осей определяется величиной собственных чисел.</p>

<p>6.Вместо многомерной системы координат (n осей) мы можем использовать всего несколько (например, две) ниаболее информативные главные оси.</p>

</article></slide><slide class=""><hgroup><h2>Задание</h2></hgroup><article  id="задание">

<p>Исследуйте структуру матрицы <em>X</em>.</p>

<pre class = 'prettyprint lang-r'>set.seed(123456789)

x1 &lt;- c(rnorm(250, 30, 4), rnorm(250, 60, 4)) 
x2 &lt;- rnorm(500, 70, 50)
x3 &lt;- rnorm(500, 40, 5)
x4 &lt;- c(rnorm(100, 10, 5), rnorm(100, 40, 5), rnorm(100, 70, 5), rnorm(200, 100, 5)) 
x5 &lt;- c(rnorm(250, 50, 5), rnorm(250, 100, 5)) 


X &lt;-data.frame(x1 = x1, x2 = x2, x3 = x3, x4 = x4, x5 = x5)</pre>

<ol>
<li>Постройте методами матричной алгебры ковариационную матрицу</li>
<li>Проведите ее спектральное разложение (вычислите ее собственные числа и собственные векторы).</li>
<li>Оцените информативность главных осей.</li>
<li>Изобразите точки в пространстве первой и второй главной оси.</li>
</ol>

</article></slide><slide class=""><hgroup><h2>Решение</h2></hgroup><article  id="решение" class="smaller">

<pre class = 'prettyprint lang-r'># Нормализуем векторы в исходной матрице 
length_X &lt;- apply(as.matrix(X), MARGIN = 2, 
                  FUN = function(x) sqrt(sum(x^2)) )
  
X_norm &lt;- X / length_X 
  
X_norm_cent &lt;- as.data.frame(scale(X_norm, 
                                   center = TRUE, scale = FALSE))

mX_norm_cent &lt;- as.matrix(X_norm_cent)

Sx_norm_cent &lt;- t(mX_norm_cent) %*% mX_norm_cent /(nrow(mX_norm_cent) - 1)

eig &lt;- eigen(Sx_norm_cent)

Lambda &lt;- eig$values 

U &lt;- eig$vectors 

U_scaled &lt;- U %*% sqrt(diag(Lambda)) 

PC &lt;- as.matrix(X_norm_cent) %*% U </pre>

</article></slide><slide class=""><hgroup><h2>Решение</h2></hgroup><article  id="решение-1" class="smaller">

<p>Исходные данные в новой системе координат</p>

<pre class = 'prettyprint lang-r'>qplot(PC[ ,1], PC[ ,2])  </pre>

<p><img src="07_intro_to_linear_algebra_part2_files/figure-html/unnamed-chunk-31-1.png" width="672" style="display: block; margin: auto;" /></p>

<p>Информативность новых осей</p>

<pre class = 'prettyprint lang-r'>Lambda / sum(Lambda) * 100</pre>

<pre >## [1] 54.751 40.002  3.634  1.295  0.318</pre>

</article></slide><slide class="segue dark nobackground level1"><hgroup class = 'auto-fadein'><h2>Level 9: Сингулярное разложение матриц (Singular value decomposition)</h2></hgroup><article  id="level-9-сингулярное-разложение-матриц-singular-value-decomposition">

</article></slide><slide class=""><hgroup><h2>Теорема Экарта-Янга</h2></hgroup><article  id="теорема-экарта-янга" class="smaller">

<p>Любую прямоугольную матрицу \(\textbf{Y}\) можно представить в виде произведения трех матриц:</p>

<p>\[
\textbf{Y}_{n \times p} = \textbf{U}_{n \times p} \textbf{D}_{p \times p} \textbf{V}&#39;_{p \times p} 
\]</p>

<p>То есть можно найти три &ldquo;вспомогательных&rdquo; матрицы, через которые можно выразить любую другую матрицу.</p>

</article></slide><slide class=""><hgroup><h2></h2></hgroup><article >

<p>\[
\textbf{Y}_{n \times p} = \textbf{U}_{n \times p} \textbf{D}_{p \times p} \textbf{V}&#39;_{p \times p} 
\]</p>

<p>Здесь</p>

<p>\(\textbf{Y}_{n \times p}\) - любая прямоугольная матрица \(\begin{pmatrix}a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1c} \\ a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2p} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ a_{r1} &amp; a_{n2} &amp; \cdots &amp; a_{np} \end{pmatrix}\)</p>

<p>\(\textbf{D}_{p \times p}\) - диагональная матрица \(\begin{pmatrix} d_{11} &amp; 0 &amp; \cdots &amp; 0 \\ 0 &amp; d_{22} &amp; \cdots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; \cdots &amp; d_{pp} \end{pmatrix}\)</p>

<p>По главной диагонали располагаются &ldquo;особые&rdquo; числа, называющиеся <strong>сингулярными числами</strong>. Сингулярные числа ранжируются от большего к меньшему.</p>

<p>\(\textbf{U}_{n \times p}\) и \(\textbf{V}_{p \times p}\) - левая и правая матрицы сингулярных векторов.</p>

</article></slide><slide class=""><hgroup><h2>Сингулярное разложение матрицы средствами R</h2></hgroup><article  id="сингулярное-разложение-матрицы-средствами-r" class="smaller">

<pre class = 'prettyprint lang-r'>set.seed(123456789)
B &lt;- matrix(round(runif(50, 1, 5))  , byrow = T, ncol=5) #Некоторая матрица
SVD &lt;- svd(B) #Сингулярное Разложение матрицы B с помощью функции svd()
V &lt;- SVD$v #&quot;Вспомогательная&quot; матрица - правые сингулярные векторы
D &lt;- SVD$d #Вектор сингулярных чисел
U &lt;- SVD$u #&quot;Вспомогательная&quot; матрица - левые сингулярные векторы </pre>

<p>Вычислим \(\textbf{U} \textbf{D} \textbf{V}&#39;\)</p>

<pre class = 'prettyprint lang-r'>U %*% diag(D) %*% t(V) </pre>

<pre >##       [,1] [,2] [,3] [,4] [,5]
##  [1,]    4    4    4    4    5
##  [2,]    5    2    5    2    3
##  [3,]    1    4    3    2    3
##  [4,]    2    1    1    5    4
##  [5,]    2    3    4    1    2
##  [6,]    4    2    3    3    4
##  [7,]    1    3    2    4    5
##  [8,]    2    5    3    1    3
##  [9,]    4    3    4    4    1
## [10,]    4    3    2    2    1</pre>

</article></slide><slide class=""><hgroup><h2>Задание</h2></hgroup><article  id="задание-1">

<p>Вычислите матрицу, которая получится при использовании только 1 и 2 сингулярного числа для матрицы \(\textbf{B}\), использованной на предыдущем слайде.</p>

</article></slide><slide class=""><hgroup><h2>Решение</h2></hgroup><article  id="решение-2">

<pre class = 'prettyprint lang-r'>U[,1:2] %*% diag(D[1:2]) %*% t(V[,1:2]) </pre>

<pre >##       [,1] [,2] [,3] [,4] [,5]
##  [1,] 3.89 4.00 4.10 4.27 4.75
##  [2,] 4.16 3.79 4.47 2.16 2.50
##  [3,] 2.53 2.55 2.68 2.50 2.79
##  [4,] 1.18 1.80 1.13 4.23 4.57
##  [5,] 3.03 2.73 3.27 1.41 1.64
##  [6,] 2.99 3.06 3.15 3.23 3.59
##  [7,] 1.79 2.32 1.79 4.38 4.76
##  [8,] 3.19 2.99 3.42 2.09 2.38
##  [9,] 3.67 3.41 3.93 2.24 2.56
## [10,] 2.95 2.67 3.18 1.42 1.65</pre>

</article></slide><slide class=""><hgroup><h2>Важное свойство сингулярных чисел</h2></hgroup><article  id="важное-свойство-сингулярных-чисел">

<p>Если вычислить матрицу на основе не всех, а части сингулярных чисел, то новая матрица будет подобна исходной матрице.</p>

<p><img src="07_intro_to_linear_algebra_part2_files/figure-html/unnamed-chunk-36-1.png" width="672" style="display: block; margin: auto;" /></p>

</article></slide><slide class=""><hgroup><h2>Применение свойства сингулярных чисел в сжатии изображений</h2></hgroup><article  id="применение-свойства-сингулярных-чисел-в-сжатии-изображений">

<p><img src="07_intro_to_linear_algebra_part2_files/figure-html/unnamed-chunk-37-1.png" width="672" style="display: block; margin: auto;" /></p>

<p><small> Пример взят из курса лекций &ldquo;Data Analysis&rdquo; by Jeffrey Leek</p>

<p>(<a href='https://github.com/jtleek/dataanalysis/tree/master/week3' title=''>https://github.com/jtleek/dataanalysis/tree/master/week3</a>)</small></p>

</article></slide><slide class=""><hgroup><h2>Произведем сингулярное разложение матрицы <code>faceData</code></h2></hgroup><article  id="произведем-сингулярное-разложение-матрицы-facedata" class="smaller">

<pre class = 'prettyprint lang-r'>SVD_face &lt;- svd(faceData)

U &lt;- SVD_face$u
D &lt;- SVD_face$d
V &lt;- SVD_face$v</pre>

</article></slide><slide class=""><hgroup><h2>Рекоструируем изображение, используя только часть информации</h2></hgroup><article  id="рекоструируем-изображение-используя-только-часть-информации">

<pre class = 'prettyprint lang-r'>reduction &lt;- function(x) U[,1:x] %*% diag(D[1:x]) %*% t(V[, 1:x])
gg_face(reduction(4))</pre>

<p><img src="07_intro_to_linear_algebra_part2_files/figure-html/unnamed-chunk-39-1.png" width="672" style="display: block; margin: auto;" /></p>

</article></slide><slide class=""><hgroup><h2>Применение SVD в биологических исследованиях</h2></hgroup><article  id="применение-svd-в-биологических-исследованиях">

<p>SVD - это метод, на котором основаны разные типы анализа, связанные со снижением размерности: PCA, CA, CCA, RDA.</p>

<p>О них в следующех лекциях</p>

</article></slide><slide class=""><hgroup><h2>Summary</h2></hgroup><article  id="summary">

<ul>
<li>Линейная алгебра позволяет решать самые разные типы задач.</li>
<li>Матричные методы лежат в основе очень многих типов анализа.</li>
<li>В основе многих методов снижения размерности лежит SVD.</li>
</ul>

</article></slide><slide class=""><hgroup><h2>Что почитать</h2></hgroup><article  id="что-почитать">

<ul>
<li>Legendre P., Legendre L. (2012) Numerical ecology. Second english edition. Elsevier, Amsterdam. Глава 2. Matrix algebra: a summary.</li>
</ul>

</article></slide><slide class="segue dark nobackground level1"><hgroup class = 'auto-fadein'><h2>Not The End</h2></hgroup><article  id="not-the-end">

<img src='images/matrix_2.jpg' title=''/></article></slide>


  <slide class="backdrop"></slide>

</slides>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

<!-- map slide visiblity events into shiny -->
<script>
  (function() {
    if (window.jQuery) {
       window.jQuery(document).on('slideleave', function(e) {
         window.jQuery(e.target).trigger('hidden');
      });
       window.jQuery(document).on('slideenter', function(e) {
         window.jQuery(e.target).trigger('shown');
      });
    }
  })();
</script>

</body>
</html>
