---
title: "За кулисами CA и CCA"
subtitle: "Анализ и визуализация многомерных данных с использованием R"
author: Вадим Хайтов, Марина Варфоломеева
presenters: [{
  name: 'Вадим Хайтов',
  company: 'Каф. Зоологии беспозвоночных, СПбГУ',
  }]
output:
 ioslides_presentation:
  widescreen: true
  css: my_styles.css
  logo: course_logo.png
---
```{r setup, include=FALSE, cache=FALSE}
#-- RUN THE FRAGMENT BETWEEN LINES BEFORE COMPILING MARKDOWN
# to configure markdown parsing
options(markdown.extensions = c("no_intra_emphasis", "tables", "fenced_code", "autolink", "strikethrough", "lax_spacing", "space_headers", "latex_math"))
#------
# output options
options(width = 70, scipen = 6, digits = 3)

# to render cyrillics in plots use cairo pdf
options(device = function(file, width = 7, height = 7, ...) {
 cairo_pdf(tempfile(), width = width, height = height, ...)
 })
library(knitr)
# chunk default options
opts_chunk$set(fig.align='center', tidy = FALSE, fig.width = 5, fig.height = 5, warning=FALSE, echo=TRUE, message=FALSE, cache = FALSE)
```

#Часть 1. Корреспондентный анализ (Correpondence analysis)
## Задача
Попробуем разобраться с "механикой" корреспондентного анализа на уровне матриц и их преобразований 

```{r, echo=FALSE}
library(ggplot2)
```

##Таблицы сопряжености
В основе CA лежит исследование таблиц сопряженности, которые описывают насколько связаны два явления. 


Для примера используем задачку из классической генетики, где анализируется цвет и форма горошин   

Пусть наши данные выглядят вот так

| Горох | Желтый | Зеленый |
| ---- | ---- | ---- |
| Гладкий | 99 | 42 |
| Морщинистый | 29 | 13 |


##Таблицы сопряжености
Представим эти данные в виде матрицы  
```{r, echo=TRUE}
peas <- matrix(c(99, 42, 29, 13), byrow = T, ncol = 2)
peas
```

В ячейках содежится **наблюдаемая частота**(O, от observed) события A (цвет горошины) при условии события B (форма горошины) 

##Таблицы сопряжености
Мы хотим выяснить, являются ли цвет и форма семян независимыми признаками.

Если эти два признака независимы, то наша *нулевая модель* выглядит так $M_0: Ft = Ft\cdot p_{y,w}  + Ft \cdot p_{y,s} + Ft \cdot p_{g,w} + Ft \cdot p_{g,s}$   
где   
$Ft$ - общая численность  
$p_{y,w}$ - Вероятность встретить желтых морщинистых  
$p_{y,s}$ - Вероятность встретить  желтых гладких  
$p_{g,w}$ - Вероятность встретить  зеленых морщинистых   
$p_{g,s}$ - Вероятность встретить  желтых гладких  


##Таблицы сопряжености
Для вычисления **ожидаемых частот** (E, от expected), при условии справедливости нулевой модели, нам нужно сделать следующее:

Найти общую численность семян $Ft$ - это просто число равное сумме всех ячеек  
Найти общие численности гладких и морщинистых семян $f_i$ - это вектор из двух чисел, равных суммам строк (маргинальная сумма строк)   

Найти общие численности желтых и зеленых семян $f_j$ - это вектор из двух чисел, равных суммам столбцов (маргинальная сумма столбцов) 

```{r}
Ft <- sum(peas)

f_i <- apply(peas, 1, FUN = sum)

f_j <- apply(peas, 2, FUN = sum)

```

##Таблицы сопряжености

| Горох | Желтый | Зеленый | Сумма | $p_i$ |
| ---- | ---- | ---- | ---- |
| Гладкий | __99__ | 42 | __141__ | `r 141/183` |
| Морщинистый | 29 | 13 | __42__ | `r 42/183` |
| Сумма | __128__ | __55__ | __183__ |
| $p_j$ | `r 128/183` | `r 55/183` |  |    

<br>
Оценка вероятности быть гладким: $p_s = \frac{141} {183}$   

Оценка вероятности быть морщинистым: $p_w = \frac{42} {183}$   

Оценка вероятности быть желтым: $p_y = \frac{128} {183}$  

Оценка вероятности быть зеленым: $p_g = \frac{55} {183}$ 


##Таблицы сопряжености
В векторном виде

```{r, echo=TRUE}
p_i <- f_i/Ft #Вектор вероятностей для формы
p_j <- f_j/Ft #Вектор вероятностей для цвета

```


##Таблицы сопряжености

Вероятности сочетний признаков

| Горох | Желтый | Зеленый |
| ---- | ---- | ---- |
| Гладкий | $p_S \times p_y$ | $p_S \times p_g$ |
| Морщинистый | $p_w \times p_y$ | $p_S \times p_y$ |

Но! в этой таблице легко узнать результат матричного произведения двух векторов

$$
\textbf{a} \bullet \textbf{b} =  
\begin{pmatrix}
p_y \\
p_g     
\end{pmatrix}
\times
\begin{pmatrix}
p_s &
p_w     
\end{pmatrix}
$$

##Таблицы сопряжености
Матрица вероятностей сочетаний
```{r, echo=TRUE}
q <- p_i %*% t(p_j) 
q
```

Матрица ожидаемых частот

```{r, echo=TRUE}
round(q * Ft, 1)
```

##Хи-квадрат, как мера сопряженности

Из курса статистики мы помним критерий $\chi^2$   


$$
\chi_{total}^2 = \sum {\frac{(O_{ij} - E_{ij})^2} {E_{ij}}}
$$


Вклад каждой ячейки в формирование общего $\chi_{total}^2$

$$
\chi_{ij} =  \frac{ (O_{ij} - E_{ij})} {\sqrt E_{ij} } = \sqrt{Ft} \left[\frac {p_{ij} - p_ip_j}{\sqrt{p_ip_j}}\right]
$$


$Ft$ - Сумма всех частот  

Здесь $\frac{(O_{ij} - E_{ij})} {\sqrt{E_{ij}}}$ - стандартизованный остаток от нулевой модели.    
Чем больше квадрат остатка, тем сильнее значение отклонияется от нулевой модели, которая предсказывает отсуствие связи. Сумма квадратов этих остатков характеризует общее отклонение от нулевой модели, то есть наличие сопряженности.


##Вычисляем $\chi^2$ вручную

```{r, echo=TRUE}
E <- (p_i %*% t(p_j) * Ft)
O <- peas

sum((O-E)^2/E)

```
Сравним
```{r, echo=TRUE}
chisq.test(x=O, p=q, correct = F)

```

Чем больше значение $\chi^2$ тем сильнее данные отклоняются от нулевой модели, тем выше сопряженность между думя явлениями. 
В случае с горохом общая сопряженность очень низкая (закон Менделя!)

##Возьмем более сложные данные 
```{r, echo=TRUE}
library(vegan)
data(mite)
head(mite)
```

##Струкутра данных

Таблица данных содержит данные по численности `r ncol(mite)` видов в `r nrow(mite)` пробах.

Нулевая модель предсказывает, что между пробами и видами нет сопряжения.

##Матрица вкладов в формирование общей сопряженности ($\chi^2$)

$$
\textbf{Q} = \frac {p_{ij} - p_ip_j}{\sqrt{p_ip_j}} = \frac{f_{ij}Ft - f_if_j}{Ft \sqrt{f_if_j}} 
$$

Матрица **Q** отличается от матрицы, содержащей $\chi_{ij}$,  только тем, что каждое ее значение разделено на $\sqrt{Ft}$. То есть это будут стандартизованные значения (что очень полезно, так как собственные значения будут меньше или равны 1). 

В этой матрице $p_i$ и $p_j$  маргинальные вероятности, то есть   

$p_i$ - Вероятность встретить особь данного вида  
$p_j$ - Вероятность встретить особь в данной пробе

$p_{ij}$ - вероятность встретить особь данного вида в данной пробе


##Вычисляем матрицу вкладов

Частоты и вероятности
```{r, echo=TRUE}
f_ij <- mite #Частота встречи данного вида в данной пробе, то есть это первичные даные!

p_ij <- mite/sum(mite) #вероятность встретить особь в данной пробе. 

Ft <- sum(mite) #Общее количество найденных животных

f_i <- apply(mite, 1, FUN = sum) #Общее количество особей в каждой пробе

p_i <- f_i/Ft #Вектор вероятностей встретить какую-либо особь в данной пробе

f_j <- apply(mite, 2, FUN = sum) #Общее количество особей в каждом виде

p_j <- f_j/Ft #Вектор вероятностей встретить особь данного вида

```

##Нулевая модель
 Ожидаемые частоты для нулевой модели, то есть при условии, что все пробы и все виды независимы

```{r}
E <- round(f_i %*% t(f_j) / Ft, 2)

head(E)
```

##Критерий $\chi^2$

```{r}
chisq.test(x=mite, p=p_ij, correct = F)
```

Вычислим величину, которую называют *"инерцией"* 

$$
Inertia = \frac{\chi^2}{Ft}
$$

Она показывает удельный вклад каждой особи в формрование общего отклонения от нулевой модели


```{r, echo=TRUE}
Inertia <- as.numeric(chisq.test(x=mite, p=p_ij, correct = F)$statistic)/Ft
Inertia
```


##Вычисляем матрицу вкладов 

```{r, echo=TRUE}
Q <- (f_ij*Ft - f_i %*% t(f_j))/(Ft*sqrt(f_i %*% t(f_j))) #Матрица, вычисленная через частоты

Q1 <-  (p_ij - p_i %*% t(p_j))/sqrt(p_i %*% t(p_j)) #Матрица, вычисленная через вероятности

#Вычитаем одну матрицу из дугой
round(sum(Q - Q1)) #Это одна  та же матрица!
```

Разность равна нулю! То есть матрицу **Q** можно вычислить и через частоты и через вероятности. 


##Сумма квадратов матрицы вкладов
```{r}
Q <- as.matrix(Q)

sum(Q^2)
```

Это та же самая $Inertia$!


## SVD матрицы вкладов

Матрица $\textbf{Q}$ содержит информацию о вкладе каждого вида в каждой пробе в формирование отклонения от нулевой модели, то есть в формирование сопряжености проб и видов.

Вспомним теорему Экарта-Янга

$$
\textbf{Q}_{n \times p} = \textbf{U}_{n \times p} \textbf{D}_{p \times p} \textbf{V}'_{p \times p} 
$$

То есть, с помощью SVD мы сможем представить матрицу **Q** как результат произведения трех "вспомогательных" матриц.   
Но! эти "вспомогательные" матрицы позволяют редуцировать исходную матрицу.

## SVD матрицы вкладов
```{r, echo=TRUE}
U <- svd(Q)$u 
D <- diag(svd(Q)$d)
V <- svd(Q)$v

```

##Размерности "вспомогательных" матриц
```{r, echo=TRUE}
dim(U) 
```

```{r, echo=TRUE}
dim(D)
```

```{r, echo=TRUE}
dim(V)
```

##Проверим восстаналивается ли матрица Q если использовать "вспомогательные" матрицы, полученные в SVD

```{r, echo=TRUE}
Qsvd <- U %*% D %*% t(V) #матрица "восстановленная" из "вспомогательных" матриц 

round(sum(Q - Qsvd)) #разность между исходной и "восстановленной" матрицами
```

Все работает!

##Поиск собственных значений и собственых векторов

Вспомним: матрица ассоциации (ковариации) - это
$$
\textbf A = X'X 
$$

В нашем случае

$$
\textbf A = Q'Q
$$

Матрица **A** - квадратная симметричная матрица 

##Поиск собственных значений и собственых векторов

```{r}
A <- t(Q) %*% Q

eig_values <- eigen(A)$values #Собственные числа матрицы A
eig_vectors <- eigen(A)$vectors #Матрица собственных векторов для матрицы A 

```

##Сравним с результатами SVD

```{r}
plot(eig_values, diag(D), main = "Квадратичная зависимость!" )
```

##Сравним с результатами SVD{.columns-2}

```{r, echo=FALSE}
plot(eig_values, diag(D)^2, main = "Квадраты сингулярных чисел матрицы Q \nравны собственным значениям \nматрицы ассоциации" )

```


**Обратите внимание на две особенности**    

1. Собственные значения для матрицы ассоциации **A = Q'Q** равны квадратам сингулярных чисел для матрицы **Q**    
2. Последнее собственое значение матрицы **A** равно нулю! Таково ее свойство (это связано с тем, что частоты в матрице **Q** центрированы). То есть матрица **A**, по своей природе, - это сингулярная матрица (ее определитель равен нулю, можете проверить).  Поэтому в дальнейшем анаизе будет использовано на одно собственное значение меньше. 

##$Inertia$ - это мера изменчивости!

Вспомним, что мы вычислили показатель $Inertia$, как сумму квадратов матрицы **Q**, или, что тоже самое, $Inertia$ - это удельный $\chi^2$. Из этого пока не следует, что $Inertia$ показывает общую изменчивость в системе. 

Вспомним, что общая дисперсия в системе - это сумма собственных значений. Ранее мы вычислили векор `eig_values`, содержащий собственные значения матрицы ассоциации **A** = **Q'Q**. 
Найдем сумму всех собственных значений, то есть суммарную диспрсию.

```{r}
sum(eig_values)
```

Это в точости значение $Inertia$!

##Главные оси {.columns-2 .smaller}

В корреспондентном анализе оси, соответствующие собственным значениям матрицы **A**, называются *главными осями* (principal axes).

Информативность главных осей оценивается как отношение значения данного собственного числа к сумме собственных чисел, то есть к $Inertia$

```{r, echo=FALSE}
Information <- data.frame(CA = 1:length(eig_values), Eigenval =round(eig_values, 5), Prop_Explained = round(eig_values/sum(eig_values), 5), Cumul_Prop=round(cumsum(eig_values/sum(eig_values)),5) )

round(Information, 3)
```

Обратите внимание, что последнее собственное значение (№35) равно нулю.


##Нарисуем ординацию проб в первых двух главных осях {.columns-2 .smaller}

Для этого нам понадобятся первые две колонки матрицы U полученной в SVD (они сответствуют первым двум сингулярным числам и, стало быть, первым двум собственным значениям и первым двум собственным векторам). 
Матрица **Q** имеет два центроида: центроид строк (пробы) и центроид колонок(виды)
Координаты центроидов задаются векторами `p_i` и `p_j`  

Для вычисления координат проб в *главных осях* необходимо значения, приведенные в матрице **U**, разделить на $\sqrt{p_i}$, то есть произвести такую матричную операцию:
$$
\textbf diag(p_i)^{-1/2} \times U
$$


```{r, echo=FALSE}
library(ggplot2)
CA_samples <- diag(p_i^(-1/2))%*% U[,1:2]


ggplot(as.data.frame(CA_samples), aes(x=V1, y=V2) ) + geom_text(label = rownames(mite)) + geom_hline(yintercept=0, linetype = 2) + geom_vline(xintercept = 0, linetype = 2) + theme_bw() + labs(x= "CA1", y = "CA2")

```


##Нарисуем ординацию видов в первых двух главных осях {.columns-2 .smaller}
Для вычисления координат видов в *главных осях* необходимо значения, приведенные в матрице **V** разделить на $\sqrt{p_j}$.   
То есть требуется провести следующее умножение матриц


$$
\textbf diag(p_j)^{-1/2} \times V
$$

```{r, echo=FALSE}
CA_species <- diag(p_j^(-1/2))%*% V[,1:2]


ggplot(as.data.frame( CA_species), aes(x = V1, y = V2) )  + geom_hline(yintercept=0, linetype = 2) + geom_vline(xintercept = 0, linetype = 2) + theme_bw() + labs(x= "CA1", y = "CA2") + geom_text(label = names(mite))

```



##Сравним с результатами специализированной функции `cca()` из пакета `vegan` {.smaller}

Собственные значения
```{r}
summary(cca(mite))$cont$importance [, 1:5]
```

Все аналогично!

##Сравним с результатами специализированной функции `cca()` из пакета `vegan`  {.columns-2 .smaller}

Ординация проб. Все аналогчно результатам, полученным вручную!

```{r}
plot(cca(mite), display = "sites")
```


##Сравним с результатами специализированной функции `cca()` из пакета `vegan` {.columns-2 .smaller}

Ординация видов. Все аналогчно результатам, полученным вручную!
```{r}
plot(cca(mite), display = "species")
```




##Sumary
Принципиальное отличие CA от PCA - это использование матрицы **Q** вместо матрицы исходных значений.   
Матрица **Q** - это матрица вкладов в формирование отклонения от нулевой гипотезы, говорящей, что сопряжения между явлениями нет.   
Вычислительный базис CA - это SVD матрицы **Q**

#Часть 2. Канонический корреспондентный анализ (Canonical correspondence analysis)

## После применения Корреспондентного анализа мы знаем общее варьирование в системе 
Эта величина оценивается "инерцией"

```{r, echo=FALSE}
data(mite)

data("mite.env")

f_ij <- mite #Частота встречи данного вида в данной пробе, то есть это первичные даные!

p_ij <- mite/sum(mite) #вероятность встретить особь в данной пробе. 

Ft <- sum(mite) #Общее количество найденных животных

f_i <- apply(mite, 1, FUN = sum) #Общее количество особей в каждой пробе

p_i <- f_i/Ft #Вектор вероятностей встретить какую-либо особь в данной пробе

f_j <- apply(mite, 2, FUN = sum) #Общее количество особей в каждом виде

p_j <- f_j/Ft #Вектор вероятностей встретить особь данного вида


Q <- (f_ij*Ft - f_i %*% t(f_j))/(Ft*sqrt(f_i %*% t(f_j))) #Матрица вкладов, вычисленная через частоты
Q <- as.matrix(Q)

# SVD матрицы Q
U <- svd(Q)$u 
D <- diag(svd(Q)$d)
V <- svd(Q)$v

Inertia_total <- sum(D^2) #Общая инерция в системе
CA_number <- sum(round(D, 2) !=0) #Количество главныю осей в CA. Обратите внимание на то, что их на одну меньше, чем исходных колонок в матрице Q

```

Общая изменчивость в системе `r Inertia_total`   
Общее количество осей `r CA_number`, как и положено, на одно меньше, чем  общее количество колонок в матрице **Q**, равное `r ncol(Q)`

##Связь между предикторами (параметрами среды) и зависимыми перменными (видами)

Для начала надо построить модельную матрицу
```{r, echo=TRUE}
X <- model.matrix(~SubsDens + WatrCont + Substrate + Topo, data =  mite.env) #Модельная матрица 

```

Количество столбцов в матрице **X** равно `r ncol(X)`   
Первая колонка - это интерцепт. Следовательно в модельной матрице "значимых" колонок `r ncol(X) - 1`.    
Вас не должно смущать, что количество колонок больше, чем количество предикторов в модели `~SubsDens + WatrCont + Substrate + Topo`, так как предикоры  `SubsDens и WatrCont`  - это непрерывные перменные, они дают по одной колонке в модельной матрице.  

Но! `Substrate и Topo` - это категориальные предикторы с `r length(levels(mite.env$Substrate))` и `r length(levels(mite.env$Topo))` градациями. При этом по одной из градаций обоих факторов уходит в базовый уровень модели.
Таким образом, в модельной матрице есть  2 колонки (от `SubsDens` и `WatrCont`) плюс (7 -1) колонок от `Substrate` плюс (2-1) колонок от `Topo`. Итого, 9 колонок!   

Внимание! Именно столько и будет канонических осей в ограниченной ординации.

##Строим матрицу предсказанных значений
Для этого надо будет решить следующее матричное уравнение

$$
\hat {\textbf Y}  = \boldsymbol {\beta \times  X}
$$

В нашем случае это будет матрица **Q_pred**

## Матрица коэффициентов 

$$
\boldsymbol{\beta} = \boldsymbol {[X'X]^{-1}X'Y}
$$

## Матрица остатков

$$
\boldsymbol {Y_{resid} = Y - \hat {Y}}
$$

В нашем случае это будет матрица **Q_resid**

##Теперь все это на языке R

```{r, echo=TRUE}
#Матрица коэффициентов
betas <- solve(t(X) %*% diag(p_i) %*% X) %*% (t(X) %*% diag(p_i)^(1/2) %*% Q)

 #Матрица предсказанных значенй
Q_pred <- diag(p_i)^(1/2) %*% X %*% betas
```

Обратите внимание, что для вычисления коэффициентов регрессии нам пришлось опять вводить в расчет маргинальные вероятности $p_i$ и $\frac{1}{\sqrt{p_i}}$. 

Это связано с природой матрицы **Q**. Эта матрица не является матрицей первичных данных, а является матрицей остатков от нулевой модели, описывающей отсутствие связи между столбцами и сроками. 

##SVD матрицы предсказанных значений

SVD матрицы предсказанных значений

```{r, echo=TRUE}
U_pred <- svd(Q_pred)$u 
D_pred <- diag(svd(Q_pred)$d)
V_pred <- svd(Q_pred)$v

```

`D_pred` - характеризует канонические (constrained) оси 

##Инерция в пространстве канонических осей

```{r}
Inertia_constrained <- sum(D_pred^2) #Инерция в ограниченной ординации

CCA_number <- sum(round(D_pred, 2) !=0) #Количество Канонических осей в СCA. 

```


Обратите внимание, в векторе сингулярных чисел `r CCA_number` ненулевых значений, то есть, как и обещали, ровно столько же, сколько колонок в модельной матрце **X** (без учета интерцепта).

Инерция в канонических осях составляет `r Inertia_constrained`.   
Можно оценить, сколько процентов от общей инерции, полученной в CA, это составляет. 
Итого, канонические оси отражают `r Inertia_constrained/Inertia_total * 100` %  общей изменчивости (инерции).


##Матрица остатков 

Это разность между матрицей **Q** и **Q_pred**

```{r, echo=TRUE}
Q_resid <- Q - Q_pred

```

##SVD матрицы остатков

```{r, echo=TRUE}
U_res <- svd(Q_resid)$u 
D_res <- diag(svd(Q_resid)$d)
V_res <- svd(Q_resid)$v

```

Сингуляные числа в матрице **D_res** задают неканонические (unconstrained) оси
Число ненулевых сингулярных чисел в матрице **D_res** составляет `r sum(round(D_res, 2) !=0)`. То есть неканонических осей столько же сколько главных осей в корреспондентном анализе. То есть, на 1 меньше, чем в колонок в матрице **Q**.


## Инерция в пространстве неканонических осей

```{r, echo=TRUE}
#Инерция ординации остатков 
Inertia_unconstrained <- sum(D_res^2) 
```

Инерция в пространстве неканонических осей составляет: `r Inertia_unconstrained`, то есть неканонические оси объясняют `r Inertia_unconstrained/ Inertia_total *100` % общего варьирования (общей инерции) в системе.    

На всякий случай обратите внимание, что 
$$Inertia_{total} = Inertia_{constrained} + Inertia_{unconstrained}$$


##Сравним то, что мы получили вручную с тем, что делает функция cca() из пакета vegan {.smaller .columns-2}

Вот то, что получено вручную     

Общая инерция, инерция в канонических осях и инерция в неканонических осях
```{r, echo=FALSE}
Inertia_total
Inertia_constrained
Inertia_unconstrained

```

Соответствующие доли 

```{r, echo=FALSE}
Inertia_total/Inertia_total
Inertia_constrained/Inertia_total
Inertia_unconstrained/Inertia_total

```

Собственные значния для канонических осей (квадраты сигулярных чисел из **D_pred**)
```{r, echo=FALSE}
round(diag(D_pred)^2, 4) [round(diag(D_pred)^2, 4) !=0]

```

Первые 8 собственных значний для канонических осей (квадраты сигулярных чисел из **D_res**)
```{r, echo=FALSE}
round(diag(D_res)^2, 5) [1:8]

```



##Сравним то, что мы получили вручную с тем, что делает функция cca() из пакета vegan

Вот то, что показывает `cca()`. Все одинаково!
```{r, echo=TRUE}
mite_cca <- cca(mite ~ SubsDens + WatrCont + Substrate + Topo, data = mite.env)
mite_cca
```


##Ординация проб в каннических (constrained) осях 

```{r, echo=TRUE}

#Координаты проб в канонических осях полученные вручную

constr_CA_samples <- diag(p_i^(-1/2))%*% U_pred

```

```{r, echo=TRUE}
#Координаты проб в канонических осях по версии cca()

cca_ord <- mite_cca$CCA$u 

```

##Посмотрим на результаты ординации в первых двух канонических осях  {.columns-2}

```{r, echo=FALSE}
library(ggplot2)
ggplot(as.data.frame(constr_CA_samples), aes(x=V1, y=V2)) + geom_text(label = rownames(mite)) + labs(x = "CCA1", y = "CCA2") + theme_bw() + geom_hline(yintercept = 0, linetype = 2) + geom_vline(xintercept = 0, linetype = 2) + ggtitle("Результаты, полученные вручную")

plot(mite_cca, display = "lc", main = "Результаты сca()")
```



##Ординация видов в каннических (constrained) осях

```{r, echo=TRUE}

#Координаты видов канонических осях полученные вручную

constr_CA_species <- diag(p_j^(-1/2))%*% V_pred

```

```{r, echo=TRUE}
#Координаты видов в канонических осях по версии cca()

cca_sp_constr <- mite_cca$CCA$v 

```


##Посмотрим на результаты ординации видов в первых двух канонических осях  {.columns-2}

```{r, echo=FALSE}
library(ggplot2)
ggplot(as.data.frame(constr_CA_species), aes(x=V1, y=V2)) + geom_text(label = colnames(mite)) + labs(x = "CCA1", y = "CCA2") + theme_bw() + geom_hline(yintercept = 0, linetype = 2) + geom_vline(xintercept = 0, linetype = 2) + ggtitle("Результаты, полученные вручную")

plot(mite_cca, display = "sp", main = "Результаты сca()", scaling = "species")

```



##Summary
- Вычислительная часть CCA ничем не отличается вычислительной части CA, но в анализ  вовлекаются вместо одной матрицы **Q** две матрицы:   
- Матрица предсказанных линейной моделью значений **Q_pred** (дает канонические оси)   
- Матрица остатков **Q_res** (дает неканонические оси)   
- Вычсление матриц **Q_pred** и **Q_res** производится с использованием матричного способа подбора коэффициентов регрессии.   






