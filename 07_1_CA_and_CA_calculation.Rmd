---
title: "Корреспондентный анализ и анализ главных компонент"
subtitle: "Анализ и визуализация многомерных данных с использованием R"
author: Вадим Хайтов, Марина Варфоломеева
presenters: [{
  name: 'Вадим Хайтов',
  company: 'Каф. Зоологии беспозвоночных, СПбГУ',
  }]
output:
 ioslides_presentation:
  widescreen: true
  css: assets/my_styles.css
  logo: assets/Linmod_logo.png
---


```{r setup, include = FALSE, cache = FALSE, purl = FALSE}
# output options
options(width = 70, scipen = 6, digits = 3)
library(knitr)
# chunk default options
opts_chunk$set(fig.align='center', tidy = FALSE, fig.width = 7, fig.height = 3, warning = FALSE)
```

```{r, libs-funs, echo=FALSE}
library(ggplot2)
library(grid)
theme_set(theme_bw(base_size = 16) + theme(legend.key = element_blank()))
update_geom_defaults("point", list(shape = 19, size = 4))
library(gridExtra)
```

## Корреспондентрый анализ и анализ главных компонент

- Сложности при анализе видового состава сообществ при помощи анализа главных компонент
    - Анализ сырых данных

- Корреспондентный анализ
    - Анализ таблиц сопряженности, хи-квадрат
    - Оси в корреспондентном анализе
    - Интерпретация графиков в корреспондентном анализе

### Вы сможете

- Находить проявление "эффекта подковы" в анализе главных компонент
- Проводить корреспондентный анализ таблиц сопряженности
- Объяснить, что именно означает взаиморасположение точек объектов и переменных на графиках результатов корреспондентного анализа
- Интерпретировать графики результатов корреспондентного анализа


# Анализ видового состава сообществ. Трансформации данных

## Пример: Птицы в лесах Австралии

Обилие 102 видов птиц в 37 сайтах в юго-восточной Австралии (Mac Nally, 1989; данные из Quinn, Keough, 2002). Можно ли описать отношения между сайтами небольшим числом главных компонент?

```{r}
library(readxl)
birds <- read_excel(path = "data/macnally.xlsx")

# имена переводим в нижний регистр
colnames(birds) <- tolower(colnames(birds))
```


## Задание: Проведите анализ главных компонент


## Результаты анализа главных компонент

```{r fig.height=4, message=FALSE}
library(vegan)
bird_pca <- rda(birds[ , -c(1, 2)], scale = TRUE)
# summary(bird_pca)
screeplot(bird_pca, type = "lines", bstick = TRUE) # график собственных чисел
```

> - Первые две компоненты объясняют умеренное количество изменчивости


## Факторные нагрузки

```{r fig.height=4}
biplot(bird_pca, display = "species", scaling = 2, type = "t")
```

> - У многих переменных факторные нагрузки велики сразу на две оси. Это может быть неудобно.

## Обратите внимание, ординация сайтов в виде подковы!

Сайты 29 и 14 на самом деле расположены далеко друг от друга и мало похожи. Почему же они сближены на графике?

```{r fig.height=4}
biplot(bird_pca, display = "sites", scaling = 1) 
```

> - Так происходит от того, что завышены корреляции между переменными из-за большого числа нулей

## Что делать если появился "эффект подковы"? {.columns-2}

**Решение №1**
нужна трансформация исходных данных

- расстояние Хеллингера (Hellinger distance)
- хордальное расстояние (chord distance)

```{r}
birds_h <- decostand(birds[ , -c(1, 2)], "hellinger")
birds_ch <- decostand(birds[ , -c(1, 2)], "norm") # chord distance
```

**Решение №2**
Изменить метод анализа


## Задание: Проведите анализ главных компонент по трансформированным данным

Сравните долю дисперсии, объясненной первыми двумя компонентами с результатами анализа нетрансформированных данных.

- В каком случае объясненная дисперсия больше?

Сравните получившиеся ординации объектов.

- Исчез ли "эффект подковы" после трансформации?
- Изменилась ли группировка объектов?

## Анализ главных компонент

```{r}
bird_h_pca <- rda(birds_h)
bird_mds <- metaMDS(birds[,-c(1,2)])
```


## Собственные числа

```{r fig.height=5}
screeplot(bird_h_pca, bstick = TRUE)
```



## Ординация до и после трансформации данных

```{r fig.width = 10, fig.height = 4.5, fig.show='hold'}
op <- par(mfrow = c(1, 3), cex = 0.9, mar = c(4, 4, 2.5, 0.5))
plot(bird_pca, display = "sites", scaling = 1, main = "PCA,\nбез трансформации", type = "p")
plot(bird_h_pca, display = "sites", scaling = 1, main = "PCA,\nтрансформация Хеллингера", type = "p")

plot(bird_mds, display = "sites", main = "MDS", type = "p")

par(op)
```

## Для успешного применения анализа главных компонент нужно:

- Линейные связи между переменными (т.к. матрица корреляций или ковариаций, подразумевает линейность связей)
- Исключить наблюдения, в которых есть пропущенные значения
- Если много нулей - трансформация данных
- Если очень много нулей - удалить такие переменные из анализа



## Пример: клещи

Клещи-орибатиды на сплавине одного из канадских озер

На площадке 10 х 2.6м взяли стратифицированную случайную выборку  (70 проб) на 7 типах субстратов. (Borcard & Legendre 1994).


![Oribatid mite](./images/OribatidMiteWithAVisitingFriendlySpringtail-AndyMurray-Flickr.jpg)

Oribatid mite with a visiting friendly springtail by [Andy Murray on Flickr](https://flic.kr/p/bqAFBs)

![Peat bog](./images/Tourbière_PeatBog-peupleloup-Flickr.jpg)

Tourbière/Peat bog* by [peupleloup on Flickr](https://flic.kr/p/51F8Ks)

* - peat bog - зыбун, сплавина (плавина)


```{r, echo=TRUE}
library(vegan)
data(mite)
head(mite)
```

## Задание: 

- Проведите анализ главных компонент

- Нарисуйте биплот расстояний


## PCA vs MDS

```{r fig.show='hold', fig.height=4}
mite_pca <- rda(mite)
mite_mds <- metaMDS(mite)
# summary(rats_ch_pca)
op <- par(mfrow = c(1, 2))
# screeplot(rats_ch_pca, bstick = TRUE, main = "График собственных чисел")
biplot(mite_pca, scaling = 1, display = "sites", main = "PCA", type = "p")
plot(mite_mds, display = c("sites"),  main = "MDS")
par(op)
```
Проблемы PCA: 

- Есть отскакивающее значение
- Вырисовывается "подкова"


## MDS или PCA?

- MDS может работать с любыми данными 
- MDS лучше отражает градиенты
- **MDS не дает в явном виде информации о связи признаков**
- **Оси MDS безлики** их важность не поддается оценке
- **Оси MDS трудно использовать как комплексные признаки**

>- Существует третий путь - **Корреспондентный анализ**

# Корреспондентный анализ

## Корреспондентны анализ "в темную"

```{r}
mite_ca <- cca(mite) 
summary(mite_ca)
```

## Ординация проб

```{r}
op <- par(mfrow = c(1, 3))
# screeplot(rats_ch_pca, bstick = TRUE, main = "График собственных чисел")
biplot(mite_pca, scaling = 1, display = "sites", main = "PCA", type = "p")
# plot(rats_mds, display = c("sites"),  main = "MDS")
plot(mite_mds, display = c("sites"),  main = "MDS")

plot(mite_ca, display = "sites", type = "p", main = "CA")
par(op)
```

## связь проб и видов
```{r}
op <- par(mfrow = c(1, 2))

plot(mite_ca, display = "sites")
plot(mite_ca, display = "species")

par(op)

```



#Механика корреспондентного анализа

##Таблицы сопряжености
В основе CA лежит исследование таблиц сопряженности, которые описывают насколько связаны два явления. 


Для примера используем задачку из классической генетики, где анализируется цвет и форма горошин   

Пусть наши данные выглядят вот так

| Горох | Желтый | Зеленый |
| ---- | ---- | ---- |
| Гладкий | 99 | 42 |
| Морщинистый | 29 | 13 |


##Таблицы сопряжености
Представим эти данные в виде матрицы  
```{r, echo=TRUE}
peas <- matrix(c(99, 42, 29, 13), byrow = T, ncol = 2)
peas
```

В ячейках содежится **наблюдаемая частота**(O, от observed) события A (цвет горошины) при условии события B (форма горошины) 

##Таблицы сопряжености
Мы хотим выяснить, являются ли цвет и форма семян независимыми признаками.

Если эти два признака независимы, то наша *нулевая модель* выглядит так $M_0: Ft = Ft\cdot p_{y,w}  + Ft \cdot p_{y,s} + Ft \cdot p_{g,w} + Ft \cdot p_{g,s}$   
где   
$Ft$ - общая численность  
$p_{y,w}$ - Вероятность встретить желтых морщинистых  
$p_{y,s}$ - Вероятность встретить  желтых гладких  
$p_{g,w}$ - Вероятность встретить  зеленых морщинистых   
$p_{g,s}$ - Вероятность встретить  желтых гладких  


##Таблицы сопряжености
Для вычисления **ожидаемых частот** (E, от expected), при условии справедливости нулевой модели, нам нужно сделать следующее:

Найти общую численность семян $Ft$ - это просто число равное сумме всех ячеек  
Найти общие численности гладких и морщинистых семян $f_i$ - это вектор из двух чисел, равных суммам строк (маргинальная сумма строк)   

Найти общие численности желтых и зеленых семян $f_j$ - это вектор из двух чисел, равных суммам столбцов (маргинальная сумма столбцов) 

```{r}
Ft <- sum(peas)

f_i <- apply(peas, 1, FUN = sum)

f_j <- apply(peas, 2, FUN = sum)

```

##Таблицы сопряжености

| Горох | Желтый | Зеленый | Сумма | $p_i$ |
| ---- | ---- | ---- | ---- |
| Гладкий | __99__ | 42 | __141__ | `r 141/183` |
| Морщинистый | 29 | 13 | __42__ | `r 42/183` |
| Сумма | __128__ | __55__ | __183__ |
| $p_j$ | `r 128/183` | `r 55/183` |  |    

<br>
Оценка вероятности быть гладким: $p_s = \frac{141} {183}$   

Оценка вероятности быть морщинистым: $p_w = \frac{42} {183}$   

Оценка вероятности быть желтым: $p_y = \frac{128} {183}$  

Оценка вероятности быть зеленым: $p_g = \frac{55} {183}$ 


##Таблицы сопряжености
В векторном виде

```{r, echo=TRUE}
p_i <- f_i/Ft #Вектор вероятностей для формы
p_j <- f_j/Ft #Вектор вероятностей для цвета

```


##Таблицы сопряжености

Вероятности сочетний признаков

| Горох | Желтый | Зеленый |
| ---- | ---- | ---- |
| Гладкий | $p_S \times p_y$ | $p_S \times p_g$ |
| Морщинистый | $p_w \times p_y$ | $p_S \times p_y$ |

Но! в этой таблице легко узнать результат матричного произведения двух векторов

$$
\textbf{a} \bullet \textbf{b} =  
\begin{pmatrix}
p_y \\
p_g     
\end{pmatrix}
\times
\begin{pmatrix}
p_s &
p_w     
\end{pmatrix}
$$

##Таблицы сопряжености
Матрица вероятностей сочетаний
```{r, echo=TRUE}
q <- p_i %*% t(p_j) 
q
```

Матрица ожидаемых частот

```{r, echo=TRUE}
round(q * Ft, 1)
```

##Хи-квадрат, как мера сопряженности

Из курса статистики мы помним критерий $\chi^2$   


$$
\chi_{total}^2 = \sum {\frac{(O_{ij} - E_{ij})^2} {E_{ij}}}
$$


Вклад каждой ячейки в формирование общего $\chi_{total}^2$

$$
\chi_{ij} =  \frac{ (O_{ij} - E_{ij})} {\sqrt E_{ij} } = \sqrt{Ft} \left[\frac {p_{ij} - p_ip_j}{\sqrt{p_ip_j}}\right]
$$


$Ft$ - Сумма всех частот  

Здесь $\frac{(O_{ij} - E_{ij})} {\sqrt{E_{ij}}}$ - стандартизованный остаток от нулевой модели.    
Чем больше квадрат остатка, тем сильнее значение отклонияется от нулевой модели, которая предсказывает отсуствие связи. Сумма квадратов этих остатков характеризует общее отклонение от нулевой модели, то есть наличие сопряженности.


##Вычисляем $\chi^2$ вручную

```{r, echo=TRUE}
E <- (p_i %*% t(p_j) * Ft)
O <- peas

sum((O-E)^2/E)

```
Сравним
```{r, echo=TRUE}
chisq.test(x=O, p=q, correct = F)

```

Чем больше значение $\chi^2$ тем сильнее данные отклоняются от нулевой модели, тем выше сопряженность между думя явлениями. 
В случае с горохом общая сопряженность очень низкая (закон Менделя!)

## Вернемся к клещам

##Струкутра данных

Таблица данных содержит данные по численности `r ncol(mite)` видов в `r nrow(mite)` пробах.

Нулевая модель предсказывает, что между пробами и видами нет сопряжения.

##Матрица вкладов в формирование общей сопряженности ($\chi^2$)

$$
\textbf{Q} = \frac {p_{ij} - p_ip_j}{\sqrt{p_ip_j}} = \frac{f_{ij}Ft - f_if_j}{Ft \sqrt{f_if_j}} 
$$

Матрица **Q** отличается от матрицы, содержащей $\chi_{ij}$,  только тем, что каждое ее значение разделено на $\sqrt{Ft}$. То есть это будут стандартизованные значения (что очень полезно, так как собственные значения будут меньше или равны 1). 

В этой матрице $p_i$ и $p_j$  маргинальные вероятности, то есть   

$p_i$ - Вероятность встретить особь данного вида  
$p_j$ - Вероятность встретить особь в данной пробе

$p_{ij}$ - вероятность встретить особь данного вида в данной пробе


##Вычисляем матрицу вкладов

Частоты и вероятности
```{r, echo=TRUE}
f_ij <- mite #Частота встречи данного вида в данной пробе, то есть это первичные даные!

p_ij <- mite/sum(mite) #вероятность встретить особь в данной пробе. 

Ft <- sum(mite) #Общее количество найденных животных

f_i <- apply(mite, 1, FUN = sum) #Общее количество особей в каждой пробе

p_i <- f_i/Ft #Вектор вероятностей встретить какую-либо особь в данной пробе

f_j <- apply(mite, 2, FUN = sum) #Общее количество особей в каждом виде

p_j <- f_j/Ft #Вектор вероятностей встретить особь данного вида

```

##Нулевая модель
 Ожидаемые частоты для нулевой модели, то есть при условии, что все пробы и все виды независимы

```{r}
E <- round(f_i %*% t(f_j) / Ft, 2)

head(E)
```

##Критерий $\chi^2$

```{r}
chisq.test(x=mite, p=p_ij, correct = F)
```

Вычислим величину, которую называют *"инерцией"* 

$$
Inertia = \frac{\chi^2}{Ft}
$$

Она показывает удельный вклад каждой особи в формрование общего отклонения от нулевой модели


```{r, echo=TRUE}
Inertia <- as.numeric(chisq.test(x=mite, p=p_ij, correct = F)$statistic)/Ft
Inertia
```


##Вычисляем матрицу вкладов 

```{r, echo=TRUE}
Q <- (f_ij*Ft - f_i %*% t(f_j))/(Ft*sqrt(f_i %*% t(f_j))) #Матрица, вычисленная через частоты

Q1 <-  (p_ij - p_i %*% t(p_j))/sqrt(p_i %*% t(p_j)) #Матрица, вычисленная через вероятности

#Вычитаем одну матрицу из дугой
round(sum(Q - Q1)) #Это одна  та же матрица!
```

Разность равна нулю! То есть матрицу **Q** можно вычислить и через частоты и через вероятности. 


##Сумма квадратов матрицы вкладов
```{r}
Q <- as.matrix(Q)

sum(Q^2)
```

Это та же самая $Inertia$!


## SVD матрицы вкладов

Матрица $\textbf{Q}$ содержит информацию о вкладе каждого вида в каждой пробе в формирование отклонения от нулевой модели, то есть в формирование сопряжености проб и видов.

Вспомним теорему Экарта-Янга

$$
\textbf{Q}_{n \times p} = \textbf{U}_{n \times p} \textbf{D}_{p \times p} \textbf{V}'_{p \times p} 
$$

То есть, с помощью SVD мы сможем представить матрицу **Q** как результат произведения трех "вспомогательных" матриц.   
Но! эти "вспомогательные" матрицы позволяют редуцировать исходную матрицу.

## SVD матрицы вкладов
```{r, echo=TRUE}
U <- svd(Q)$u 
D <- diag(svd(Q)$d)
V <- svd(Q)$v

```

##Размерности "вспомогательных" матриц
```{r, echo=TRUE}
dim(U) 
```

```{r, echo=TRUE}
dim(D)
```

```{r, echo=TRUE}
dim(V)
```

##Проверим восстаналивается ли матрица Q если использовать "вспомогательные" матрицы, полученные в SVD

```{r, echo=TRUE}
Qsvd <- U %*% D %*% t(V) #матрица "восстановленная" из "вспомогательных" матриц 

round(sum(Q - Qsvd)) #разность между исходной и "восстановленной" матрицами
```

Все работает!

##Поиск собственных значений и собственых векторов

Вспомним: Соственные числа и собственные векторы мы искали для матрицы ассоциации (ковариации).

Эта матрица $\textbf A = X'X $

Где $X$ - это центрированная матрица 

Но! Центрованная матрица, по сути, и есть матрица отклонений от нулевой модели.

Нулевая модель  - Значения признака у всех объектов равны среднему.



##Поиск собственных значений и собственых векторов

В случае, матрицы сопряжения $ \textbf{Q}$ - это тоже матрица отклонений от нулевой модели, поэтому  

$$
\textbf A = Q'Q
$$

Матрица **A** - квадратная симметричная матрица 

##Поиск собственных значений и собственых векторов

```{r}
A <- t(Q) %*% Q

eig_values <- eigen(A)$values #Собственные числа матрицы A
eig_vectors <- eigen(A)$vectors #Матрица собственных векторов для матрицы A 

```

##Сравним с результатами SVD

```{r}
plot(eig_values, diag(D), main = "Квадратичная зависимость!" )
```

##Сравним с результатами SVD{.columns-2}

```{r, echo=FALSE}
plot(eig_values, diag(D)^2, main = "Квадраты сингулярных чисел матрицы Q \nравны собственным значениям \nматрицы ассоциации" )

```


**Обратите внимание на две особенности**    

1. Собственные значения для матрицы ассоциации **A = Q'Q** равны квадратам сингулярных чисел для матрицы **Q**    
2. Последнее собственое значение матрицы **A** равно нулю! Таково ее свойство (это связано с тем, что частоты в матрице **Q** центрированы). То есть матрица **A**, по своей природе, - это сингулярная матрица (ее определитель равен нулю, можете проверить).  Поэтому в дальнейшем анаизе будет использовано на одно собственное значение меньше.

##$Inertia$ - это мера изменчивости!

Вспомним, что мы вычислили показатель $Inertia$, как сумму квадратов матрицы **Q**, или, что тоже самое, $Inertia$ - это удельный $\chi^2$. Из этого пока не следует, что $Inertia$ показывает общую изменчивость в системе. 

Вспомним, что общая дисперсия в системе - это сумма собственных значений. Ранее мы вычислили вектор `eig_values`, содержащий собственные значения матрицы ассоциации **A** = **Q'Q**. 
Найдем сумму всех собственных значений, то есть суммарную диспрсию.

```{r}
sum(eig_values)
```

Это в точости значение $Inertia$!

##Главные оси {.columns-2 .smaller}

В корреспондентном анализе оси, соответствующие собственным значениям матрицы **A**, называются *главными осями* (principal axes).

Информативность главных осей оценивается как отношение значения данного собственного числа к сумме собственных чисел, то есть к $Inertia$

```{r, echo=FALSE}
Information <- data.frame(CA = 1:length(eig_values), Eigenval =round(eig_values, 5), Prop_Explained = round(eig_values/sum(eig_values), 5), Cumul_Prop=round(cumsum(eig_values/sum(eig_values)),5) )

round(Information, 3)
```

Обратите внимание, что последнее собственное значение (№35) равно нулю.



## Свойства главных осей

- Главные оси независимы друг от друга (перпендикулярны)
- Каждая последующая объясняет меньше общей инерции (общего хи-квадрат, а не изменчивости!!!)
- Всего осей может быть не больше чем минимальное из этих значений: (число строк - 1), (число столбцов - 1)
- Первая ось - переменные, которые объясняют максимум зависимости строк от столбцов (значения которых сильнее всего отличаются от ожидаемых для данных объектов)
- Результаты изображаются в виде точечных графиков, похожих на биплоты (осторожно, scaling!)



##Нарисуем ординацию проб в первых двух главных осях {.columns-2 .smaller}

Для этого нам понадобятся первые две колонки матрицы U полученной в SVD (они сответствуют первым двум сингулярным числам и, стало быть, первым двум собственным значениям и первым двум собственным векторам). 
Матрица **Q** имеет два центроида: центроид строк (пробы) и центроид колонок(виды)
Координаты центроидов задаются векторами `p_i` и `p_j`  

Для вычисления координат проб в *главных осях* необходимо значения, приведенные в матрице **U**, разделить на $\sqrt{p_i}$, то есть произвести такую матричную операцию:
$$
\textbf diag(p_i)^{-1/2} \times U
$$


```{r, echo=FALSE}
library(ggplot2)
CA_samples <- diag(p_i^(-1/2))%*% U[,1:2]


ggplot(as.data.frame(CA_samples), aes(x=V1, y=V2) ) + geom_text(label = rownames(mite)) + geom_hline(yintercept=0, linetype = 2) + geom_vline(xintercept = 0, linetype = 2) + theme_bw() + labs(x= "CA1", y = "CA2")

```


##Нарисуем ординацию видов в первых двух главных осях {.columns-2 .smaller}
Для вычисления координат видов в *главных осях* необходимо значения, приведенные в матрице **V** разделить на $\sqrt{p_j}$.   
То есть требуется провести следующее умножение матриц


$$
\textbf diag(p_j)^{-1/2} \times V
$$

```{r, echo=FALSE}
CA_species <- diag(p_j^(-1/2))%*% V[,1:2]


ggplot(as.data.frame( CA_species), aes(x = V1, y = V2) )  + geom_hline(yintercept=0, linetype = 2) + geom_vline(xintercept = 0, linetype = 2) + theme_bw() + labs(x= "CA1", y = "CA2") + geom_text(label = names(mite))

```



##Сравним с результатами специализированной функции `cca()` из пакета `vegan` {.smaller}

Собственные значения
```{r}
summary(cca(mite))$cont$importance [, 1:5]
```

Все аналогично!

##Сравним с результатами специализированной функции `cca()` из пакета `vegan`  {.columns-2 .smaller}

Ординация проб. Все аналогчно результатам, полученным вручную!

```{r}
plot(cca(mite), display = "sites")
```


##Сравним с результатами специализированной функции `cca()` из пакета `vegan` {.columns-2 .smaller}

Ординация видов. Все аналогчно результатам, полученным вручную!
```{r}
plot(cca(mite), display = "species")
```




##Sumary
Принципиальное отличие CA от PCA - это использование матрицы **Q** вместо матрицы исходных значений.   
Матрица **Q** - это матрица вкладов в формирование отклонения от нулевой гипотезы, говорящей, что сопряжения между явлениями нет.   
Вычислительный базис CA - это SVD матрицы **Q**



## Дополнительные ресурсы

- Borcard, D., Gillet, F., Legendre, P., 2011. Numerical ecology with R. Springer.
- Legendre, P., Legendre, L., 2012. Numerical ecology. Elsevier.
- Oksanen, J., 2011. Multivariate analysis of ecological communities in R: vegan tutorial. R package version 2–0.
- The Ordination Web Page URL http://ordination.okstate.edu/ (accessed 10.21.13).
- Quinn, G.G.P., Keough, M.J., 2002. Experimental design and data analysis for biologists. Cambridge University Press.
- Zuur, A.F., Ieno, E.N., Smith, G.M., 2007. Analysing ecological data. Springer.
