---
title: "Анализ главных компонент"
subtitle: "Анализ и визуализация многомерных данных с использованием R"
author: Вадим Хайтов, Марина Варфоломеева
presenters: [{
  name: 'Вадим Хайтов',
  company: 'Каф. Зоологии беспозвоночных, СПбГУ',
  }]
output:
 ioslides_presentation:
  widescreen: true
  css: assets/my_styles.css
  logo: assets/Linmod_logo.png
---
```{r setup, include = FALSE, cache = FALSE, purl = FALSE}
# output options
options(width = 70, scipen = 6, digits = 3)
library(knitr)
# chunk default options
opts_chunk$set(fig.align='center', tidy = FALSE, fig.width = 7, fig.height = 3, warning = FALSE)
```

```{r, libs-funs, echo=FALSE}
library(ggplot2)
library(grid)
theme_set(theme_bw(base_size = 10) + theme(legend.key = element_blank()))
update_geom_defaults("point", list(shape = 19, size = 4))
library(gridExtra)
```

## Вы сможете

- Найти собственые значения матрицы и собственные вектора матрицы и объяснить их смысл.
- Проводить анализ главных компонент при помощи базовых функций R и функций из пакета `vegan`
- Оценивать долю дисперсии, объясненной компонентами
- Снизить размерность данных, оставив небольшое число компонент
- Интерпретировать смысл компонент по их факторным нагрузкам
- Строить ординацию объектов в пространстве главных компонент
- Создавать комплексные переменные и использовать их в других видах анализов

#Еще немного матричной алгебры

##Ортогональные вектора 
  
Скалярное произведение двух векторов 

$$
\textbf{a} \bullet \textbf{b} =  
\begin{pmatrix}
a_1 \\
a_3 \\    
a_4 \\
a_5 \\
a_6 \\
a_7
\end{pmatrix}
\times
\begin{pmatrix}
b_1 &
b_3 &    
b_4 &
b_5 &
b_6 &
b_7
\end{pmatrix}
= x
$$

Скалярное произведение может быть представлено в виде 
$$
\textbf{a} \bullet \textbf{b} = ||a|| \cdot ||b|| \cdot \cos \Theta 
$$

Если $\Theta = 90^{\circ}$, то скалярное произведение равно нулю.   
**Важно!** Те векторы, скалярное произведение которых равно нулю, называются **ортогональными**

##Ортогональная матрица
Это матрица, все колонки которой являются ортогональными векторами
Простейший пример ортогональной матрицы
```{r}
A <- matrix(c( 0,  0, 10,
               0,  5, 0,
               40, 0, 0  ), ncol = 3)
```

Свойство ортогональной матрицы: По главной диагонали произведения $\textbf{A}\textbf{A}'$ будут располагаться квадраты длин векторов (столбцов) 

```{r}
A %*% t(A)
```


##Номированные (Нормализованные) вектора
$$
\textbf{c} = \textbf{b} \cdot 1/||\textbf{b}||
$$

Нормирование векторов, делает их длины равными единице.


##Ортонормальные матрицы  
Это матрицы все вектора (колонки) которой нормированы и при этом ортогональны. 


```{r}
normA <-  matrix(c( 0,  0, 1,
                    0,  1, 0,
                    1,  0, 0  ), ncol = 3)
```

Свойство ортонормальных матриц: $\textbf{A}' = \textbf{A}^{-1}$

```{r}
solve(normA)
```

```{r}
t(normA)
```

##Каноническая форма матрицы ассоциации {.smaller}
Обычно в биологических исследованиях признаки объектов взаимозависимы. То есть между признаками есть ненулевая корреляция.

Однако часто нам необходимо описать наши объекты в терминах *независимых* переменных. То есть вместо $p$ взаимозависимых признаков получить $p$ взамонезависимых признаков, корреляция между которыми равна нулю. 

То же самое на языке матриц
$$ Вместо \textbf{A}_{pp} =  
\begin{pmatrix}
a_{11} & a_{12} & \cdots & a_{1p} \\
a_{21} & a_{22} & \cdots & a_{2p} \\    
\vdots & \vdots & \ddots & \vdots \\
a_{p1} & a_{p2} & \cdots & a_{pp}
\end{pmatrix}
 хотим 
 \pmb{\Lambda} =  
\begin{pmatrix}
\lambda_{11} & 0 \cdots & 0 \\
0 & \lambda_{22} & \cdots & 0 \\    
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & \lambda_{pp}
\end{pmatrix}
$$

##Каноническая форма матрицы ассоциации

Матрица $\pmb{\Lambda}$ называется **канонической формой** матрицы $\textbf{A}$.
Значения, располагающиеся по главной диагонали матрицы $\pmb{\Lambda}$ называются **собственными числами** (eigen values) матрицы $\textbf{A}$. Собственных чисел столько же, сколько признаков, т.е. столбцов в матрице $\textbf{A}$.



##Собственные значения и собственные вектора матрицы
Связь между матрицей $\textbf{A}$ и матрицей $\pmb{\Lambda}$ осуществляется с помощью собственных векторов. Собственных векторов столько же, сколько и собственных чисел. 

$$
\textbf{AV} = \textbf{V}\pmb{\Lambda}
$$

Тогда 

$$
\textbf{A} = \textbf{V}\pmb{\Lambda}\textbf{V}^{-1}
$$

То есть, матрицу ассоциации можно представить в виде произведения двух "вспомогательных" матриц. При этом матрица $\textbf{V}$ ортонормальная матрица, то есть $\textbf{V}' = \textbf{V}^{-1}$.

##Симулированная матрица
```{r, echo=FALSE}
set.seed(12345)
Dat <- data.frame(Sp1 = round(rnorm(7, 150, 5)), Sp2 = round(rnorm(7, 150, 5)), Sp3 = round(rnorm(7, 150, 5)), Sp4 = round(rnorm(7, 150, 5)), Sp5 = round(rnorm(7, 150, 5)) ) 
rownames(Dat) <- c("Sample1", "Sample2", "Sample3", "Sample4", "Sample5", "Sample6", "Sample7" )
Dat
```

Найдем матрицу ассоциации признаков

```{r}
A <- round(cor(Dat), 2)
A
```

##Вычислим собственные числа и  собственные вектора

```{r}
EIG <- eigen(A)
L <- EIG$values #Вектор собственных чисел
V <- EIG$vectors #Матрица собственных векторов
```

Матрица $\textbf{V}$ ортонормальная матрица, то есть должно выполняться $\textbf{V}' = \textbf{V}^{-1}$.   
Проверим
```{r}
round(t(V) - solve(V))
```



##Разложение матрицы ассоциации
Матрица ассоциации
```{r}
A
```

Восстановленная через собственные вектора и собственные числа матрица
```{r}
round(V %*% diag(L) %*% t(V), 2)

```



## Свойства собственных векторов:

- Собственные вектора и собственные числа есть только у некоторых квадратных матриц
- У матрицы $n \times n$ будет _n_ собственных векторов
- Собственные вектора перпендикулярны друг другу (ортогональны)
- Собственные вектора задают направления в пространстве, вдоль которых дисперсия максимальна
- Первый собственны вектор, связан с первым собственным числом (максимальное значение $\lambda_i$), соответствует максимальной дисперсии, второй - чуть меньшей и т.д.



# Анализ главных компонент: Principal Component Analysis


## Суть PCA - проекция точек в многомерном пространстве на пространство с меньшей  размерностью

![Flocks of peep sandpipers in the marsh](./images/Migration-DonMcCullough-Flickr.jpg)

Migration by Don McCullough
on [Flickr](https://flic.kr/p/fEFhCj)

## Не все проекции несут важную информацию

![black shadows for a white horses / les negres ombres dels cavalls blancs by  Ferran Jordà](./images/BlackShadows-FerranJorda-Flickr.jpg)

black shadows for a white horses / les negres ombres dels cavalls blancs by  Ferran Jordà
on [Flickr](https://flic.kr/p/9XJxiL)

## Можно найти оптимальную проекцию, чтобы сохранить максимум информации в минимуме измерений

![Cat's shadow](./images/CatsShadow-MarinaDelCastell-Flickr.jpg)

Cat's shadow by Marina del Castell on [Flickr](https://flic.kr/p/ebe5UF)



## Применение PCA

- Снижение размерности в многомерных данных
- сжатие изображений
- распознавание лиц
- анализ формы (геометрическая морфометрия)
- создание комплексных признаков для других анализов


## Описание связи нескольких переменных - построение матрицы ассоциации 

- Ковариация - мера совместного варьирования двух признаков

$Cov(X, Y) = \frac {\sum_{i = 1}^n{(x_i - \bar x)(y_i - \bar y)}}{n - 1}$,   
$- \infty < Cov(X, Y) < \infty$

```{r, echo=FALSE, fig.width=10, fig.height=4}
# Function adapted from http://stats.stackexchange.com/a/15035/21302
# returns a data frame of two variables which correlate with a population correlation of rho
# If desired, one of both variables can be fixed to an existing variable by specifying x
getBiCop <- function(n, rho, mar.fun=rnorm, x = NULL, ...) {
     if (!is.null(x)) {X1 <- x} else {X1 <- mar.fun(n, ...)}
     if (!is.null(x) & length(x) != n) warning("Variable x does not have the same length as n!")
     C <- matrix(rho, nrow = 2, ncol = 2)
     diag(C) <- 1
     C <- chol(C)
     X2 <- mar.fun(n)
     X <- cbind(X1,X2)
     # induce correlation (does not change X1)
     df <- X %*% C
     df <- as.data.frame(df)
     colnames(df) <- c("X", "Y")
     ## if desired: check results
     #all.equal(X1,X[,1])
     #cor(X)
     return(df)
}
corrs <- c(-0.9, -0.5, 0, 0.5, 0.9)
set.seed(245817)
dats <- lapply(corrs, getBiCop, n = 20, x = rnorm(20, sd = 1.7))

g_covr <- lapply(1:length(dats), function(i) ggplot(data = dats[[i]], aes(x = X, y = Y)) + geom_point(alpha = 0.7, colour = "steelblue") + coord_equal(xlim = c(-7, 7), ylim = c(-7, 7))+ theme(axis.text = element_blank(), axis.ticks = element_blank(), plot.margin = unit(c(0.5, 0.5, 0.5, 0.5), "mm"), axis.ticks.margin = unit(0, "mm"), panel.margin = unit(0.5, "mm")) + annotate(geom = "text", x = 0, y = 6, label = paste("cov =", round(cov(dats[[i]]$X, dats[[i]]$Y), 2)), size = 8) + annotate(geom = "text", x = 0, y = -6, label = paste("r =", round(cor(dats[[i]]$X, dats[[i]]$Y), 2)), size = 8))

```

- Корреляция Пирсона - стандартизованная ковариация

$Cor(X, Y) = \frac {Cov(X, Y)}{SD_X SD_Y}$,    
$- 1 < Cor(X, Y) < 1$


##Ковариация и корреляция
```{r, echo=FALSE, fig.height=5}
grid.arrange(g_covr[[1]], g_covr[[2]], g_covr[[3]], g_covr[[4]],  nrow = 2)
```


## Ковариация зависит от масштаба шкалы

```{r, echo=FALSE, fig.height=3}
set.seed(245817)
dat_scale <- getBiCop(n = 20, rho = 0.8, x = rnorm(20, sd = 1.7))
dat_scale$Y10 <- dat_scale$Y*10

g_scat <- ggplot(data = dat_scale, aes(x = X, y = Y)) +
    geom_point(alpha = 0.7, colour = "steelblue") + 
    coord_cartesian(xlim = c(-7, 7), ylim = c(-7, 7))+ 
    theme(plot.margin = unit(c(0.5, 0.5, 0.5, 0.5), "mm"), panel.margin = unit(0.5, "mm")) 

g_unscaled <- g_scat + 
    annotate(geom = "text", x = 0, y = 6, label = paste("cov =", round(cov(dat_scale$X, dat_scale$Y), 2)), size = 8) + 
    annotate(geom = "text", x = 0, y = -6, label = paste("r =", round(cor(dat_scale$X, dat_scale$Y), 2)), size = 8)

g_scaled <- g_scat + aes(y = Y10) + coord_cartesian(xlim = c(-7, 7), ylim = c(-70, 70)) + 
    annotate(geom = "text", x = 0, y = 60, label = paste("cov =", round(cov(dat_scale$X, dat_scale$Y10), 2)), size = 8) + 
    annotate(geom = "text", x = 0, y = -60, label = paste("r =", round(cor(dat_scale$X, dat_scale$Y10), 2)), size = 8)

grid.arrange(g_unscaled + ggtitle(""), g_scaled + ggtitle("Масштаб Y увеличен"), nrow = 1)
```

## Ковариация чувствительна к выбросам

```{r, echo=FALSE, fig.height=3}
dat_scale$Y_outlier <- dat_scale$Y
dat_scale <- dat_scale[order(dat_scale$Y_outlier), ]
dat_scale$Y_outlier[19] <- dat_scale$Y[19] + 15

g_scat_out <- g_scat %+% dat_scale + aes(y = Y_outlier) + coord_cartesian(xlim = c(-7, 7), ylim = c(-7, 26)) + 
    annotate(geom = "text", x = 0, y = 23, label = paste("cov =", round(cov(dat_scale$X, dat_scale$Y_outlier), 2)), size = 8) + 
    annotate(geom = "text", x = 0, y = -5, label = paste("r =", round(cor(dat_scale$X, dat_scale$Y_outlier), 2)), size = 8)

grid.arrange(g_unscaled + ggtitle("Без выбросов"), g_scat_out + ggtitle("С выбросом"), nrow = 1)
```

## Матрица ковариаций - описывает совместное варьирование множества переменных

$$
C = 
\begin{pmatrix}
cov(x, x) & cov(x, y) & cov(x, z) \\
cov(y, x) & cov(y, y) & cov(y, z) \\
cov(z, x) & cov(z, y) & cov(y, z) \\
\end{pmatrix}$$

Поскольку дисперсия - это ковариация признака с самим собой

$$Cov(X, X) = \frac {\sum_{i = 1}^n{(x_i - \bar x)(x_i - \bar x)}}{n - 1} = \frac {\sum_{i = 1}^n{(x_i - \bar x)^2}}{n - 1} = SD^2$$

большая диагональ матрицы ковариаций содержит дисперсии признаков

$$
С = \begin{pmatrix}
SD_x^2 & cov(x, y) & cov(x, z) \\
cov(y, x) & SD_y^2 & cov(y, z) \\
cov(z, x) & cov(z, y) & SD_z^2 \\
\end{pmatrix}$$

> - __Раз матрица ковариаций описывает изменчивость всех признаков, хорошо бы научиться находить с ее помощью максимально варьирующие направления...__

## Пример работы PCA: морфометрия медуз из реки Хоксбери 

Данные о размерах медуз _Catostylus mosaicus_ из реки Хоксбери (Новый Южный Уэльс, Австралия). Часть медуз собрана на острове Дангар, другая - в заливе Саламандер.


![Bluber jellies](./images/Blubberjellies-KirstiScott-Flickr.jpg)

Blubber jellies! by  Kirsti Scott
on [Flickr](https://flic.kr/p/nWikVp)

<div class = "footnote">Данные из Lunn & McNeil 1991</div>


## Двумерные исходные данные {.columns-2}

```{r echo = FALSE, fig.width=5}
jelly <- read.delim("data/jellyfish.csv")
library(ggplot2)
theme_set(theme_bw(base_size = 16) + theme(legend.key = element_blank()))
update_geom_defaults("point", list(shape = 19, size = 4))
p_raw <- ggplot(jelly, aes(x = width, y = length)) + 
  geom_point() + 
  geom_hline(yintercept = 0) + 
  geom_vline(xintercept = 0) + 
  coord_equal()
p_raw
```


```{r, echo = FALSE}
head(jelly, 10)
```

## Центрируем исходные данные {.columns-2}

```{r echo = FALSE, fig.width=5}
jelly$c_width <- jelly$width - mean(jelly$width)
jelly$c_length <- jelly$length - mean(jelly$length)
jelly <- jelly[order(jelly$c_width), ]
# library(dplyr)
# jelly <- jelly %>% 
#   mutate(c_width = width - mean(width),
#          c_length = length - mean(length))

p_centered <- ggplot(jelly, aes(x = c_width, y = c_length)) + 
  geom_point() + 
  geom_hline(yintercept = 0) + 
  geom_vline(xintercept = 0) + 
  coord_equal()
p_centered + ggtitle("Центр координат сместился")
```


```{r echo=FALSE}
head(jelly[, c(1, 4:5)], 10)
```

## Находим новые оси с помощью собственных векторов и собственных чисел {.columns-2}

```{r echo = FALSE, fig.width=5}
#covariance and new axes
Covar <- cov(jelly[, c("c_width", "c_length")])
eigen_values <- eigen(Covar)$values
eigen_vectors <- eigen(Covar)$vectors


jelly$pc_axis1 <- eigen_vectors[2,1]/eigen_vectors[1,1] * jelly$c_width
jelly$pc_axis2 <- eigen_vectors[2,2]/eigen_vectors[1,2] * jelly$c_width


# plot of future axes
p_futureax <- ggplot(jelly, aes(x = c_width, y = c_length)) + 
  geom_point() + 
  geom_hline(yintercept = 0) + 
  geom_vline(xintercept = 0) +
  geom_line(aes(x = c_width, y = pc_axis1), linetype = "dashed") +
  geom_line(aes(x = pc_axis2, y = c_width), linetype = "dashed") + 
  coord_equal() +
  geom_segment(aes(x = 0, y = 0, xend = eigen_vectors[1, 1], yend = eigen_vectors[2,1]), colour = "red", size = 2) + 
  geom_segment(aes(x = 0, y = 0, xend = eigen_vectors[1, 2], yend = eigen_vectors[2,2]), colour = "red", size = 2)

p_futureax + ggtitle("Новые оси")
```

Матрица ковариаций:
```{r echo=FALSE}
Covar
```

Собственные числа:
```{r echo=FALSE}
eigen_values
```

<br />
Собственные вектора:
```{r echo=FALSE}
eigen_vectors
```


## Пересчитываем координаты точек в новом пространстве

```{r echo=FALSE, fig.width=5}
p_futureax + ggtitle("До поворота осей")
# p_futureax + 
#   geom_line(aes(x = c_width, y = predict(lm(c_length ~ c_width))), colour = "red", size = 1) + 
#   geom_line(aes(x = predict(lm(c_width ~ c_length)), y = c_length), colour = "steelblue", size = 1)
```


## Пересчитываем координаты точек в новом пространстве

```{r echo=FALSE, fig.width=5, message=FALSE}
# row_feature_vec <- t(eigen_vectors)
# row_data_centered <- t(jelly[, 4:5])
# findat <- row_feature_vec %*% row_data_centered
# fincoord <- as.data.frame(t(findat))

fincoord <- as.matrix(jelly[,4:5]) %*% eigen_vectors

fincoord <- as.data.frame(fincoord)

colnames(fincoord) <- c("PC1", "PC2")

p_rotated <- ggplot(fincoord, aes(x = PC1, y = PC2)) +
  geom_point() +   
  geom_hline(yintercept = 0) + 
  geom_vline(xintercept = 0) + 
  coord_equal(ratio = 1, ylim = c(-10, 10))

p_rotated + ggtitle("После поворота осей")


# Можно добыть данные обратно
# row_data_backtransformed <- t(row_feature_vec) %*% findat
# row_data_orig <- t(row_feature_vec) %*% findat + orig_mean

# То же самое в vegan
# Вектора масштабированы
# library(vegan)
# jelly_pca <- rda(jelly[, 4:5], scale = TRUE)
# biplot(jelly_pca) 
# spec <- as.data.frame(scores(jelly_pca, display=c("sites"), scaling = 3))
# vars <- as.data.frame(scores(jelly_pca, display=c("species"), scaling = 3))
# p_rotated + geom_segment(data = jelly_vars, aes(x = 0, y = 0, xend = PC1, yend = PC2, label = rownames(jelly_vars))) + coord_equal(xlim = c(-3.5, 4.5), ylim = c(-1, 1))
```



## Все то же самое с техникой SVD

```{r}
SVD <-svd(jelly[,4:5])
U <- SVD$u
V <- SVD$v
D <- SVD$d
```

## Сравним результаты {.columns-2}
Новые координаты в PCA и "реконструированные" значения в SVD

```{r, echo=FALSE}
qplot(fincoord$PC1, U %*% diag(D) [,1], geom = "point") + labs(x="PC1")
```


Собственные вектора, вычисленные по матрице ковариации 
```{r, echo=FALSE}
eigen_vectors
```

Правая матрица сингулярных векторов
```{r, echo=FALSE}
V
```


## PCA и  SVD

PCA - это частный случай SVD, в котором в качестве исходной матрицы взяты центрированные величины.


## Медузы упорядочились по размеру

```{r echo=FALSE, fig.width=9, message=FALSE}
p_rotated + 
  aes(colour = jelly$location) + 
  scale_color_brewer("Location", palette = "Set1") +
  coord_equal()
```

- PC1 - больше всего изменчивости
- PC2 - то, что осталось

## PCA для действительно многомерных данных
Пример: Потребление белков в странах Европы с разными видами продуктов питания

![Paleo Diet by zsoolt](./images/PaleoDiet-zsoolt-Flickr.jpg)

Paleo Diet by zsoolt on [Flickr](https://flic.kr/p/pPK1nz)

<div class = "footnote">Данные из Weber, 1973</div>


## Открываем данные {.smaller}

```{r, echo=FALSE}
#library(XLConnect, quietly = TRUE)
# protein <- readWorksheetFromFile(file="protein.xls", sheet = 1)
protein <- read.table(file="data/protein.csv", sep="\t", dec=".", header=TRUE)
protein$region <- factor(protein$region)
rownames(protein) <- protein$country
head(protein)
```

<div class = "footnote">Данные из Weber, 1973</div>


## Задание

- Постройте ординацию стран при помощи nMDS с использованием евклидова расстояния
- Постройте график ординации.
- Нанесите при помощи envfit вектора изменения исходных переменных.

##Решение

```{r, message=FALSE, fig.width = 7}
library(vegan)
ord_nmds <- metaMDS(protein[, 3:ncol(protein)], distance = "euclidean", trace = 0)
ef <- envfit(ord_nmds, protein[, 3:ncol(protein)])
plot(ord_nmds, display = "site", type = "t", cex = 0.8)
plot(ef)
```

## Проведем исследование тех же данных с помощью PCA

```{r, message=FALSE, eval = FALSE}
library(vegan)
prot_pca <- rda(protein[, -c(1, 2)], scale = TRUE)
summary(prot_pca)
```

```{r, echo=FALSE}
prot_pca <- rda(protein[, -c(1, 2)], scale = TRUE)
```

## Собственные вектора и собственные числа

```
Eigenvalues, and their contribution to the correlations 

Importance of components:
                         PC1    PC2    PC3    PC4     PC5     PC6  ...  
Eigenvalue            4.0064 1.6350 1.1279 0.9547 0.46384 0.32513 ...
Proportion Explained  0.4452 0.1817 0.1253 0.1061 0.05154 0.03613 ...
Cumulative Proportion 0.4452 0.6268 0.7521 0.8582 0.90976 0.94589 ...
```

## Свойства главных компонент (Principal Components)

- Не скоррелированы друг с другом (ортогональны)
- Вдоль компонент максимальный разброс
- Чем больше собственное число (__Eigenvalue__), тем больше дисперсия вдоль оси (__Proportion Explained [variance]__)


## Сколько компонент нужно оставить?

- Эмпирические правила
    - Объясняют больше чем по Broken Stick Model.
    - Компоненты у которых соб. число > 1 (правило Кайзера-Гатмана)
    - В сумме объясняют заданный % от общей изменчивости (60-80%) - слишком субъективно

> - Это условности!

## Постройте график собственных чисел в ggplot

### Собственные числа можно добыть так:

```{r}
eigenvals(prot_pca) # собственные числа
bstick(prot_pca) # ожидаемое по Brocken Stick Model
```

```{r, echo=FALSE}
# Вам понадобятся
# geom_line(aes(group = 1))
# geom_point()
```


## Графики собственных чисел {.columns-2}

```{r, evplot, echo = FALSE}
screeplot(prot_pca, type = "lines", bstick = TRUE) # график собственных чисел

```





```{r, echo=FALSE}
# Данные для ggplot
eig <- data.frame(pc = factor(1:length(eigenvals(prot_pca))),
           eigenval = as.vector(eigenvals(prot_pca)), 
           bstick = bstick(prot_pca))
# График
eig_p_kaiser <- ggplot(eig, aes(x = pc, y = eigenval)) + 
  geom_line(aes(group = 1), colour = "red") + geom_point(colour = "red") + 
  geom_hline(yintercept = mean(eigenvals(prot_pca)), colour = "gray70") +
  labs(x = "Компоненты", y = "Собственные числа")
# график с brocken stick model
eig_p_bstick <- eig_p_kaiser + 
  geom_line(aes(x = pc, y = bstick, group = 1)) + 
  geom_point(aes(x = pc, y = bstick))

eig_p_bstick

```

## Интерпретация компонент {.smaller .columns-2}

Факторные нагрузки оценивают вклады переменных в изменчивость по главной компоненте

- Модуль значения нагрузки - величина вклада 
- Знак значения нагрузки - направление вклада

```{r}
scores(prot_pca, display = "species", 
       choices = c(1, 2, 3), scaling = 0)
```

### Первая главная компонента:

Высокие __положительные нагрузки по первой главной компоненте__ у переменных `cereals` и `nuts`. Значит, чем больше значение первой компоненты, тем больше потребление злаков и орехов.

Высокие __отрицательные нагрузки__ у переменных `eggs`, `milk`, `whitemeat`, `redmeat`. Значит, чем меньше значение первой компоненты, тем больше потребление яиц, молока, белого и красного мяса. 

> - Т.е. первую компоненту можно назвать "Мясо - злаки и орехи"

## Интерпретация компонент {.smaller .columns-2}

Факторные нагрузки оценивают вклады переменных в изменчивость по главной компоненте

- Модуль значения нагрузки - величина вклада 
- Знак значения нагрузки - направление вклада

```{r}
scores(prot_pca, display = "species", 
       choices = c(1, 2, 3), scaling = 0)
```


### Вторая главная компонента:

Высокие __положительные нагрузки по второй главной компоненте__ у переменных `fish`, `frveg`. Значит, чем больше значение второй компоненты, тем больше потребление рыбы, овощей.

Высоких __отрицательных нагрузкок по второй главной компоненте__ нет ни у одной из переменных. 

> - Т.е. вторую компоненту можно назвать "Потребление рыбы и овощей"


## Параметр `scaling`

Внимание! Координаты объектов или переменных можно получить в нескольких вариантах, отличающихся масштабом. От этого масштаба будет зависеть интерпретация.

```{r, echo=FALSE, results='asis'}
df <- data.frame(
  scaling = c("1", "2", "3", "0"),
  bip = c("биплот расстояний", "биплот корреляций", "", ""),
  scaled = c("координаты объектов масштабированы (х корень из соб. чисел)", "координаты признаков масштабированы (х корень из соб. чисел)", "масштабированы координаты объектов и признаков (х корень 4-й степени из соб. чисел)", "нет масштабирования"),
  dist = c("апроксимируют евклидовы", "НЕ апроксимируют евклидовы", "", ""),
  ang = c("нет смысла", "отражают корреляции", "", "")
  )

colnames(df) <- c("scaling", "Название графика", "Масштаб", "Расстояния между объектами", "Углы между векторами")

kable(df)
```


## Можно нарисовать график факторных нагрузок {.columns-2 .smaller}

```{r, fig.width=5}
biplot(prot_pca, display = "species", scaling = 2)
```


```{r}
scores(prot_pca, display = "species", 
       choices = c(1, 2, 3), scaling = 2)
```


## График факторных нагрузок в ggplot

```{r, load-p, echo=FALSE}
df_load <- as.data.frame(scores(prot_pca, display = "species", 
                                choices = c(1, 2, 3), scaling = 2))
# поправки для размещения подписей
df_load$hjust[df_load$PC1 >= 0] <- -0.1
df_load$hjust[df_load$PC1 < 0] <- 1
df_load$vjust[df_load$PC2 >= 0] <- -0.1
df_load$vjust[df_load$PC2 < 0] <- 1
library(grid) # для стрелочек
ar <- arrow(length = unit(0.25, "cm"))

p_load <- ggplot(df_load) + 
  geom_text(aes(x = PC1, y = PC2, label = rownames(df_load)), 
            size = 5, vjust = df_load$vjust, hjust = df_load$hjust) + 
  geom_segment(aes(x = 0, y = 0, xend = PC1, yend = PC2), 
               colour = "grey40", arrow = ar) + 
  coord_equal(xlim = c(-1.9, 1.9), ylim = c(-1.9, 1.9))
p_load
```


##Ординация объектов {.columns-2 .smaller}

Факторные координаты отражают положение объектов в пространстве главных компонент

```{r, fig.height=5, fig.width=5}
biplot(prot_pca, display = "sites")
```


```{r}
# Значения факторов (= факторные координаты)
head(scores(prot_pca, display = "sites", choices = c(1, 2, 3), scaling = 1))
```

## График ординации в ggplot

```{r, fig.height=5, fig.width=7, echo=FALSE}
df_scores <- data.frame(protein[, 1:2],
  scores(prot_pca, display = "sites", choices = c(1, 2, 3), scaling = 1))

p_scores <- ggplot(df_scores, aes(x = PC1, y = PC2, colour = region)) + geom_point() + 
  geom_text(aes(label = country), hjust = 1, vjust = -1) + 
  coord_equal(xlim = c(-1.4, 1.4), ylim = c(-1.4, 1.4))
p_scores
```


## Несколько графиков рядом

```{r, warning=FALSE, fig.width=10, echo=FALSE}
library(gridExtra)
grid.arrange(p_load, p_scores, ncol = 2, widths = c(0.40, 0.60))
```


## В данном случае, результаты PCA похожи на nMDS {.columns-2}

```{r, compare-nmds, warning=FALSE, echo = FALSE, fig.height=8, fig.width=14, echo=FALSE, message=FALSE, results='hide'}
nMDS <- ggplot(as.data.frame(metaMDS(protein[, -c(1:2)])$points), aes(x=MDS1, y=MDS2)) + geom_point(aes(color = protein$region))

```

```{r, echo = FALSE, fig.height=5, fig.width=5}
p_scores + guides(color=FALSE)
nMDS + guides(color=FALSE)
```


## В этом примере результаты PCA похожи на nMDS потому что:

- исходные признаки - количественные, нормально распределенные переменные, связанные друг с другом линейно. Для ординации объектов с такими признаками подходит PCA. Для описания различий между объектами подходит евклидово расстояние
- для данной nMDS-ординации использовано евклидово расстояние
- расстояния между объектами на любой ординации PCA соответствуют их евклидовым расстояниям в пространстве главных компонент

## Создание комплексных переменных

Факторные координаты - это новые составные признаки, которых можно использовать вместо исходных переменных

Свойства факторных координат:

- Среднее = 0, Дисперсия = 1
- Не коррелируют друг с другом

Применение:

  - Уменьшение числа зависимых переменных - для дисперсионного анализа
  - Уменьшение числа предикторов - во множественной регрессии

```{r, echo=FALSE}
# Значения факторов (= факторные координаты)
head(scores(prot_pca, display = "sites", 
       choices = c(1, 2, 3), scaling = 1))
```

## При помощи дисперсионного анализа проверьте, различается ли значение первой главной компоненты ("Мясо - злаки и орехи") между разными регионами Европы


```{r}
# Значения факторов (= факторные координаты)
df <- data.frame(region = protein$region,
  scores(prot_pca, display = "sites", choices = c(1, 2, 3), scaling = 1))
mod <- lm(PC1 ~ region, data = df)
anova(mod)
```

> - Регионы Европы различаются по потреблению мяса, злаков и орехов

## Проверка условий применимости дисперсионного анализа


```{r, fig.width=10, fig.height=4, echo = FALSE}
mod_diag <- fortify(mod)
res_p <- ggplot(data = mod_diag, aes(x = .fitted, y = .stdresid)) + geom_point(aes(size = .cooksd)) + geom_hline(yintercept = 0) + geom_smooth(method="loess", se=FALSE) 
mean_val <- mean(mod_diag$.stdresid)
sd_val <- sd(mod_diag$.stdresid)
norm_p <- ggplot(mod_diag, aes(sample = .stdresid)) + geom_point(stat = "qq") + geom_abline(intercept = mean_val, slope = sd_val)
grid.arrange(res_p, norm_p, ncol = 2, widths = c(0.55, 0.45))
```

> - Условия применимости дисперсионного анализа выполняются


## График значений первой компоненты по регионам

```{r, pc1_p, fig.width = 10, fig.height=6, echo=FALSE}
df$region <- reorder(df$region, df$PC1, FUN=mean)
ggplot(df, aes(x = region, y = PC1, colour = region)) + 
  stat_summary(geom = "pointrange", fun.data = "mean_cl_boot", size = 1) + 
  theme(axis.text.x = element_text(angle = 30, vjust = 1, hjust = 1)) 
```


## Пост-хок тест

```{r}
TukeyHSD(aov(mod))
```

  

## Summary

- Применение метода главных компонент (PCA):
    - снижение размерности данных
    - исследование связей между переменными
    - построение ординации объектов
    - создание комплексных переменных
- Терминология:
    - Собственные числа - вклад компонент в общую изменчивость
    - Факторные нагрузки - корреляции исходных переменных с компонентами - используются для интерпретации
    - Значения факторов - новые координаты объектов в пространстве уменьшенной размерности



## Что почитать

- Borcard, D., Gillet, F., Legendre, P., 2011. Numerical ecology with R. Springer.
- Legendre, P., Legendre, L., 2012. Numerical ecology. Elsevier.
- Oksanen, J., 2011. Multivariate analysis of ecological communities in R: vegan tutorial. R package version 2–0.
- The Ordination Web Page [WWW Document], n.d. URL http://ordination.okstate.edu/ (accessed 10.21.13).
- Quinn, G.G.P., Keough, M.J., 2002. Experimental design and data analysis for biologists. Cambridge University Press.
- Zuur, A.F., Ieno, E.N., Smith, G.M., 2007. Analysing ecological data. Springer.

