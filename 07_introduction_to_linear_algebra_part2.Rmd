---
title: "Краткое введение в мир линейной алгебры. Часть 3"
subtitle: "Анализ и визуализация многомерных данных с использованием R"
author: Вадим Хайтов, Марина Варфоломеева
presenters: [{
    name: 'Вадим Хайтов',
    company: 'Каф. Зоологии беспозвоночных, СПбГУ',
    }]
output:
 ioslides_presentation:
  widescreen: true
  css: assets/my_styles.css
  logo: assets/Linmod_logo.png
---

## Вы сможете
- Объяснить что такое матрицы и какие бывают их основные разновидности
- Выполнить базовые операции с матрицами с использованием функций R
- Применить в среде R методы линейной алгебры для решения простейших задач

```{r setup, include = FALSE, cache = FALSE, purl = FALSE}
# output options
options(width = 70, scipen = 6, digits = 3)
library(knitr)
library(ggplot2)
library(grid)
library(gridExtra)


# chunk default options
opts_chunk$set(fig.align='center', tidy = FALSE, fig.width = 7, fig.height = 3, warning = FALSE)

ar <- arrow(type = "closed", length = unit(0.15,"cm"))
```


# Level 8:  Собственные значения, собственные векторы и главные оси 

## Постановка проблемы

Одним из подходов к снижению размерности многомерного пространства признаков является представление исходной матрицы ($\textbf{Y}$) в **ортогональной форме**. 

В исходной матрице большинство векторов неортогональны, то есть взаимозависимые (между ними существует корреляция).  

Задача сводится к тому, чтобы заменить исходные векторы (признаки) новыми комплексными признаками, которые были бы взаимно независимыми, то есть ортогональными. 

Каждый из этих векторов несет определнную часть  информации об исходной системе векторов (берет на себя некоторую долю общей дисперсии). 




## Собственные числа и собственные векторы  {.smaller .columns-2}

Связь между векторами в исходной матрице $\textbf{Y}$ выражается через ковариационную  матрицу  $\textbf{S}$ или корреляционную матрицу ($\textbf {R}$).   

$\textbf{S}$ - это квадратная симметричная матрица.

Задача сводится к тому, чтобы из матрицы $\textbf{S}$


получить матрицу

$$ 
\bf{\Lambda} =  
\begin{pmatrix}
\lambda_{1} & 0 & \cdots & 0 \\
0 & \lambda_{2} & \cdots & 0 \\    
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & \lambda_{p}
\end{pmatrix}
$$

По главной диагонали лежат так называемые **собственные значения**, все остальные значения равны нулю  

Каждому собственному значению $\lambda_i$ ставится в соответствие так называемый **собственный вектор**  $\textbf{u}_i$. Совокупность собственных векторов формирует матрицу собственных векторов $\textbf{U}$

Собственные векторы перпендикулярны друг другу (ортогональны). 


## Связь между матрицами

Связь между матрицей $\bf{\Lambda}$ и $\textbf{S}$ выражается следующим уравнением

$$
\textbf{S}\textbf{U} = \textbf{U}\bf{\Lambda} 
$$


где   
$\textbf{S}$ - матрица ковариации    
$\textbf{U}$ - матрица собственных векторов     
$\bf{\Lambda}$ - Диагональная матрица собственных значений     

Тогда 

$$
\textbf{S} = \textbf{U}\bf{\Lambda}\textbf{U}^{-1} 
$$
Это позволяет раскладывать матрицу $\textbf{S}$  на составляющие (eigenvalues decomposition). 

Каждое собственное число в матрице $\bf{\Lambda}$ (и соответствующий ему ветор из матрицы $\textbf{U}$) несет часть информации об исходной матрице $\textbf{S}$.  

Разложение матрицы позволяет использовать не всю информацию, а только ее наиболее важную часть. 
 

## Геомтерическая интерпретация: Главные оси {.smaller}



Во многих многомерных методах требуется найти оси максимального варьирования 

```{r, purl=TRUE}
set.seed(123456789)

x <- rnorm(1000, 50, 10)
y <- 10 * x + rnorm(1000, 0, 100)
XY <-data.frame(x = x, y = y)
qplot(XY$x, XY$y) + labs(x = "Переменная 1", y = "Переменная 2") + 
  geom_point(aes(x = mean(x), y = mean(y)), size = 4, color = "yellow")

```




## Геомтерическая интерпретация: Главные оси {.smaller}

```{r, purl=TRUE}
qplot(XY$x, XY$y) + labs(x = "Переменная 1", y = "Переменная 2") + 
  geom_point(aes(x = mean(x), y = mean(y)), size = 4, color = "yellow") + 
  stat_ellipse(type = "norm", level = 0.9993) + 
  geom_segment(aes(x = mean(x), y = mean(y), xend = (mean(x) + 3.5*sd(x)), yend = (mean(y) + 3.5*sd(y))),  arrow = ar, color = "blue", size = 2)

```


Главные оси проходят через центроид ($\mathbf{\mu} = [\mu_1, \mu_2 \dots \mu_n]$) и некоторую точку на поверхности эллипса ($\mathbf{y} = [y_1, y_2 \dots y_n]$).

Задача: найти такую точку.

Эта точка соответствует максимальной длине вектора, соединяющего центроид и данную точку.


## Нормализуем векторы {.smaller}

```{r, purl=TRUE}
x_norm <- XY$x/sqrt(sum(XY$x)^2)
y_norm <- XY$y/sqrt(sum(XY$y)^2)


XY_norm <- data.frame(x = x_norm, y = y_norm)


ggplot(XY_norm , aes(x = x, y = y)) + geom_point() +  
  geom_point(aes(x = mean(x), y = mean(y)), size = 4, color = "yellow")  + 
  labs(x = "Нормализованная Переменная 1", y = "Нормализованная Переменная 2")

```



## Центрируем нормализованные векторы {.smaller}


```{r, purl=TRUE, fig.height=4}
XY_norm_cent <- as.data.frame(scale(XY_norm, center = TRUE, scale = FALSE))

ggplot(XY_norm_cent , aes(x = x, y = y)) + geom_point() +  
  geom_point(aes(x = mean(x), y = mean(y)), size = 4, color = "yellow") + 
  labs(x = "Центрированная Нормализованная Переменная 1", y = "Центрированная Нормализованная Переменная 2")

```




## Находим ковариационную матрицу {.smaller}

```{r, purl=TRUE}
mXY_norm_cent <- as.matrix(XY_norm_cent)

Sxy_norm_cent <- t(mXY_norm_cent) %*% mXY_norm_cent /(nrow(mXY_norm_cent) - 1)

Sxy_norm_cent
```


## Находим собственные числа и собственные векторы {.smaller} 

```{r, purl=TRUE}
eig <- eigen(Sxy_norm_cent) # Стандартная функция R для извлечения собственных чисел и собственных векторов

Lambda <- eig$values # Собственные числа

Lambda

U <- eig$vectors # Собственные векторы

U
```

## Стандартизованные собственные векторы {.smaller}
Собственные векторы безразмерны.

Но их можно масштабировать, выразив в величинах стандартных отклонений $\textbf{U}_{scaled} = \textbf{U}\bf{\Lambda}^{1/2}$  

Почему так?

$\textbf{U}_{scaled} \textbf{U}'_{scaled} = [\textbf{U}\bf{\Lambda}^{1/2}][\textbf{U}\bf{\Lambda}^{1/2}]' = \textbf{U}\bf{\Lambda}\textbf{U}' = \textbf{U}\bf{\Lambda}\textbf{U}^{-1} = \textbf{S}$

Ковариационная матрица для стандартизованной матрицы собственных векторов совпадает с ковариационной матрицей исходной матрицы



```{r, purl=TRUE}
U_scaled <- U %*% sqrt(diag(Lambda)) #

(U %*% sqrt(diag(Lambda))) %*% t(U %*% sqrt(diag(Lambda)))

```


## Рисуем собственные векторы {.smaller}

```{r, purl=TRUE}

PC1 <- data.frame(x = c(mean(XY_norm_cent$x), U_scaled[1, 1]), 
                  y = c(mean(XY_norm_cent$y),  U_scaled[2,1]))

PC2 <- data.frame(x = c(mean(XY_norm_cent$x),  U_scaled[1, 2]), 
                  y = c(mean(XY_norm_cent$y),  U_scaled[2,2]))

ggplot(XY_norm_cent, aes(x = x, y = y)) + geom_point() +  
  geom_point(aes(x = mean(x), y = mean(y)), size = 4, color = "yellow") + 
  geom_line(data = PC1, aes(x = x, y = y), color = "yellow", size = 1)  +
  geom_line(data = PC2, aes(x = x, y = y), color = "yellow", size = 1) +
  coord_equal()


```

## Рисуем главные оси {.smaller .columns-2}

<!-- <img src="images/Principal_axes_Legendre_and_Legendre.png" style="height:350px;"> -->

<!-- <small>Из  Legendre & Legendre, 2012</small> -->

<!-- ![](images/Principal_axes_Legendre_and_Legendre.png) -->

<!-- $$ -->
<!-- \cos(\alpha_{11}) = u_{11}\\ -->
<!-- \cos(\alpha_{21}) = u_{21}\\ -->
<!-- \cos(\alpha_{12}) = u_{12}\\ -->
<!-- \cos(\alpha_{22}) = u_{22} -->
<!-- $$ -->

Матрица собственных векторов

```{r}
U
```



```{r,  fig.height=3, fig.align='left', purl=TRUE}
Pl_main_axis <- ggplot(XY_norm_cent, aes(x = x, y = y)) + 
  geom_point() +  
  geom_point(aes(x = mean(x), y = mean(y)), 
             size = 4, color = "yellow") + 
  geom_line(data = PC1, aes(x = x, y = y), 
            color = "yellow", size = 1.5)  +
  geom_line(data = PC2, aes(x = x, y = y), 
            color = "yellow", size = 1.5) +
  coord_equal() + 
  geom_abline(slope = tan(acos(U[1,1])), 
              color = "blue") +
  geom_abline(slope = tan(acos(U[1,1]) + 
                            acos(U[2,1]) + acos(U[2,2])), 
              color = "blue") 

```


<br>
<br>
<br>



```{r}
Pl_main_axis
```


Для удобства восприятия лучше повернуть систему координат так, чтобы ось максимального варьирования была расположена горизонтально.


## Вращение осей {.smaller .columns-2}

<img src="images/Principal_axes_Legendre_and_Legendre.png" style="height:350px;">

<small>Из  Legendre & Legendre, 2012</small>

$$
\cos(\alpha_{11}) = u_{11}\\
\cos(\alpha_{21}) = u_{21}\\
\cos(\alpha_{12}) = u_{12}\\
\cos(\alpha_{22}) = u_{22}
$$



Вращающая матрица

```{r, purl=TRUE}
angle <- -1 * acos(U[1,1]) #Отрицательный угол, так как 
# поворачиваем оси по часовой стрелке

Rot <- matrix(c(cos(angle), sin(angle), 
                -sin(angle), cos(angle)), nrow = 2)
Rot
```

## Вращение осей {.smaller}

Новые оси

```{r, purl=TRUE}
XY_norm_cent_rot <- as.data.frame(t(Rot %*% t(mXY_norm_cent)))

Pl_main_axis_rotated <- ggplot(XY_norm_cent, aes(x = x, y = y)) + 
  geom_point(color = "gray") + 
  geom_point(data = XY_norm_cent_rot, aes(x = V1, y = V2)) + 
  labs(x = "Первая главная ось", y = "Вторая главная ось") +
  geom_hline(yintercept = 0) + 
  geom_vline(xintercept = 0)

Pl_main_axis_rotated

```

## Вращение осей {.smaller}

Новые и старые оси

```{r}
Pl_main_axis_rotated +
  geom_point(aes(x = mean(x), y = mean(y)), 
             size = 4, color = "yellow") +
  geom_line(data = PC1, aes(x = x, y = y), 
            color = "yellow", size = 1.5)  +
  geom_line(data = PC2, aes(x = x, y = y), 
            color = "yellow", size = 1.5) +
  coord_equal() + 
  geom_abline(slope = tan(acos(U[1,1])), 
              color = "green") +
  geom_abline(slope = tan(acos(U[1,1]) + 
                            acos(U[2,1]) + acos(U[2,2])), 
              color = "green") 


```


## Главное о снижении размерности {.smaller}

1.Информацию об исходной матрице $\textbf{Y}$ можно сконцентрировать в ковариационной матрице

$$
\textbf{S} = \frac{1}{n - 1} \textbf{Y}_{centered}'\textbf{Y}_{centered}
$$

2.Матрицу ковариации можно разложить на собственные числа и собственные векторы 


$$
\textbf{S} = \textbf{U}\bf{\Lambda}\textbf{U}^{-1} 
$$

3.Собственные векторы ортогональны и позволяют созадать новую систему координат. Новые оси (главные оси) "опираются" на собственные векторы. 

4.Каждая ось в новой системе координат несет некоторую часть информации об исходной системе координат. Информативность осей определяется величиной собственных чисел.

5.Вместо многомерной системы координат (n осей) мы можем использовать всего несколько (например, две) ниаболее информативные главные оси.




# Level 9: Сингулярное разложение матриц (Singular value decomposition)

## Теорема Экарта-Янга {.smaller}
Любую прямоугольную матрицу $\textbf{Y}$ можно представить в виде произведения трех матриц: 

$$
\textbf{Y}_{n \times p} = \textbf{U}_{n \times p} \textbf{D}_{p \times p} \textbf{V}'_{p \times p} 
$$

То есть можно найти три "вспомогательных" матрицы, через которые можно выразить любую другую матрицу. 

---

$$
\textbf{Y}_{n \times p} = \textbf{U}_{n \times p} \textbf{D}_{p \times p} \textbf{V}'_{p \times p} 
$$

Здесь    

$\textbf{Y}_{n \times p}$ - любая прямоугольная матрица $\begin{pmatrix}a_{11} & a_{12} & \cdots & a_{1c} \\ a_{21} & a_{22} & \cdots & a_{2p} \\  \vdots & \vdots & \ddots & \vdots \\ a_{r1} & a_{n2} & \cdots & a_{np} \end{pmatrix}$   

$\textbf{D}_{p \times p}$ - диагональная матрица $\begin{pmatrix} d_{11} & 0 & \cdots & 0 \\ 0 & d_{22} & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & d_{pp} \end{pmatrix}$

По главной диагонали располагаются "особые" числа, называющиеся **сингулярными числами**. Сингулярные числа ранжируются от большего к меньшему.

$\textbf{U}_{n \times p}$ и $\textbf{V}_{p \times p}$ - левая и правая матрицы сингулярных векторов.

## Сингулярное разложение матрицы средствами R {.smaller}
```{r, purl=TRUE}
set.seed(123456789)
B <- matrix(round(runif(50, 1, 5))  , byrow = T, ncol=5) #Некоторая матрица
SVD <- svd(B) #Сингулярное Разложение матрицы B с помощью функции svd()
V <- SVD$v #"Вспомогательная" матрица - левые сингулярные векторы
D <- SVD$d #Вектор сингулярных чисел
U <- SVD$u #"Вспомогательная" матрица - правые сингулярные векторы 


```

Вычислим $\textbf{V} \textbf{D} \textbf{U}'$

```{r, purl=TRUE}
U %*% diag(D) %*% t(V) 
```

## Задание

Вычислите матрицу, которая получится при использовании только 1 и 2 сингулярного числа для матрицы $\textbf{B}$, использованной на предыдущем слайде.


## Решение
```{r, purl=TRUE}
U[,1:2] %*% diag(D[1:2]) %*% t(V[,1:2]) 

```


## Важное свойство сингулярных чисел

Если вычислить матрицу на основе не всех, а части сингулярных чисел, то новая матрица будет подобна исходной матрице.


```{r, echo=FALSE, purl=FALSE}
library(ggplot2)
Dat <- data.frame(Init = rep(as.vector(B), 4), SingValue = rep(2:5, each = length(as.vector(B))), Calc = c(as.vector((U[,1:2] %*% diag(D[1:2]) %*% t(V[,1:2]))), as.vector((U[,1:3] %*% diag(D[1:3]) %*% t(V[,1:3]))), as.vector((U[,1:4] %*% diag(D[1:4]) %*% t(V[,1:4]))), as.vector((U[,1:5] %*% diag(D[1:5]) %*% t(V[,1:5]))))) 

ggplot(Dat, aes(x = Init, y = Calc)) + geom_point(size = 2) + labs(x = "значения в исходной матрицы", y = "значения в редуцированной матрице") + facet_wrap(~SingValue) + geom_abline(slope = 1)  

```


## Применение свойства сингулярных чисел в сжатии изображений

```{r,echo=FALSE, purl=TRUE}
load("data/face.rda")

gg_face <- function(x) {
  library(reshape2)
  library(ggplot2)
    rotate <- function(x) t(apply(x, 2, rev))
  dd <- rotate(x)
  ddd <- melt(dd)
  ggplot(ddd, aes(Var1, Var2)) + geom_tile(aes(fill = value)) + scale_fill_gradient(low = "darkblue",   high =  "white" ) + coord_equal()
}
gg_face(faceData)

```

<small> Пример взят из курса лекций "Data Analysis" by Jeffrey Leek  

(https://github.com/jtleek/dataanalysis/tree/master/week3)</small>




## Произведем сингулярное разложение матрицы `faceData` {.smaller}

```{r, purl=TRUE}
SVD_face <- svd(faceData)

U <- SVD_face$u
D <- SVD_face$d
V <- SVD_face$v

```

## Рекоструируем изображение, используя только часть информации

```{r, purl=TRUE}
reduction <- function(x) U[,1:x] %*% diag(D[1:x]) %*% t(V[, 1:x])
gg_face(reduction(4))
```



## Применение SVD в биологических исследованиях

SVD - это метод, на котором основаны разные типы анализа, связанные со снижением размерности: PCA, CA, CCA, RDA.

О них в следующех лекциях

## Summary
- Линейная алгебра позволяет решать самые разные типы задач.
- Матричные методы лежат в основе очень многих типов анализа.
- В основе многих методов снижения размерности лежит SVD.

## Что почитать
* Legendre P., Legendre L. (2012) Numerical ecology. Second english edition. Elsevier, Amsterdam. Глава 2. Matrix algebra: a summary.

# Not The End
![](images/matrix_2.jpg)

